{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfa0f37d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-21T19:49:56.881009Z",
     "iopub.status.busy": "2024-04-21T19:49:56.880746Z",
     "iopub.status.idle": "2024-04-21T19:50:02.392470Z",
     "shell.execute_reply": "2024-04-21T19:50:02.391458Z"
    },
    "papermill": {
     "duration": 5.519902,
     "end_time": "2024-04-21T19:50:02.394806",
     "exception": false,
     "start_time": "2024-04-21T19:49:56.874904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5389b304",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T19:50:02.405802Z",
     "iopub.status.busy": "2024-04-21T19:50:02.405378Z",
     "iopub.status.idle": "2024-04-21T19:50:02.409914Z",
     "shell.execute_reply": "2024-04-21T19:50:02.409085Z"
    },
    "papermill": {
     "duration": 0.012027,
     "end_time": "2024-04-21T19:50:02.411901",
     "exception": false,
     "start_time": "2024-04-21T19:50:02.399874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 27\n",
    "batch_size = 1\n",
    "max_epochs = 100\n",
    "vocab_size = 50_000\n",
    "max_seq_len = 4000\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dd75e7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T19:50:02.421607Z",
     "iopub.status.busy": "2024-04-21T19:50:02.421326Z",
     "iopub.status.idle": "2024-04-21T19:50:02.428093Z",
     "shell.execute_reply": "2024-04-21T19:50:02.427448Z"
    },
    "papermill": {
     "duration": 0.013763,
     "end_time": "2024-04-21T19:50:02.429980",
     "exception": false,
     "start_time": "2024-04-21T19:50:02.416217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84a68743",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T19:50:02.439535Z",
     "iopub.status.busy": "2024-04-21T19:50:02.439285Z",
     "iopub.status.idle": "2024-04-21T19:50:05.524271Z",
     "shell.execute_reply": "2024-04-21T19:50:05.523470Z"
    },
    "papermill": {
     "duration": 3.092253,
     "end_time": "2024-04-21T19:50:05.526532",
     "exception": false,
     "start_time": "2024-04-21T19:50:02.434279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_json('/kaggle/input/pii-detection-removal-from-educational-data/train.json')\n",
    "test = pd.read_json('/kaggle/input/pii-detection-removal-from-educational-data/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c7a505c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T19:50:05.536784Z",
     "iopub.status.busy": "2024-04-21T19:50:05.536477Z",
     "iopub.status.idle": "2024-04-21T19:50:39.812783Z",
     "shell.execute_reply": "2024-04-21T19:50:39.811960Z"
    },
    "papermill": {
     "duration": 34.284169,
     "end_time": "2024-04-21T19:50:39.815266",
     "exception": false,
     "start_time": "2024-04-21T19:50:05.531097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(corpus):\n",
    "    tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_lg\")\n",
    "\n",
    "    def tokenizer_fn(data_iterator):\n",
    "        for text in data_iterator:\n",
    "            yield tokenizer(text)\n",
    "\n",
    "    vocab = build_vocab_from_iterator(tokenizer_fn(corpus), specials=[\"<unk>\"], max_tokens=vocab_size)\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "    return vocab, tokenizer\n",
    "\n",
    "corpus = list(train[\"full_text\"].values) + list(test[\"full_text\"].values)\n",
    "vocab, tokenizer = build_vocabulary(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "057f03cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T19:50:39.825896Z",
     "iopub.status.busy": "2024-04-21T19:50:39.825381Z",
     "iopub.status.idle": "2024-04-21T19:50:39.833884Z",
     "shell.execute_reply": "2024-04-21T19:50:39.832985Z"
    },
    "papermill": {
     "duration": 0.015847,
     "end_time": "2024-04-21T19:50:39.835776",
     "exception": false,
     "start_time": "2024-04-21T19:50:39.819929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_text(dataframe, vocab, is_test=False):\n",
    "    dataframe[\"token_num\"] = dataframe[\"tokens\"].apply(lambda x: np.array(vocab(x), dtype=np.int64))\n",
    "    dataframe['seq_len'] = dataframe['tokens'].apply(lambda x: len(x))\n",
    "    percentiles = [i * 0.1 for i in range(10)] + [.95, .99, .995]\n",
    "    buckets = np.quantile(dataframe['seq_len'], percentiles)\n",
    "    bucket_labels = [i for i in range(len(buckets) - 1)]\n",
    "    dataframe['bucket'] = pd.cut(dataframe['seq_len'], bins=buckets, labels=bucket_labels)\n",
    "    dataframe[\"bucket\"] = dataframe[\"bucket\"].fillna(0)\n",
    "    dataframe[\"bucket\"] = dataframe[\"bucket\"].astype(int)\n",
    "    dataframe[\"seq_len\"] = dataframe[\"seq_len\"].astype(int)\n",
    "    if is_test:\n",
    "        return dataframe[['token_num', 'seq_len', 'bucket', 'document']]\n",
    "    return dataframe[['token_num', 'labels', \"seq_len\", \"bucket\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e04b4c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T19:50:39.845512Z",
     "iopub.status.busy": "2024-04-21T19:50:39.845264Z",
     "iopub.status.idle": "2024-04-21T19:50:40.786087Z",
     "shell.execute_reply": "2024-04-21T19:50:40.785333Z"
    },
    "papermill": {
     "duration": 0.948279,
     "end_time": "2024-04-21T19:50:40.788397",
     "exception": false,
     "start_time": "2024-04-21T19:50:39.840118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = tokenize_text(train, vocab, False)\n",
    "test = tokenize_text(test, vocab, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5680c274",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T19:50:40.799026Z",
     "iopub.status.busy": "2024-04-21T19:50:40.798364Z",
     "iopub.status.idle": "2024-04-21T19:50:41.171693Z",
     "shell.execute_reply": "2024-04-21T19:50:41.170854Z"
    },
    "papermill": {
     "duration": 0.381049,
     "end_time": "2024-04-21T19:50:41.174018",
     "exception": false,
     "start_time": "2024-04-21T19:50:40.792969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_2_id = {\n",
    "    \"O\": 0,\n",
    "    \"B-NAME_STUDENT\": 1,\n",
    "    \"I-NAME_STUDENT\": 2,\n",
    "    \"B-URL_PERSONAL\": 3,\n",
    "    \"B-ID_NUM\": 4,\n",
    "    \"B-EMAIL\": 5,\n",
    "    \"I-STREET_ADDRESS\": 6,\n",
    "    \"I-PHONE_NUM\": 7,\n",
    "    \"B-USERNAME\": 8,\n",
    "    \"B-PHONE_NUM\": 9,\n",
    "    \"B-STREET_ADDRESS\": 10,\n",
    "    \"I-URL_PERSONAL\": 11,\n",
    "    \"I-ID_NUM\": 12\n",
    "}\n",
    "\n",
    "id_2_label = {v: k for k, v in label_2_id.items()}\n",
    "train[\"labels\"] = train[\"labels\"].apply(lambda x: [label_2_id[l] for l in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca1efe7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T19:50:41.184455Z",
     "iopub.status.busy": "2024-04-21T19:50:41.184159Z",
     "iopub.status.idle": "2024-04-21T19:50:41.222051Z",
     "shell.execute_reply": "2024-04-21T19:50:41.221201Z"
    },
    "papermill": {
     "duration": 0.04519,
     "end_time": "2024-04-21T19:50:41.223912",
     "exception": false,
     "start_time": "2024-04-21T19:50:41.178722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def complete_batch(dataframe, batch_size):\n",
    "    complete_buckets = []\n",
    "    buckets = [bucket_df for _, bucket_df in dataframe.groupby('bucket')]\n",
    "\n",
    "    for gr_id, bucket in enumerate(buckets):\n",
    "        l = len(bucket)\n",
    "        remainder = l % batch_size\n",
    "        integer = l // batch_size\n",
    "\n",
    "        if remainder != 0:\n",
    "            bucket = pd.concat([bucket, pd.concat([bucket.iloc[:1]] * (batch_size - remainder))], ignore_index=True)\n",
    "            integer += 1\n",
    "\n",
    "        batch_ids = []\n",
    "        for i in range(integer):\n",
    "            batch_ids.extend([f'{i}_bucket{gr_id}'] * batch_size)\n",
    "\n",
    "        bucket['batch_id'] = batch_ids\n",
    "        complete_buckets.append(bucket)\n",
    "    return pd.concat(complete_buckets, ignore_index=True)\n",
    "\n",
    "\n",
    "def shuffle_batches(dataframe):\n",
    "    batch_buckets = [df_new for _, df_new in dataframe.groupby('batch_id')]\n",
    "    random.shuffle(batch_buckets)\n",
    "    return pd.concat(batch_buckets).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def concater_collate(batch):\n",
    "    (xx, yy, lengths, buckets) = zip(*batch)\n",
    "    xx = torch.cat(xx, 0)\n",
    "    yy = torch.from_numpy(np.array(yy))\n",
    "    return xx, yy, list(lengths), list(buckets)\n",
    "\n",
    "def concater_collate_test(batch):\n",
    "    (xx, lengths, buckets, documents) = zip(*batch)\n",
    "    xx = torch.cat(xx, 0)\n",
    "    return xx, list(lengths), list(buckets), list(documents)\n",
    "\n",
    "\n",
    "class PIIDataset(Dataset):\n",
    "    def __init__(self, dataframe, batch_size, is_test=False):\n",
    "        dataframe = complete_batch(dataframe=dataframe, batch_size=batch_size)\n",
    "        dataframe = shuffle_batches(dataframe=dataframe)\n",
    "        self.is_test = is_test\n",
    "        if is_test:\n",
    "            self.dataframe = dataframe[['token_num', 'seq_len', 'bucket', 'document']]\n",
    "        else:\n",
    "            self.dataframe = dataframe[['token_num', 'labels', 'seq_len', 'bucket']]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.is_test:\n",
    "            X, seq_len, bucket, document = self.dataframe.iloc[index, :]\n",
    "            X = torch.from_numpy(X)\n",
    "            return X, seq_len, bucket, document\n",
    "             \n",
    "        else:\n",
    "            X, Y, seq_len, bucket = self.dataframe.iloc[index, :]\n",
    "            Y = torch.from_numpy(np.array(Y))\n",
    "            padding = max_seq_len - len(Y)\n",
    "            Y = F.pad(Y, (0, padding), value=99)\n",
    "            X = torch.from_numpy(X)\n",
    "            return X, Y, seq_len, bucket\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b93c112f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T19:50:41.233909Z",
     "iopub.status.busy": "2024-04-21T19:50:41.233644Z",
     "iopub.status.idle": "2024-04-21T19:50:42.561790Z",
     "shell.execute_reply": "2024-04-21T19:50:42.560900Z"
    },
    "papermill": {
     "duration": 1.336034,
     "end_time": "2024-04-21T19:50:42.564455",
     "exception": false,
     "start_time": "2024-04-21T19:50:41.228421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = PIIDataset(train, batch_size, False)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=concater_collate,\n",
    "    drop_last=False,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7bf0b49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T19:50:42.575258Z",
     "iopub.status.busy": "2024-04-21T19:50:42.574969Z",
     "iopub.status.idle": "2024-04-21T19:50:42.595267Z",
     "shell.execute_reply": "2024-04-21T19:50:42.594546Z"
    },
    "papermill": {
     "duration": 0.02779,
     "end_time": "2024-04-21T19:50:42.597057",
     "exception": false,
     "start_time": "2024-04-21T19:50:42.569267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RotateChord(nn.Module):\n",
    "    def __init__(self, n_tracks, track_size):\n",
    "        super(RotateChord, self).__init__()\n",
    "        self.n_tracks = n_tracks\n",
    "        self.track_size = track_size\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "\n",
    "        ys = torch.split(\n",
    "            tensor=x,\n",
    "            split_size_or_sections=lengths,\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "        zs = []\n",
    "\n",
    "        for y in ys:\n",
    "            y = torch.split(\n",
    "                tensor=y,\n",
    "                split_size_or_sections=self.track_size,\n",
    "                dim=-1\n",
    "            )\n",
    "            z = [y[0]]\n",
    "            for i in range(1, len(y)):\n",
    "                offset = -2 ** (i - 1)\n",
    "                z.append(torch.roll(y[i], shifts=offset, dims=0))\n",
    "            z = torch.cat(z, -1)\n",
    "            zs.append(z)\n",
    "\n",
    "        z = torch.cat(zs, 0)\n",
    "        assert z.shape == x.shape, 'shape mismatch'\n",
    "        return z\n",
    "\n",
    "\n",
    "class ChordMixerBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_size,\n",
    "            n_tracks,\n",
    "            track_size,\n",
    "            hidden_size,\n",
    "            mlp_dropout,\n",
    "            layer_dropout\n",
    "    ):\n",
    "        super(ChordMixerBlock, self).__init__()\n",
    "\n",
    "        self.mixer = MLP(\n",
    "            embedding_size,\n",
    "            hidden_size,\n",
    "            embedding_size,\n",
    "            act_layer=nn.GELU,\n",
    "            drop=mlp_dropout\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(layer_dropout)\n",
    "\n",
    "        self.rotator = RotateChord(n_tracks, track_size)\n",
    "\n",
    "    def forward(self, data, lengths):\n",
    "        res_con = data\n",
    "        data = self.mixer(data)\n",
    "        data = self.dropout(data)\n",
    "        data = self.rotator(data, lengths)\n",
    "        data = data + res_con\n",
    "        return data\n",
    "\n",
    "\n",
    "class ChordMixer(nn.Module):\n",
    "    def __init__(self, vocab_size=vocab_size, max_seq_len=max_seq_len, track_size=16, hidden_size=196, mlp_dropout=0.0, layer_dropout=0.0):\n",
    "        super(ChordMixer, self).__init__()\n",
    "        self.max_n_layers = math.ceil(np.log2(max_seq_len))\n",
    "        n_tracks = math.ceil(np.log2(max_seq_len))\n",
    "        embedding_size = int(n_tracks * track_size)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_size\n",
    "        )\n",
    "\n",
    "        self.chordmixer_blocks = nn.ModuleList(\n",
    "            [\n",
    "                ChordMixerBlock(\n",
    "                    embedding_size,\n",
    "                    n_tracks,\n",
    "                    track_size,\n",
    "                    hidden_size,\n",
    "                    mlp_dropout,\n",
    "                    layer_dropout\n",
    "                )\n",
    "                for _ in range(self.max_n_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.reshape = nn.Linear(embedding_size, max_seq_len * embedding_size)\n",
    "        self.final = nn.Linear(embedding_size, 13)\n",
    "\n",
    "    def forward(self, data, lengths):\n",
    "        n_layers = math.ceil(np.log2(lengths[0]))\n",
    "\n",
    "        data = self.embedding(data)\n",
    "        for layer in range(n_layers):\n",
    "            data = self.chordmixer_blocks[layer](data, lengths)\n",
    "\n",
    "        data = [torch.mean(t, dim=0) for t in torch.split(data, lengths)]\n",
    "        data = torch.stack(data)\n",
    "        \n",
    "        data = self.reshape(data)\n",
    "        data = data.view(data.size(0), self.max_seq_len, -1)\n",
    "        \n",
    "        data = self.final(data)\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b485514c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T19:50:42.606514Z",
     "iopub.status.busy": "2024-04-21T19:50:42.606276Z",
     "iopub.status.idle": "2024-04-21T19:50:42.615714Z",
     "shell.execute_reply": "2024-04-21T19:50:42.614884Z"
    },
    "papermill": {
     "duration": 0.016343,
     "end_time": "2024-04-21T19:50:42.617652",
     "exception": false,
     "start_time": "2024-04-21T19:50:42.601309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, train_dataloader, criterion, optimizer):\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def train(self, current_epoch_nr):\n",
    "        self.model.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        items_processed = 0\n",
    "\n",
    "        for idx, (x, y, seq_len, _) in enumerate(self.train_dataloader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            y_hat = self.model(x, seq_len)\n",
    "\n",
    "            y_hat_flat = y_hat.view(-1, y_hat.size(-1))\n",
    "            y_flat = y.view(-1)\n",
    "            \n",
    "            mask = y_flat.ne(99)\n",
    "            y_hat_flat_masked = y_hat_flat[mask]\n",
    "            y_flat_masked = y_flat[mask]\n",
    "\n",
    "            loss = self.criterion(y_hat_flat_masked, y_flat_masked)\n",
    "            loss.backward()\n",
    "\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            items_processed += y.size(0)\n",
    "            \n",
    "            if idx % 500 == 0:\n",
    "                print(f\"Epoch {current_epoch_nr + 1} / {max_epochs} - Batch {idx} - Loss: {running_loss / items_processed}\")\n",
    "                \n",
    "        epoch_loss = running_loss / items_processed\n",
    "        if (current_epoch_nr + 1) % 5 == 0:\n",
    "            torch.save(self.model.state_dict(), f\"checkpoints/epoch_{current_epoch_nr + 1}_loss_{epoch_loss}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfc40053",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T19:50:42.627337Z",
     "iopub.status.busy": "2024-04-21T19:50:42.626837Z",
     "iopub.status.idle": "2024-04-22T03:02:43.208510Z",
     "shell.execute_reply": "2024-04-22T03:02:43.207455Z"
    },
    "papermill": {
     "duration": 25920.589136,
     "end_time": "2024-04-22T03:02:43.210972",
     "exception": false,
     "start_time": "2024-04-21T19:50:42.621836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 100 - Batch 0 - Loss: 2.6077630519866943\n",
      "Epoch 1 / 100 - Batch 500 - Loss: 0.042729510629341265\n",
      "Epoch 1 / 100 - Batch 1000 - Loss: 0.024450827573749404\n",
      "Epoch 1 / 100 - Batch 1500 - Loss: 0.01809900605410919\n",
      "Epoch 1 / 100 - Batch 2000 - Loss: 0.015232472826109417\n",
      "Epoch 1 / 100 - Batch 2500 - Loss: 0.013295595788560917\n",
      "Epoch 1 / 100 - Batch 3000 - Loss: 0.01186585846767043\n",
      "Epoch 1 / 100 - Batch 3500 - Loss: 0.010999038123663214\n",
      "Epoch 1 / 100 - Batch 4000 - Loss: 0.010202813884706603\n",
      "Epoch 1 / 100 - Batch 4500 - Loss: 0.009529156558160038\n",
      "Epoch 1 / 100 - Batch 5000 - Loss: 0.009101886835153917\n",
      "Epoch 1 / 100 - Batch 5500 - Loss: 0.008667488734432972\n",
      "Epoch 1 / 100 - Batch 6000 - Loss: 0.008363036199648387\n",
      "Epoch 1 / 100 - Batch 6500 - Loss: 0.008159541033390208\n",
      "Epoch 2 / 100 - Batch 0 - Loss: 0.0003226705302949995\n",
      "Epoch 2 / 100 - Batch 500 - Loss: 0.0035526624074072844\n",
      "Epoch 2 / 100 - Batch 1000 - Loss: 0.003967821415739723\n",
      "Epoch 2 / 100 - Batch 1500 - Loss: 0.004188299826726447\n",
      "Epoch 2 / 100 - Batch 2000 - Loss: 0.004502329083706805\n",
      "Epoch 2 / 100 - Batch 2500 - Loss: 0.0045580294631670775\n",
      "Epoch 2 / 100 - Batch 3000 - Loss: 0.00451016806152124\n",
      "Epoch 2 / 100 - Batch 3500 - Loss: 0.004621854398537164\n",
      "Epoch 2 / 100 - Batch 4000 - Loss: 0.0045313591877084005\n",
      "Epoch 2 / 100 - Batch 4500 - Loss: 0.004424512083998082\n",
      "Epoch 2 / 100 - Batch 5000 - Loss: 0.004410467187970953\n",
      "Epoch 2 / 100 - Batch 5500 - Loss: 0.00436066215310709\n",
      "Epoch 2 / 100 - Batch 6000 - Loss: 0.004336532802166313\n",
      "Epoch 2 / 100 - Batch 6500 - Loss: 0.004354101756793801\n",
      "Epoch 3 / 100 - Batch 0 - Loss: 0.0002938437683042139\n",
      "Epoch 3 / 100 - Batch 500 - Loss: 0.003196386876146437\n",
      "Epoch 3 / 100 - Batch 1000 - Loss: 0.003580821979427695\n",
      "Epoch 3 / 100 - Batch 1500 - Loss: 0.003709055642469464\n",
      "Epoch 3 / 100 - Batch 2000 - Loss: 0.003927640705084301\n",
      "Epoch 3 / 100 - Batch 2500 - Loss: 0.004013396880314472\n",
      "Epoch 3 / 100 - Batch 3000 - Loss: 0.003980167697780657\n",
      "Epoch 3 / 100 - Batch 3500 - Loss: 0.004097119247678761\n",
      "Epoch 3 / 100 - Batch 4000 - Loss: 0.004022354867571281\n",
      "Epoch 3 / 100 - Batch 4500 - Loss: 0.0039312128820341245\n",
      "Epoch 3 / 100 - Batch 5000 - Loss: 0.003929635517735247\n",
      "Epoch 3 / 100 - Batch 5500 - Loss: 0.0038981407093897063\n",
      "Epoch 3 / 100 - Batch 6000 - Loss: 0.0038608818605885536\n",
      "Epoch 3 / 100 - Batch 6500 - Loss: 0.003883635005608056\n",
      "Epoch 4 / 100 - Batch 0 - Loss: 0.00016229927132371813\n",
      "Epoch 4 / 100 - Batch 500 - Loss: 0.0028085279782444307\n",
      "Epoch 4 / 100 - Batch 1000 - Loss: 0.003218456499563434\n",
      "Epoch 4 / 100 - Batch 1500 - Loss: 0.0033134335549163288\n",
      "Epoch 4 / 100 - Batch 2000 - Loss: 0.003483513309850108\n",
      "Epoch 4 / 100 - Batch 2500 - Loss: 0.00376481172705182\n",
      "Epoch 4 / 100 - Batch 3000 - Loss: 0.003760692965216498\n",
      "Epoch 4 / 100 - Batch 3500 - Loss: 0.0038297808746873357\n",
      "Epoch 4 / 100 - Batch 4000 - Loss: 0.003762849750225231\n",
      "Epoch 4 / 100 - Batch 4500 - Loss: 0.003687409450280964\n",
      "Epoch 4 / 100 - Batch 5000 - Loss: 0.003660185826715367\n",
      "Epoch 4 / 100 - Batch 5500 - Loss: 0.00362842322157071\n",
      "Epoch 4 / 100 - Batch 6000 - Loss: 0.0035938917690002802\n",
      "Epoch 4 / 100 - Batch 6500 - Loss: 0.0035889729249692147\n",
      "Epoch 5 / 100 - Batch 0 - Loss: 5.653569314745255e-05\n",
      "Epoch 5 / 100 - Batch 500 - Loss: 0.0023952364196217846\n",
      "Epoch 5 / 100 - Batch 1000 - Loss: 0.002931696801475745\n",
      "Epoch 5 / 100 - Batch 1500 - Loss: 0.002952319342463657\n",
      "Epoch 5 / 100 - Batch 2000 - Loss: 0.0030742038736902254\n",
      "Epoch 5 / 100 - Batch 2500 - Loss: 0.0034031149092856893\n",
      "Epoch 5 / 100 - Batch 3000 - Loss: 0.003383441628604358\n",
      "Epoch 5 / 100 - Batch 3500 - Loss: 0.0035291044495511306\n",
      "Epoch 5 / 100 - Batch 4000 - Loss: 0.0034597235372505176\n",
      "Epoch 5 / 100 - Batch 4500 - Loss: 0.0033688826115596793\n",
      "Epoch 5 / 100 - Batch 5000 - Loss: 0.0033317431324658567\n",
      "Epoch 5 / 100 - Batch 5500 - Loss: 0.0032825966588182886\n",
      "Epoch 5 / 100 - Batch 6000 - Loss: 0.0032900595333353994\n",
      "Epoch 5 / 100 - Batch 6500 - Loss: 0.003304195012620833\n",
      "Epoch 6 / 100 - Batch 0 - Loss: 1.2859522030339576e-05\n",
      "Epoch 6 / 100 - Batch 500 - Loss: 0.00449008471736272\n",
      "Epoch 6 / 100 - Batch 1000 - Loss: 0.004489541883016943\n",
      "Epoch 6 / 100 - Batch 1500 - Loss: 0.004133574332898784\n",
      "Epoch 6 / 100 - Batch 2000 - Loss: 0.004003330084245953\n",
      "Epoch 6 / 100 - Batch 2500 - Loss: 0.0038968572440548563\n",
      "Epoch 6 / 100 - Batch 3000 - Loss: 0.0037493599201820246\n",
      "Epoch 6 / 100 - Batch 3500 - Loss: 0.003742229909238042\n",
      "Epoch 6 / 100 - Batch 4000 - Loss: 0.0036106907486848218\n",
      "Epoch 6 / 100 - Batch 4500 - Loss: 0.003480886886188357\n",
      "Epoch 6 / 100 - Batch 5000 - Loss: 0.003469740616602015\n",
      "Epoch 6 / 100 - Batch 5500 - Loss: 0.0034095473697873563\n",
      "Epoch 6 / 100 - Batch 6000 - Loss: 0.003492464166673578\n",
      "Epoch 6 / 100 - Batch 6500 - Loss: 0.0034688086670488\n",
      "Epoch 7 / 100 - Batch 0 - Loss: 1.3246114576759282e-05\n",
      "Epoch 7 / 100 - Batch 500 - Loss: 0.003065192527268986\n",
      "Epoch 7 / 100 - Batch 1000 - Loss: 0.00299833732751057\n",
      "Epoch 7 / 100 - Batch 1500 - Loss: 0.003008159169242531\n",
      "Epoch 7 / 100 - Batch 2000 - Loss: 0.002971674002766583\n",
      "Epoch 7 / 100 - Batch 2500 - Loss: 0.0030201426833497426\n",
      "Epoch 7 / 100 - Batch 3000 - Loss: 0.0030484005658837784\n",
      "Epoch 7 / 100 - Batch 3500 - Loss: 0.0031086136937935995\n",
      "Epoch 7 / 100 - Batch 4000 - Loss: 0.003029021191361399\n",
      "Epoch 7 / 100 - Batch 4500 - Loss: 0.002945421480918606\n",
      "Epoch 7 / 100 - Batch 5000 - Loss: 0.003008647067490501\n",
      "Epoch 7 / 100 - Batch 5500 - Loss: 0.0029587482002950216\n",
      "Epoch 7 / 100 - Batch 6000 - Loss: 0.0029606790643733672\n",
      "Epoch 7 / 100 - Batch 6500 - Loss: 0.00295701930910748\n",
      "Epoch 8 / 100 - Batch 0 - Loss: 0.0001463981025153771\n",
      "Epoch 8 / 100 - Batch 500 - Loss: 0.002216778784108232\n",
      "Epoch 8 / 100 - Batch 1000 - Loss: 0.0023821359823399508\n",
      "Epoch 8 / 100 - Batch 1500 - Loss: 0.002786348939008133\n",
      "Epoch 8 / 100 - Batch 2000 - Loss: 0.0028914139189364692\n",
      "Epoch 8 / 100 - Batch 2500 - Loss: 0.002939551728397165\n",
      "Epoch 8 / 100 - Batch 3000 - Loss: 0.0028743218913687752\n",
      "Epoch 8 / 100 - Batch 3500 - Loss: 0.0029802862758366855\n",
      "Epoch 8 / 100 - Batch 4000 - Loss: 0.0028874975473530593\n",
      "Epoch 8 / 100 - Batch 4500 - Loss: 0.002818308029308373\n",
      "Epoch 8 / 100 - Batch 5000 - Loss: 0.0027622988771501514\n",
      "Epoch 8 / 100 - Batch 5500 - Loss: 0.002698640474116371\n",
      "Epoch 8 / 100 - Batch 6000 - Loss: 0.002702369578219001\n",
      "Epoch 8 / 100 - Batch 6500 - Loss: 0.002728266401504617\n",
      "Epoch 9 / 100 - Batch 0 - Loss: 8.421060556429438e-06\n",
      "Epoch 9 / 100 - Batch 500 - Loss: 0.0018720121660666274\n",
      "Epoch 9 / 100 - Batch 1000 - Loss: 0.002038765154559591\n",
      "Epoch 9 / 100 - Batch 1500 - Loss: 0.0021569698345483414\n",
      "Epoch 9 / 100 - Batch 2000 - Loss: 0.002165070747182071\n",
      "Epoch 9 / 100 - Batch 2500 - Loss: 0.0022597425205028933\n",
      "Epoch 9 / 100 - Batch 3000 - Loss: 0.002246515233344338\n",
      "Epoch 9 / 100 - Batch 3500 - Loss: 0.0023988823008796044\n",
      "Epoch 9 / 100 - Batch 4000 - Loss: 0.0023477391514425293\n",
      "Epoch 9 / 100 - Batch 4500 - Loss: 0.0022885445611450436\n",
      "Epoch 9 / 100 - Batch 5000 - Loss: 0.0022634261633491326\n",
      "Epoch 9 / 100 - Batch 5500 - Loss: 0.0022432372967685957\n",
      "Epoch 9 / 100 - Batch 6000 - Loss: 0.002227002142980613\n",
      "Epoch 9 / 100 - Batch 6500 - Loss: 0.002270610095412956\n",
      "Epoch 10 / 100 - Batch 0 - Loss: 5.152552148501854e-06\n",
      "Epoch 10 / 100 - Batch 500 - Loss: 0.0018073579808944066\n",
      "Epoch 10 / 100 - Batch 1000 - Loss: 0.0019297130577653654\n",
      "Epoch 10 / 100 - Batch 1500 - Loss: 0.0019448710796370573\n",
      "Epoch 10 / 100 - Batch 2000 - Loss: 0.002335472988075969\n",
      "Epoch 10 / 100 - Batch 2500 - Loss: 0.0023581718872495443\n",
      "Epoch 10 / 100 - Batch 3000 - Loss: 0.002312964464217951\n",
      "Epoch 10 / 100 - Batch 3500 - Loss: 0.002386150892763706\n",
      "Epoch 10 / 100 - Batch 4000 - Loss: 0.002315428265899682\n",
      "Epoch 10 / 100 - Batch 4500 - Loss: 0.0022557396192127523\n",
      "Epoch 10 / 100 - Batch 5000 - Loss: 0.002292942322074031\n",
      "Epoch 10 / 100 - Batch 5500 - Loss: 0.0022550589795226153\n",
      "Epoch 10 / 100 - Batch 6000 - Loss: 0.002314826945325229\n",
      "Epoch 10 / 100 - Batch 6500 - Loss: 0.002442576134596344\n",
      "Epoch 11 / 100 - Batch 0 - Loss: 0.00040004774928092957\n",
      "Epoch 11 / 100 - Batch 500 - Loss: 0.0017406084998309531\n",
      "Epoch 11 / 100 - Batch 1000 - Loss: 0.0019152581341754175\n",
      "Epoch 11 / 100 - Batch 1500 - Loss: 0.0018409423594266882\n",
      "Epoch 11 / 100 - Batch 2000 - Loss: 0.0020327269799513756\n",
      "Epoch 11 / 100 - Batch 2500 - Loss: 0.002055817360324206\n",
      "Epoch 11 / 100 - Batch 3000 - Loss: 0.002001808046284837\n",
      "Epoch 11 / 100 - Batch 3500 - Loss: 0.0020436049315735905\n",
      "Epoch 11 / 100 - Batch 4000 - Loss: 0.001977637529161556\n",
      "Epoch 11 / 100 - Batch 4500 - Loss: 0.0019004940798595302\n",
      "Epoch 11 / 100 - Batch 5000 - Loss: 0.0018924771961102574\n",
      "Epoch 11 / 100 - Batch 5500 - Loss: 0.001865216899641145\n",
      "Epoch 11 / 100 - Batch 6000 - Loss: 0.0018972996243601429\n",
      "Epoch 11 / 100 - Batch 6500 - Loss: 0.0018933032823528163\n",
      "Epoch 12 / 100 - Batch 0 - Loss: 0.0005862824618816376\n",
      "Epoch 12 / 100 - Batch 500 - Loss: 0.0016578033245220022\n",
      "Epoch 12 / 100 - Batch 1000 - Loss: 0.0017079342531616214\n",
      "Epoch 12 / 100 - Batch 1500 - Loss: 0.0017728010627701782\n",
      "Epoch 12 / 100 - Batch 2000 - Loss: 0.0016789521112286743\n",
      "Epoch 12 / 100 - Batch 2500 - Loss: 0.0020000800077119886\n",
      "Epoch 12 / 100 - Batch 3000 - Loss: 0.0019172548933466239\n",
      "Epoch 12 / 100 - Batch 3500 - Loss: 0.001908844502557636\n",
      "Epoch 12 / 100 - Batch 4000 - Loss: 0.0018446136523641514\n",
      "Epoch 12 / 100 - Batch 4500 - Loss: 0.0017882430761099618\n",
      "Epoch 12 / 100 - Batch 5000 - Loss: 0.0018092441587080155\n",
      "Epoch 12 / 100 - Batch 5500 - Loss: 0.001772262369116831\n",
      "Epoch 12 / 100 - Batch 6000 - Loss: 0.0017108735664929172\n",
      "Epoch 12 / 100 - Batch 6500 - Loss: 0.0016780876417769811\n",
      "Epoch 13 / 100 - Batch 0 - Loss: 5.8806632296182215e-05\n",
      "Epoch 13 / 100 - Batch 500 - Loss: 0.0011990737221433203\n",
      "Epoch 13 / 100 - Batch 1000 - Loss: 0.001366552612892985\n",
      "Epoch 13 / 100 - Batch 1500 - Loss: 0.0013183883228274171\n",
      "Epoch 13 / 100 - Batch 2000 - Loss: 0.0012798142241279942\n",
      "Epoch 13 / 100 - Batch 2500 - Loss: 0.0014939302231707367\n",
      "Epoch 13 / 100 - Batch 3000 - Loss: 0.0014583350592969153\n",
      "Epoch 13 / 100 - Batch 3500 - Loss: 0.0014517722894676285\n",
      "Epoch 13 / 100 - Batch 4000 - Loss: 0.0014903741328369463\n",
      "Epoch 13 / 100 - Batch 4500 - Loss: 0.0014418283582257304\n",
      "Epoch 13 / 100 - Batch 5000 - Loss: 0.0014585986275588148\n",
      "Epoch 13 / 100 - Batch 5500 - Loss: 0.0014256741085625951\n",
      "Epoch 13 / 100 - Batch 6000 - Loss: 0.001390975625150501\n",
      "Epoch 13 / 100 - Batch 6500 - Loss: 0.0013689818057118057\n",
      "Epoch 14 / 100 - Batch 0 - Loss: 0.00010499612108105794\n",
      "Epoch 14 / 100 - Batch 500 - Loss: 0.0007330500818065458\n",
      "Epoch 14 / 100 - Batch 1000 - Loss: 0.000897777906918823\n",
      "Epoch 14 / 100 - Batch 1500 - Loss: 0.0009277917100982167\n",
      "Epoch 14 / 100 - Batch 2000 - Loss: 0.0010020591899696416\n",
      "Epoch 14 / 100 - Batch 2500 - Loss: 0.00103914319801389\n",
      "Epoch 14 / 100 - Batch 3000 - Loss: 0.0010322169493593649\n",
      "Epoch 14 / 100 - Batch 3500 - Loss: 0.0010705468386136056\n",
      "Epoch 14 / 100 - Batch 4000 - Loss: 0.0010625345689176462\n",
      "Epoch 14 / 100 - Batch 4500 - Loss: 0.0010395559148709318\n",
      "Epoch 14 / 100 - Batch 5000 - Loss: 0.0010206200723003294\n",
      "Epoch 14 / 100 - Batch 5500 - Loss: 0.0010039960573009598\n",
      "Epoch 14 / 100 - Batch 6000 - Loss: 0.0010248066256716322\n",
      "Epoch 14 / 100 - Batch 6500 - Loss: 0.001005362130640802\n",
      "Epoch 15 / 100 - Batch 0 - Loss: 7.234946679091081e-05\n",
      "Epoch 15 / 100 - Batch 500 - Loss: 0.0006195689743613388\n",
      "Epoch 15 / 100 - Batch 1000 - Loss: 0.0007749937409815239\n",
      "Epoch 15 / 100 - Batch 1500 - Loss: 0.0006953969727066652\n",
      "Epoch 15 / 100 - Batch 2000 - Loss: 0.0007691206096488837\n",
      "Epoch 15 / 100 - Batch 2500 - Loss: 0.0007809391474205476\n",
      "Epoch 15 / 100 - Batch 3000 - Loss: 0.0007989735575689844\n",
      "Epoch 15 / 100 - Batch 3500 - Loss: 0.0008764281966699937\n",
      "Epoch 15 / 100 - Batch 4000 - Loss: 0.0008487106733500797\n",
      "Epoch 15 / 100 - Batch 4500 - Loss: 0.0008244410448669115\n",
      "Epoch 15 / 100 - Batch 5000 - Loss: 0.0008367563561447589\n",
      "Epoch 15 / 100 - Batch 5500 - Loss: 0.0008385433352213332\n",
      "Epoch 15 / 100 - Batch 6000 - Loss: 0.0008299150580116544\n",
      "Epoch 15 / 100 - Batch 6500 - Loss: 0.0008240663789289471\n",
      "Epoch 16 / 100 - Batch 0 - Loss: 5.85286361456383e-05\n",
      "Epoch 16 / 100 - Batch 500 - Loss: 0.0005225137041519825\n",
      "Epoch 16 / 100 - Batch 1000 - Loss: 0.0006138700226360795\n",
      "Epoch 16 / 100 - Batch 1500 - Loss: 0.0005662945827056817\n",
      "Epoch 16 / 100 - Batch 2000 - Loss: 0.0007163407000766376\n",
      "Epoch 16 / 100 - Batch 2500 - Loss: 0.0007632213590753427\n",
      "Epoch 16 / 100 - Batch 3000 - Loss: 0.0007581355570811674\n",
      "Epoch 16 / 100 - Batch 3500 - Loss: 0.0007563644916811651\n",
      "Epoch 16 / 100 - Batch 4000 - Loss: 0.0007222651218051036\n",
      "Epoch 16 / 100 - Batch 4500 - Loss: 0.0007174676664886742\n",
      "Epoch 16 / 100 - Batch 5000 - Loss: 0.000735382633007793\n",
      "Epoch 16 / 100 - Batch 5500 - Loss: 0.0007465787665181337\n",
      "Epoch 16 / 100 - Batch 6000 - Loss: 0.0007286669140085899\n",
      "Epoch 16 / 100 - Batch 6500 - Loss: 0.0007200904806559509\n",
      "Epoch 17 / 100 - Batch 0 - Loss: 3.025071748652408e-07\n",
      "Epoch 17 / 100 - Batch 500 - Loss: 0.0004529023386571002\n",
      "Epoch 17 / 100 - Batch 1000 - Loss: 0.00042956545247913063\n",
      "Epoch 17 / 100 - Batch 1500 - Loss: 0.0005128527193198118\n",
      "Epoch 17 / 100 - Batch 2000 - Loss: 0.0005313797556016085\n",
      "Epoch 17 / 100 - Batch 2500 - Loss: 0.0005433398395440354\n",
      "Epoch 17 / 100 - Batch 3000 - Loss: 0.0005649175037466819\n",
      "Epoch 17 / 100 - Batch 3500 - Loss: 0.0005699039948357974\n",
      "Epoch 17 / 100 - Batch 4000 - Loss: 0.0005461438032626836\n",
      "Epoch 17 / 100 - Batch 4500 - Loss: 0.0005375465774273906\n",
      "Epoch 17 / 100 - Batch 5000 - Loss: 0.0005547897101690743\n",
      "Epoch 17 / 100 - Batch 5500 - Loss: 0.000568762436800061\n",
      "Epoch 17 / 100 - Batch 6000 - Loss: 0.0005522943739763989\n",
      "Epoch 17 / 100 - Batch 6500 - Loss: 0.0005631572385642471\n",
      "Epoch 18 / 100 - Batch 0 - Loss: 3.80130563826242e-06\n",
      "Epoch 18 / 100 - Batch 500 - Loss: 0.0003832862115215894\n",
      "Epoch 18 / 100 - Batch 1000 - Loss: 0.0003828781949425527\n",
      "Epoch 18 / 100 - Batch 1500 - Loss: 0.00045228763637510447\n",
      "Epoch 18 / 100 - Batch 2000 - Loss: 0.0006738709991837629\n",
      "Epoch 18 / 100 - Batch 2500 - Loss: 0.0006510599137009985\n",
      "Epoch 18 / 100 - Batch 3000 - Loss: 0.0006173329353089493\n",
      "Epoch 18 / 100 - Batch 3500 - Loss: 0.0006101241515400016\n",
      "Epoch 18 / 100 - Batch 4000 - Loss: 0.0005952218506706641\n",
      "Epoch 18 / 100 - Batch 4500 - Loss: 0.0005667900260970477\n",
      "Epoch 18 / 100 - Batch 5000 - Loss: 0.0005686332349104642\n",
      "Epoch 18 / 100 - Batch 5500 - Loss: 0.0005584489972997672\n",
      "Epoch 18 / 100 - Batch 6000 - Loss: 0.0005763099114942778\n",
      "Epoch 18 / 100 - Batch 6500 - Loss: 0.0005727029915459831\n",
      "Epoch 19 / 100 - Batch 0 - Loss: 2.125958781107329e-05\n",
      "Epoch 19 / 100 - Batch 500 - Loss: 0.00034730690134697283\n",
      "Epoch 19 / 100 - Batch 1000 - Loss: 0.00041602253377999313\n",
      "Epoch 19 / 100 - Batch 1500 - Loss: 0.000421746663461822\n",
      "Epoch 19 / 100 - Batch 2000 - Loss: 0.000439990419780244\n",
      "Epoch 19 / 100 - Batch 2500 - Loss: 0.0005562717458062565\n",
      "Epoch 19 / 100 - Batch 3000 - Loss: 0.00050988137354112\n",
      "Epoch 19 / 100 - Batch 3500 - Loss: 0.0005219203369018049\n",
      "Epoch 19 / 100 - Batch 4000 - Loss: 0.0004974561870359745\n",
      "Epoch 19 / 100 - Batch 4500 - Loss: 0.00045662232742491504\n",
      "Epoch 19 / 100 - Batch 5000 - Loss: 0.00044759976296790573\n",
      "Epoch 19 / 100 - Batch 5500 - Loss: 0.0004314329764667469\n",
      "Epoch 19 / 100 - Batch 6000 - Loss: 0.0004457052202934715\n",
      "Epoch 19 / 100 - Batch 6500 - Loss: 0.00045492795001785626\n",
      "Epoch 20 / 100 - Batch 0 - Loss: 2.4055304947978584e-07\n",
      "Epoch 20 / 100 - Batch 500 - Loss: 0.000420310537546896\n",
      "Epoch 20 / 100 - Batch 1000 - Loss: 0.0004524481411933173\n",
      "Epoch 20 / 100 - Batch 1500 - Loss: 0.0005653729560310395\n",
      "Epoch 20 / 100 - Batch 2000 - Loss: 0.0006597032524104948\n",
      "Epoch 20 / 100 - Batch 2500 - Loss: 0.0006606001023452643\n",
      "Epoch 20 / 100 - Batch 3000 - Loss: 0.0005975165089645761\n",
      "Epoch 20 / 100 - Batch 3500 - Loss: 0.0005731886129089535\n",
      "Epoch 20 / 100 - Batch 4000 - Loss: 0.0005342632578770279\n",
      "Epoch 20 / 100 - Batch 4500 - Loss: 0.0004873031908995959\n",
      "Epoch 20 / 100 - Batch 5000 - Loss: 0.0004956612634652467\n",
      "Epoch 20 / 100 - Batch 5500 - Loss: 0.0004748136206716466\n",
      "Epoch 20 / 100 - Batch 6000 - Loss: 0.0005173739604767069\n",
      "Epoch 20 / 100 - Batch 6500 - Loss: 0.0005256291804038122\n",
      "Epoch 21 / 100 - Batch 0 - Loss: 7.579693829029566e-06\n",
      "Epoch 21 / 100 - Batch 500 - Loss: 0.00019316958828880204\n",
      "Epoch 21 / 100 - Batch 1000 - Loss: 0.00021420119234328124\n",
      "Epoch 21 / 100 - Batch 1500 - Loss: 0.0006020600863206484\n",
      "Epoch 21 / 100 - Batch 2000 - Loss: 0.0005256762021267636\n",
      "Epoch 21 / 100 - Batch 2500 - Loss: 0.0004773972338939793\n",
      "Epoch 21 / 100 - Batch 3000 - Loss: 0.0004444665077441858\n",
      "Epoch 21 / 100 - Batch 3500 - Loss: 0.00046606782085085893\n",
      "Epoch 21 / 100 - Batch 4000 - Loss: 0.00043470220850255086\n",
      "Epoch 21 / 100 - Batch 4500 - Loss: 0.00039644832408460056\n",
      "Epoch 21 / 100 - Batch 5000 - Loss: 0.0004329577785312333\n",
      "Epoch 21 / 100 - Batch 5500 - Loss: 0.00040914125427646617\n",
      "Epoch 21 / 100 - Batch 6000 - Loss: 0.00043838648734210393\n",
      "Epoch 21 / 100 - Batch 6500 - Loss: 0.0004541469507965947\n",
      "Epoch 22 / 100 - Batch 0 - Loss: 4.658432317228289e-06\n",
      "Epoch 22 / 100 - Batch 500 - Loss: 0.0001743517152046752\n",
      "Epoch 22 / 100 - Batch 1000 - Loss: 0.00018879268667944054\n",
      "Epoch 22 / 100 - Batch 1500 - Loss: 0.0002093704118465336\n",
      "Epoch 22 / 100 - Batch 2000 - Loss: 0.00022370963628072363\n",
      "Epoch 22 / 100 - Batch 2500 - Loss: 0.0002493835819392879\n",
      "Epoch 22 / 100 - Batch 3000 - Loss: 0.0002601578588679728\n",
      "Epoch 22 / 100 - Batch 3500 - Loss: 0.00030434282314353427\n",
      "Epoch 22 / 100 - Batch 4000 - Loss: 0.0002877141676757845\n",
      "Epoch 22 / 100 - Batch 4500 - Loss: 0.0003032499823751376\n",
      "Epoch 22 / 100 - Batch 5000 - Loss: 0.0003164895613187624\n",
      "Epoch 22 / 100 - Batch 5500 - Loss: 0.00030792228495099447\n",
      "Epoch 22 / 100 - Batch 6000 - Loss: 0.0003179945613772576\n",
      "Epoch 22 / 100 - Batch 6500 - Loss: 0.000342089304686623\n",
      "Epoch 23 / 100 - Batch 0 - Loss: 3.847561345082795e-07\n",
      "Epoch 23 / 100 - Batch 500 - Loss: 0.00023714336507493755\n",
      "Epoch 23 / 100 - Batch 1000 - Loss: 0.0002476423172128784\n",
      "Epoch 23 / 100 - Batch 1500 - Loss: 0.00024195935205831838\n",
      "Epoch 23 / 100 - Batch 2000 - Loss: 0.00026674846545075154\n",
      "Epoch 23 / 100 - Batch 2500 - Loss: 0.00031809043635543647\n",
      "Epoch 23 / 100 - Batch 3000 - Loss: 0.00029330892789983747\n",
      "Epoch 23 / 100 - Batch 3500 - Loss: 0.00035680186752472444\n",
      "Epoch 23 / 100 - Batch 4000 - Loss: 0.00034667919979133074\n",
      "Epoch 23 / 100 - Batch 4500 - Loss: 0.0003701099836187308\n",
      "Epoch 23 / 100 - Batch 5000 - Loss: 0.0003694145763675396\n",
      "Epoch 23 / 100 - Batch 5500 - Loss: 0.000353903743191872\n",
      "Epoch 23 / 100 - Batch 6000 - Loss: 0.0003435640436393817\n",
      "Epoch 23 / 100 - Batch 6500 - Loss: 0.00033275181186230156\n",
      "Epoch 24 / 100 - Batch 0 - Loss: 3.781856094065006e-06\n",
      "Epoch 24 / 100 - Batch 500 - Loss: 0.00018445662223752848\n",
      "Epoch 24 / 100 - Batch 1000 - Loss: 0.00023728493067089721\n",
      "Epoch 24 / 100 - Batch 1500 - Loss: 0.00019590564789493067\n",
      "Epoch 24 / 100 - Batch 2000 - Loss: 0.00029025975215802885\n",
      "Epoch 24 / 100 - Batch 2500 - Loss: 0.0003132137841694603\n",
      "Epoch 24 / 100 - Batch 3000 - Loss: 0.0003029058697140707\n",
      "Epoch 24 / 100 - Batch 3500 - Loss: 0.0003130469704812348\n",
      "Epoch 24 / 100 - Batch 4000 - Loss: 0.00033140471238515395\n",
      "Epoch 24 / 100 - Batch 4500 - Loss: 0.00032509061822302013\n",
      "Epoch 24 / 100 - Batch 5000 - Loss: 0.0003061725695887529\n",
      "Epoch 24 / 100 - Batch 5500 - Loss: 0.0002900378267635509\n",
      "Epoch 24 / 100 - Batch 6000 - Loss: 0.0002780118134010338\n",
      "Epoch 24 / 100 - Batch 6500 - Loss: 0.000286341711105498\n",
      "Epoch 25 / 100 - Batch 0 - Loss: 1.531605903437594e-06\n",
      "Epoch 25 / 100 - Batch 500 - Loss: 0.0001355066628891887\n",
      "Epoch 25 / 100 - Batch 1000 - Loss: 0.00022101255687654784\n",
      "Epoch 25 / 100 - Batch 1500 - Loss: 0.0002494602962174284\n",
      "Epoch 25 / 100 - Batch 2000 - Loss: 0.000375645172422073\n",
      "Epoch 25 / 100 - Batch 2500 - Loss: 0.0003449812088767126\n",
      "Epoch 25 / 100 - Batch 3000 - Loss: 0.0003209267692529459\n",
      "Epoch 25 / 100 - Batch 3500 - Loss: 0.00030588860881212434\n",
      "Epoch 25 / 100 - Batch 4000 - Loss: 0.00029864464832269944\n",
      "Epoch 25 / 100 - Batch 4500 - Loss: 0.0002951546984862847\n",
      "Epoch 25 / 100 - Batch 5000 - Loss: 0.00029293551475380336\n",
      "Epoch 25 / 100 - Batch 5500 - Loss: 0.0002908676371487896\n",
      "Epoch 25 / 100 - Batch 6000 - Loss: 0.00027240739507671126\n",
      "Epoch 25 / 100 - Batch 6500 - Loss: 0.0002935241088934897\n",
      "Epoch 26 / 100 - Batch 0 - Loss: 9.250844641428557e-07\n",
      "Epoch 26 / 100 - Batch 500 - Loss: 0.00011767641424104868\n",
      "Epoch 26 / 100 - Batch 1000 - Loss: 0.0003412729436743468\n",
      "Epoch 26 / 100 - Batch 1500 - Loss: 0.0003467250783679507\n",
      "Epoch 26 / 100 - Batch 2000 - Loss: 0.00035319736777221803\n",
      "Epoch 26 / 100 - Batch 2500 - Loss: 0.0003143266871762614\n",
      "Epoch 26 / 100 - Batch 3000 - Loss: 0.00028775461822831195\n",
      "Epoch 26 / 100 - Batch 3500 - Loss: 0.00035269733033232325\n",
      "Epoch 26 / 100 - Batch 4000 - Loss: 0.000329508641475432\n",
      "Epoch 26 / 100 - Batch 4500 - Loss: 0.0003047016393373075\n",
      "Epoch 26 / 100 - Batch 5000 - Loss: 0.0002916390245274443\n",
      "Epoch 26 / 100 - Batch 5500 - Loss: 0.000287611777956314\n",
      "Epoch 26 / 100 - Batch 6000 - Loss: 0.0002926475085013866\n",
      "Epoch 26 / 100 - Batch 6500 - Loss: 0.0003019234775007098\n",
      "Epoch 27 / 100 - Batch 0 - Loss: 2.1633244614349678e-05\n",
      "Epoch 27 / 100 - Batch 500 - Loss: 0.00012803722877796004\n",
      "Epoch 27 / 100 - Batch 1000 - Loss: 0.0002027612511227449\n",
      "Epoch 27 / 100 - Batch 1500 - Loss: 0.00023313382471963837\n",
      "Epoch 27 / 100 - Batch 2000 - Loss: 0.00024962243863550245\n",
      "Epoch 27 / 100 - Batch 2500 - Loss: 0.00022041565965871617\n",
      "Epoch 27 / 100 - Batch 3000 - Loss: 0.0002545654843223435\n",
      "Epoch 27 / 100 - Batch 3500 - Loss: 0.00030310766638851316\n",
      "Epoch 27 / 100 - Batch 4000 - Loss: 0.0002869587310192753\n",
      "Epoch 27 / 100 - Batch 4500 - Loss: 0.0002682774430442937\n",
      "Epoch 27 / 100 - Batch 5000 - Loss: 0.00025677040898968703\n",
      "Epoch 27 / 100 - Batch 5500 - Loss: 0.0002776032313268994\n",
      "Epoch 27 / 100 - Batch 6000 - Loss: 0.00026681185360855953\n",
      "Epoch 27 / 100 - Batch 6500 - Loss: 0.0002656310817224602\n",
      "Epoch 28 / 100 - Batch 0 - Loss: 1.0991068393195746e-06\n",
      "Epoch 28 / 100 - Batch 500 - Loss: 0.00023214744284897997\n",
      "Epoch 28 / 100 - Batch 1000 - Loss: 0.00021541991598701663\n",
      "Epoch 28 / 100 - Batch 1500 - Loss: 0.0001970174140325659\n",
      "Epoch 28 / 100 - Batch 2000 - Loss: 0.00018738921471342504\n",
      "Epoch 28 / 100 - Batch 2500 - Loss: 0.0001967168293762533\n",
      "Epoch 28 / 100 - Batch 3000 - Loss: 0.000184678819900268\n",
      "Epoch 28 / 100 - Batch 3500 - Loss: 0.00020753597885476\n",
      "Epoch 28 / 100 - Batch 4000 - Loss: 0.0002122627959976708\n",
      "Epoch 28 / 100 - Batch 4500 - Loss: 0.0001971981756638663\n",
      "Epoch 28 / 100 - Batch 5000 - Loss: 0.0001990442129640102\n",
      "Epoch 28 / 100 - Batch 5500 - Loss: 0.0001970528518560283\n",
      "Epoch 28 / 100 - Batch 6000 - Loss: 0.0001904271319153121\n",
      "Epoch 28 / 100 - Batch 6500 - Loss: 0.00018597522036759953\n",
      "Epoch 29 / 100 - Batch 0 - Loss: 1.858634846030327e-08\n",
      "Epoch 29 / 100 - Batch 500 - Loss: 0.00026812142495235457\n",
      "Epoch 29 / 100 - Batch 1000 - Loss: 0.00027759068696229746\n",
      "Epoch 29 / 100 - Batch 1500 - Loss: 0.00029594941700874134\n",
      "Epoch 29 / 100 - Batch 2000 - Loss: 0.0002472968153074992\n",
      "Epoch 29 / 100 - Batch 2500 - Loss: 0.0002617697194366534\n",
      "Epoch 29 / 100 - Batch 3000 - Loss: 0.00024289826889176572\n",
      "Epoch 29 / 100 - Batch 3500 - Loss: 0.00023041431395317676\n",
      "Epoch 29 / 100 - Batch 4000 - Loss: 0.00021844591858796367\n",
      "Epoch 29 / 100 - Batch 4500 - Loss: 0.0002023163899075083\n",
      "Epoch 29 / 100 - Batch 5000 - Loss: 0.00021342217801865332\n",
      "Epoch 29 / 100 - Batch 5500 - Loss: 0.00021457017643705252\n",
      "Epoch 29 / 100 - Batch 6000 - Loss: 0.00021048806140829226\n",
      "Epoch 29 / 100 - Batch 6500 - Loss: 0.0002002257481217889\n",
      "Epoch 30 / 100 - Batch 0 - Loss: 1.3245288243979303e-07\n",
      "Epoch 30 / 100 - Batch 500 - Loss: 0.0003062528634383914\n",
      "Epoch 30 / 100 - Batch 1000 - Loss: 0.00028280377179784816\n",
      "Epoch 30 / 100 - Batch 1500 - Loss: 0.00031240671986497274\n",
      "Epoch 30 / 100 - Batch 2000 - Loss: 0.000273897063388997\n",
      "Epoch 30 / 100 - Batch 2500 - Loss: 0.0003773283051460816\n",
      "Epoch 30 / 100 - Batch 3000 - Loss: 0.00033554549915296633\n",
      "Epoch 30 / 100 - Batch 3500 - Loss: 0.00030148202316467954\n",
      "Epoch 30 / 100 - Batch 4000 - Loss: 0.00027409815807204614\n",
      "Epoch 30 / 100 - Batch 4500 - Loss: 0.0002690404519145745\n",
      "Epoch 30 / 100 - Batch 5000 - Loss: 0.00029108569886442435\n",
      "Epoch 30 / 100 - Batch 5500 - Loss: 0.00027283250591372604\n",
      "Epoch 30 / 100 - Batch 6000 - Loss: 0.0002550135471647408\n",
      "Epoch 30 / 100 - Batch 6500 - Loss: 0.00024816793223064173\n",
      "Epoch 31 / 100 - Batch 0 - Loss: 2.0081831308971232e-08\n",
      "Epoch 31 / 100 - Batch 500 - Loss: 0.0002775504757985495\n",
      "Epoch 31 / 100 - Batch 1000 - Loss: 0.0003018293426922791\n",
      "Epoch 31 / 100 - Batch 1500 - Loss: 0.0002329489478083817\n",
      "Epoch 31 / 100 - Batch 2000 - Loss: 0.00020010563268477738\n",
      "Epoch 31 / 100 - Batch 2500 - Loss: 0.00020320121369960578\n",
      "Epoch 31 / 100 - Batch 3000 - Loss: 0.00019563694638593043\n",
      "Epoch 31 / 100 - Batch 3500 - Loss: 0.00018395874507100255\n",
      "Epoch 31 / 100 - Batch 4000 - Loss: 0.00017713918483084438\n",
      "Epoch 31 / 100 - Batch 4500 - Loss: 0.00016933334611265148\n",
      "Epoch 31 / 100 - Batch 5000 - Loss: 0.00016519665427012971\n",
      "Epoch 31 / 100 - Batch 5500 - Loss: 0.0001793220776942957\n",
      "Epoch 31 / 100 - Batch 6000 - Loss: 0.00017223380216767353\n",
      "Epoch 31 / 100 - Batch 6500 - Loss: 0.00016387006338896114\n",
      "Epoch 32 / 100 - Batch 0 - Loss: 3.888167299237466e-08\n",
      "Epoch 32 / 100 - Batch 500 - Loss: 0.00020712248794811574\n",
      "Epoch 32 / 100 - Batch 1000 - Loss: 0.00015632483085488286\n",
      "Epoch 32 / 100 - Batch 1500 - Loss: 0.00013528791778571388\n",
      "Epoch 32 / 100 - Batch 2000 - Loss: 0.00013238149688366447\n",
      "Epoch 32 / 100 - Batch 2500 - Loss: 0.00014057931972730398\n",
      "Epoch 32 / 100 - Batch 3000 - Loss: 0.0001375046422459124\n",
      "Epoch 32 / 100 - Batch 3500 - Loss: 0.00015335829892083416\n",
      "Epoch 32 / 100 - Batch 4000 - Loss: 0.0001721443346828615\n",
      "Epoch 32 / 100 - Batch 4500 - Loss: 0.000172277447269188\n",
      "Epoch 32 / 100 - Batch 5000 - Loss: 0.00017329305536758802\n",
      "Epoch 32 / 100 - Batch 5500 - Loss: 0.00017419230823927903\n",
      "Epoch 32 / 100 - Batch 6000 - Loss: 0.00016204157049803654\n",
      "Epoch 32 / 100 - Batch 6500 - Loss: 0.00016118822036949597\n",
      "Epoch 33 / 100 - Batch 0 - Loss: 9.334953938378021e-05\n",
      "Epoch 33 / 100 - Batch 500 - Loss: 0.00013373043980534797\n",
      "Epoch 33 / 100 - Batch 1000 - Loss: 0.00012249194984549988\n",
      "Epoch 33 / 100 - Batch 1500 - Loss: 0.00011880010627536076\n",
      "Epoch 33 / 100 - Batch 2000 - Loss: 0.00016027509800788003\n",
      "Epoch 33 / 100 - Batch 2500 - Loss: 0.000174393589897263\n",
      "Epoch 33 / 100 - Batch 3000 - Loss: 0.0001602452038728907\n",
      "Epoch 33 / 100 - Batch 3500 - Loss: 0.00017365701836084225\n",
      "Epoch 33 / 100 - Batch 4000 - Loss: 0.00017323557980644032\n",
      "Epoch 33 / 100 - Batch 4500 - Loss: 0.00016696111711990915\n",
      "Epoch 33 / 100 - Batch 5000 - Loss: 0.00017364526421446223\n",
      "Epoch 33 / 100 - Batch 5500 - Loss: 0.00016279307048126877\n",
      "Epoch 33 / 100 - Batch 6000 - Loss: 0.00015608277524161373\n",
      "Epoch 33 / 100 - Batch 6500 - Loss: 0.00018062186131404006\n",
      "Epoch 34 / 100 - Batch 0 - Loss: 1.0681831419390164e-08\n",
      "Epoch 34 / 100 - Batch 500 - Loss: 0.00013483770926021703\n",
      "Epoch 34 / 100 - Batch 1000 - Loss: 0.0001472982972828617\n",
      "Epoch 34 / 100 - Batch 1500 - Loss: 0.00013256033381394005\n",
      "Epoch 34 / 100 - Batch 2000 - Loss: 0.00017092379073779622\n",
      "Epoch 34 / 100 - Batch 2500 - Loss: 0.0001936078363808774\n",
      "Epoch 34 / 100 - Batch 3000 - Loss: 0.00019423470503262898\n",
      "Epoch 34 / 100 - Batch 3500 - Loss: 0.00021318115431112248\n",
      "Epoch 34 / 100 - Batch 4000 - Loss: 0.0001997810595270849\n",
      "Epoch 34 / 100 - Batch 4500 - Loss: 0.00018103730374727348\n",
      "Epoch 34 / 100 - Batch 5000 - Loss: 0.00018897889549650954\n",
      "Epoch 34 / 100 - Batch 5500 - Loss: 0.00019233225692633107\n",
      "Epoch 34 / 100 - Batch 6000 - Loss: 0.00018711569756280392\n",
      "Epoch 34 / 100 - Batch 6500 - Loss: 0.00018813961478585383\n",
      "Epoch 35 / 100 - Batch 0 - Loss: 1.2476321842314064e-07\n",
      "Epoch 35 / 100 - Batch 500 - Loss: 6.781650960503073e-05\n",
      "Epoch 35 / 100 - Batch 1000 - Loss: 9.863196993996583e-05\n",
      "Epoch 35 / 100 - Batch 1500 - Loss: 0.00010413121021179217\n",
      "Epoch 35 / 100 - Batch 2000 - Loss: 0.00018048223391210907\n",
      "Epoch 35 / 100 - Batch 2500 - Loss: 0.00017739999458291268\n",
      "Epoch 35 / 100 - Batch 3000 - Loss: 0.00017326087827072528\n",
      "Epoch 35 / 100 - Batch 3500 - Loss: 0.0002115112373571374\n",
      "Epoch 35 / 100 - Batch 4000 - Loss: 0.00019708848653933816\n",
      "Epoch 35 / 100 - Batch 4500 - Loss: 0.0001806653157906746\n",
      "Epoch 35 / 100 - Batch 5000 - Loss: 0.00017393807012606473\n",
      "Epoch 35 / 100 - Batch 5500 - Loss: 0.00018813411116998488\n",
      "Epoch 35 / 100 - Batch 6000 - Loss: 0.0001825946069380487\n",
      "Epoch 35 / 100 - Batch 6500 - Loss: 0.00017491531871992612\n",
      "Epoch 36 / 100 - Batch 0 - Loss: 1.773183377906662e-08\n",
      "Epoch 36 / 100 - Batch 500 - Loss: 7.518226738302098e-05\n",
      "Epoch 36 / 100 - Batch 1000 - Loss: 0.00012592848882104175\n",
      "Epoch 36 / 100 - Batch 1500 - Loss: 0.0001734158671241695\n",
      "Epoch 36 / 100 - Batch 2000 - Loss: 0.0001642007059585069\n",
      "Epoch 36 / 100 - Batch 2500 - Loss: 0.00017642184311318645\n",
      "Epoch 36 / 100 - Batch 3000 - Loss: 0.00015918181100598225\n",
      "Epoch 36 / 100 - Batch 3500 - Loss: 0.00015205952329992676\n",
      "Epoch 36 / 100 - Batch 4000 - Loss: 0.00015237498618210808\n",
      "Epoch 36 / 100 - Batch 4500 - Loss: 0.00015332438219071824\n",
      "Epoch 36 / 100 - Batch 5000 - Loss: 0.00014604278039475658\n",
      "Epoch 36 / 100 - Batch 5500 - Loss: 0.00014452859145460962\n",
      "Epoch 36 / 100 - Batch 6000 - Loss: 0.00014437397889481912\n",
      "Epoch 36 / 100 - Batch 6500 - Loss: 0.00014628156847679055\n",
      "Epoch 37 / 100 - Batch 0 - Loss: 8.54546822193214e-10\n",
      "Epoch 37 / 100 - Batch 500 - Loss: 0.00010327511003028022\n",
      "Epoch 37 / 100 - Batch 1000 - Loss: 0.0001236368522945971\n",
      "Epoch 37 / 100 - Batch 1500 - Loss: 0.00016994608104028595\n",
      "Epoch 37 / 100 - Batch 2000 - Loss: 0.00016792458139973474\n",
      "Epoch 37 / 100 - Batch 2500 - Loss: 0.00015868166062744466\n",
      "Epoch 37 / 100 - Batch 3000 - Loss: 0.00017015149309561784\n",
      "Epoch 37 / 100 - Batch 3500 - Loss: 0.00023158603811734653\n",
      "Epoch 37 / 100 - Batch 4000 - Loss: 0.00020812124983327334\n",
      "Epoch 37 / 100 - Batch 4500 - Loss: 0.00019011237957784435\n",
      "Epoch 37 / 100 - Batch 5000 - Loss: 0.00017420093645950378\n",
      "Epoch 37 / 100 - Batch 5500 - Loss: 0.00016129454097611422\n",
      "Epoch 37 / 100 - Batch 6000 - Loss: 0.00016055372832617367\n",
      "Epoch 37 / 100 - Batch 6500 - Loss: 0.00016189956525628003\n",
      "Epoch 38 / 100 - Batch 0 - Loss: 2.0636215936065128e-07\n",
      "Epoch 38 / 100 - Batch 500 - Loss: 0.0001551814194908906\n",
      "Epoch 38 / 100 - Batch 1000 - Loss: 0.0002291902301402752\n",
      "Epoch 38 / 100 - Batch 1500 - Loss: 0.00025002562808891147\n",
      "Epoch 38 / 100 - Batch 2000 - Loss: 0.0002211161232276344\n",
      "Epoch 38 / 100 - Batch 2500 - Loss: 0.0001994694479702536\n",
      "Epoch 38 / 100 - Batch 3000 - Loss: 0.00019750633259194383\n",
      "Epoch 38 / 100 - Batch 3500 - Loss: 0.00021016624477454145\n",
      "Epoch 38 / 100 - Batch 4000 - Loss: 0.00019716233139785024\n",
      "Epoch 38 / 100 - Batch 4500 - Loss: 0.00019011000058293356\n",
      "Epoch 38 / 100 - Batch 5000 - Loss: 0.00018116275231282137\n",
      "Epoch 38 / 100 - Batch 5500 - Loss: 0.00017502610895866663\n",
      "Epoch 38 / 100 - Batch 6000 - Loss: 0.00017976372722384272\n",
      "Epoch 38 / 100 - Batch 6500 - Loss: 0.00017731272094165292\n",
      "Epoch 39 / 100 - Batch 0 - Loss: 9.079473528572635e-08\n",
      "Epoch 39 / 100 - Batch 500 - Loss: 7.856747856573329e-05\n",
      "Epoch 39 / 100 - Batch 1000 - Loss: 9.411367955348336e-05\n",
      "Epoch 39 / 100 - Batch 1500 - Loss: 0.00012104546010582268\n",
      "Epoch 39 / 100 - Batch 2000 - Loss: 0.0001008176523610717\n",
      "Epoch 39 / 100 - Batch 2500 - Loss: 0.00011181424045165237\n",
      "Epoch 39 / 100 - Batch 3000 - Loss: 0.00021016713349447088\n",
      "Epoch 39 / 100 - Batch 3500 - Loss: 0.00020728391012400315\n",
      "Epoch 39 / 100 - Batch 4000 - Loss: 0.00019178417100160087\n",
      "Epoch 39 / 100 - Batch 4500 - Loss: 0.00017701217228256643\n",
      "Epoch 39 / 100 - Batch 5000 - Loss: 0.00017358302217417812\n",
      "Epoch 39 / 100 - Batch 5500 - Loss: 0.000181660184902136\n",
      "Epoch 39 / 100 - Batch 6000 - Loss: 0.00017264286640382415\n",
      "Epoch 39 / 100 - Batch 6500 - Loss: 0.00016358712987307367\n",
      "Epoch 40 / 100 - Batch 0 - Loss: 1.184172560897423e-06\n",
      "Epoch 40 / 100 - Batch 500 - Loss: 4.760545988110137e-05\n",
      "Epoch 40 / 100 - Batch 1000 - Loss: 5.876972057662642e-05\n",
      "Epoch 40 / 100 - Batch 1500 - Loss: 9.09878379133912e-05\n",
      "Epoch 40 / 100 - Batch 2000 - Loss: 0.00010757378765968839\n",
      "Epoch 40 / 100 - Batch 2500 - Loss: 0.0001153965219210181\n",
      "Epoch 40 / 100 - Batch 3000 - Loss: 0.00013332239447786632\n",
      "Epoch 40 / 100 - Batch 3500 - Loss: 0.00013594776895681321\n",
      "Epoch 40 / 100 - Batch 4000 - Loss: 0.0001368025976310012\n",
      "Epoch 40 / 100 - Batch 4500 - Loss: 0.00012497519985723188\n",
      "Epoch 40 / 100 - Batch 5000 - Loss: 0.00012987610764445757\n",
      "Epoch 40 / 100 - Batch 5500 - Loss: 0.0001271963913956697\n",
      "Epoch 40 / 100 - Batch 6000 - Loss: 0.00011786127522824322\n",
      "Epoch 40 / 100 - Batch 6500 - Loss: 0.00011417780174468059\n",
      "Epoch 41 / 100 - Batch 0 - Loss: 2.4354553929128997e-08\n",
      "Epoch 41 / 100 - Batch 500 - Loss: 0.00012173877401045651\n",
      "Epoch 41 / 100 - Batch 1000 - Loss: 0.00017986220941153594\n",
      "Epoch 41 / 100 - Batch 1500 - Loss: 0.00017357205469111563\n",
      "Epoch 41 / 100 - Batch 2000 - Loss: 0.00014970698047513423\n",
      "Epoch 41 / 100 - Batch 2500 - Loss: 0.00013009934205656407\n",
      "Epoch 41 / 100 - Batch 3000 - Loss: 0.00012279880339292722\n",
      "Epoch 41 / 100 - Batch 3500 - Loss: 0.00014347821077721653\n",
      "Epoch 41 / 100 - Batch 4000 - Loss: 0.00014942331244007233\n",
      "Epoch 41 / 100 - Batch 4500 - Loss: 0.00013727703659096173\n",
      "Epoch 41 / 100 - Batch 5000 - Loss: 0.00013438916815844255\n",
      "Epoch 41 / 100 - Batch 5500 - Loss: 0.00013163993089921572\n",
      "Epoch 41 / 100 - Batch 6000 - Loss: 0.00013746400364625852\n",
      "Epoch 41 / 100 - Batch 6500 - Loss: 0.00013094996036728932\n",
      "Epoch 42 / 100 - Batch 0 - Loss: 3.364677354511514e-07\n",
      "Epoch 42 / 100 - Batch 500 - Loss: 0.00011722268594618558\n",
      "Epoch 42 / 100 - Batch 1000 - Loss: 0.0001311713383677403\n",
      "Epoch 42 / 100 - Batch 1500 - Loss: 0.00011493621100640098\n",
      "Epoch 42 / 100 - Batch 2000 - Loss: 0.000212999913350213\n",
      "Epoch 42 / 100 - Batch 2500 - Loss: 0.00018960959348428101\n",
      "Epoch 42 / 100 - Batch 3000 - Loss: 0.00018747731965050409\n",
      "Epoch 42 / 100 - Batch 3500 - Loss: 0.00018006510937937313\n",
      "Epoch 42 / 100 - Batch 4000 - Loss: 0.00017424082813984228\n",
      "Epoch 42 / 100 - Batch 4500 - Loss: 0.00016898622945363923\n",
      "Epoch 42 / 100 - Batch 5000 - Loss: 0.0001683527122192078\n",
      "Epoch 42 / 100 - Batch 5500 - Loss: 0.00015973492326408145\n",
      "Epoch 42 / 100 - Batch 6000 - Loss: 0.0001984773536853311\n",
      "Epoch 42 / 100 - Batch 6500 - Loss: 0.00019266800612583422\n",
      "Epoch 43 / 100 - Batch 0 - Loss: 2.4845550683494366e-07\n",
      "Epoch 43 / 100 - Batch 500 - Loss: 0.00020941411562552917\n",
      "Epoch 43 / 100 - Batch 1000 - Loss: 0.00014010198637289358\n",
      "Epoch 43 / 100 - Batch 1500 - Loss: 0.00010401335870091762\n",
      "Epoch 43 / 100 - Batch 2000 - Loss: 0.00020626618441369876\n",
      "Epoch 43 / 100 - Batch 2500 - Loss: 0.0001791311037554362\n",
      "Epoch 43 / 100 - Batch 3000 - Loss: 0.00015565566197459565\n",
      "Epoch 43 / 100 - Batch 3500 - Loss: 0.00016939219239863558\n",
      "Epoch 43 / 100 - Batch 4000 - Loss: 0.0001645619046289544\n",
      "Epoch 43 / 100 - Batch 4500 - Loss: 0.00015230700824387272\n",
      "Epoch 43 / 100 - Batch 5000 - Loss: 0.00014959629474189538\n",
      "Epoch 43 / 100 - Batch 5500 - Loss: 0.00014585811814886633\n",
      "Epoch 43 / 100 - Batch 6000 - Loss: 0.00014080953426337273\n",
      "Epoch 43 / 100 - Batch 6500 - Loss: 0.00013671504637933387\n",
      "Epoch 44 / 100 - Batch 0 - Loss: 8.545465668419183e-09\n",
      "Epoch 44 / 100 - Batch 500 - Loss: 3.5307458298224524e-05\n",
      "Epoch 44 / 100 - Batch 1000 - Loss: 0.00011845869085304222\n",
      "Epoch 44 / 100 - Batch 1500 - Loss: 9.422511732223308e-05\n",
      "Epoch 44 / 100 - Batch 2000 - Loss: 0.00010137898743092016\n",
      "Epoch 44 / 100 - Batch 2500 - Loss: 9.905948128431455e-05\n",
      "Epoch 44 / 100 - Batch 3000 - Loss: 9.344730549956399e-05\n",
      "Epoch 44 / 100 - Batch 3500 - Loss: 0.00011524167722580754\n",
      "Epoch 44 / 100 - Batch 4000 - Loss: 0.0001298805305916563\n",
      "Epoch 44 / 100 - Batch 4500 - Loss: 0.00011864703016597526\n",
      "Epoch 44 / 100 - Batch 5000 - Loss: 0.00013327224882916868\n",
      "Epoch 44 / 100 - Batch 5500 - Loss: 0.00012666803288520617\n",
      "Epoch 44 / 100 - Batch 6000 - Loss: 0.00012056917179178192\n",
      "Epoch 44 / 100 - Batch 6500 - Loss: 0.00017511325100214988\n",
      "Epoch 45 / 100 - Batch 0 - Loss: 9.916785757013713e-07\n",
      "Epoch 45 / 100 - Batch 500 - Loss: 3.217015784974135e-05\n",
      "Epoch 45 / 100 - Batch 1000 - Loss: 5.865927774980767e-05\n",
      "Epoch 45 / 100 - Batch 1500 - Loss: 5.708794570233832e-05\n",
      "Epoch 45 / 100 - Batch 2000 - Loss: 9.993545276901707e-05\n",
      "Epoch 45 / 100 - Batch 2500 - Loss: 0.00013554232253122812\n",
      "Epoch 45 / 100 - Batch 3000 - Loss: 0.0001198639398485285\n",
      "Epoch 45 / 100 - Batch 3500 - Loss: 0.0001594924286992267\n",
      "Epoch 45 / 100 - Batch 4000 - Loss: 0.00015080885652249962\n",
      "Epoch 45 / 100 - Batch 4500 - Loss: 0.00014189259272608698\n",
      "Epoch 45 / 100 - Batch 5000 - Loss: 0.00014443326937134355\n",
      "Epoch 45 / 100 - Batch 5500 - Loss: 0.00014339883534092408\n",
      "Epoch 45 / 100 - Batch 6000 - Loss: 0.00013695688767316073\n",
      "Epoch 45 / 100 - Batch 6500 - Loss: 0.0001451583509411809\n",
      "Epoch 46 / 100 - Batch 0 - Loss: 3.915871786830394e-07\n",
      "Epoch 46 / 100 - Batch 500 - Loss: 1.493404456890162e-05\n",
      "Epoch 46 / 100 - Batch 1000 - Loss: 7.438337430177043e-05\n",
      "Epoch 46 / 100 - Batch 1500 - Loss: 7.380403620416648e-05\n",
      "Epoch 46 / 100 - Batch 2000 - Loss: 9.652034504917858e-05\n",
      "Epoch 46 / 100 - Batch 2500 - Loss: 0.00010563420667513905\n",
      "Epoch 46 / 100 - Batch 3000 - Loss: 9.261883224765245e-05\n",
      "Epoch 46 / 100 - Batch 3500 - Loss: 9.482508062000167e-05\n",
      "Epoch 46 / 100 - Batch 4000 - Loss: 0.0001038339782192264\n",
      "Epoch 46 / 100 - Batch 4500 - Loss: 0.00012315772743226128\n",
      "Epoch 46 / 100 - Batch 5000 - Loss: 0.00011778244135597335\n",
      "Epoch 46 / 100 - Batch 5500 - Loss: 0.00013286722941183127\n",
      "Epoch 46 / 100 - Batch 6000 - Loss: 0.00012590759006025608\n",
      "Epoch 46 / 100 - Batch 6500 - Loss: 0.0001278711250395799\n",
      "Epoch 47 / 100 - Batch 0 - Loss: 8.545467666820628e-10\n",
      "Epoch 47 / 100 - Batch 500 - Loss: 0.0002872152887473791\n",
      "Epoch 47 / 100 - Batch 1000 - Loss: 0.0002567728919646928\n",
      "Epoch 47 / 100 - Batch 1500 - Loss: 0.0002346403871226794\n",
      "Epoch 47 / 100 - Batch 2000 - Loss: 0.00019417671280177033\n",
      "Epoch 47 / 100 - Batch 2500 - Loss: 0.00018142190544499868\n",
      "Epoch 47 / 100 - Batch 3000 - Loss: 0.00015886219025569438\n",
      "Epoch 47 / 100 - Batch 3500 - Loss: 0.00017415190379854674\n",
      "Epoch 47 / 100 - Batch 4000 - Loss: 0.0001640560603968512\n",
      "Epoch 47 / 100 - Batch 4500 - Loss: 0.0001520159893869393\n",
      "Epoch 47 / 100 - Batch 5000 - Loss: 0.00014770198214970236\n",
      "Epoch 47 / 100 - Batch 5500 - Loss: 0.0001458531221429845\n",
      "Epoch 47 / 100 - Batch 6000 - Loss: 0.00013849400954341003\n",
      "Epoch 47 / 100 - Batch 6500 - Loss: 0.00013717078524668475\n",
      "Epoch 48 / 100 - Batch 0 - Loss: 5.340916153784292e-09\n",
      "Epoch 48 / 100 - Batch 500 - Loss: 0.00010131389547540646\n",
      "Epoch 48 / 100 - Batch 1000 - Loss: 8.080694034030043e-05\n",
      "Epoch 48 / 100 - Batch 1500 - Loss: 8.197029702571517e-05\n",
      "Epoch 48 / 100 - Batch 2000 - Loss: 7.33851656090083e-05\n",
      "Epoch 48 / 100 - Batch 2500 - Loss: 6.601065426412678e-05\n",
      "Epoch 48 / 100 - Batch 3000 - Loss: 5.677724807348584e-05\n",
      "Epoch 48 / 100 - Batch 3500 - Loss: 5.415173636405834e-05\n",
      "Epoch 48 / 100 - Batch 4000 - Loss: 7.904957124446407e-05\n",
      "Epoch 48 / 100 - Batch 4500 - Loss: 8.289075908051245e-05\n",
      "Epoch 48 / 100 - Batch 5000 - Loss: 0.00011238768924793528\n",
      "Epoch 48 / 100 - Batch 5500 - Loss: 0.00011098196515160492\n",
      "Epoch 48 / 100 - Batch 6000 - Loss: 0.00010437501735244273\n",
      "Epoch 48 / 100 - Batch 6500 - Loss: 0.00010991320758422656\n",
      "Epoch 49 / 100 - Batch 0 - Loss: 2.2004556399224384e-08\n",
      "Epoch 49 / 100 - Batch 500 - Loss: 3.835318556672417e-05\n",
      "Epoch 49 / 100 - Batch 1000 - Loss: 0.000150989082186965\n",
      "Epoch 49 / 100 - Batch 1500 - Loss: 0.0001313326525947935\n",
      "Epoch 49 / 100 - Batch 2000 - Loss: 0.000103519968135802\n",
      "Epoch 49 / 100 - Batch 2500 - Loss: 0.00011758943545696084\n",
      "Epoch 49 / 100 - Batch 3000 - Loss: 0.00011655634630780832\n",
      "Epoch 49 / 100 - Batch 3500 - Loss: 0.000112962288566342\n",
      "Epoch 49 / 100 - Batch 4000 - Loss: 0.00010350410575276609\n",
      "Epoch 49 / 100 - Batch 4500 - Loss: 0.00010626785984194353\n",
      "Epoch 49 / 100 - Batch 5000 - Loss: 0.00010190960736973483\n",
      "Epoch 49 / 100 - Batch 5500 - Loss: 9.571657219161892e-05\n",
      "Epoch 49 / 100 - Batch 6000 - Loss: 0.00010095818238895782\n",
      "Epoch 49 / 100 - Batch 6500 - Loss: 0.00010896247133397449\n",
      "Epoch 50 / 100 - Batch 0 - Loss: 2.179090685672236e-08\n",
      "Epoch 50 / 100 - Batch 500 - Loss: 5.464718642826325e-05\n",
      "Epoch 50 / 100 - Batch 1000 - Loss: 4.344943174926913e-05\n",
      "Epoch 50 / 100 - Batch 1500 - Loss: 4.202347269669035e-05\n",
      "Epoch 50 / 100 - Batch 2000 - Loss: 4.206423551708433e-05\n",
      "Epoch 50 / 100 - Batch 2500 - Loss: 6.86811502475827e-05\n",
      "Epoch 50 / 100 - Batch 3000 - Loss: 6.85718052285968e-05\n",
      "Epoch 50 / 100 - Batch 3500 - Loss: 0.00011344442493759201\n",
      "Epoch 50 / 100 - Batch 4000 - Loss: 0.00011375408159624644\n",
      "Epoch 50 / 100 - Batch 4500 - Loss: 0.00015283281916506593\n",
      "Epoch 50 / 100 - Batch 5000 - Loss: 0.00014274892260029534\n",
      "Epoch 50 / 100 - Batch 5500 - Loss: 0.0001328779654983792\n",
      "Epoch 50 / 100 - Batch 6000 - Loss: 0.00013888424921112248\n",
      "Epoch 50 / 100 - Batch 6500 - Loss: 0.00016394823782627936\n",
      "Epoch 51 / 100 - Batch 0 - Loss: 4.700006606128682e-09\n",
      "Epoch 51 / 100 - Batch 500 - Loss: 1.7379985607852597e-05\n",
      "Epoch 51 / 100 - Batch 1000 - Loss: 0.00011701318650838987\n",
      "Epoch 51 / 100 - Batch 1500 - Loss: 0.0001091674011238971\n",
      "Epoch 51 / 100 - Batch 2000 - Loss: 0.0001054647018039546\n",
      "Epoch 51 / 100 - Batch 2500 - Loss: 0.00010235199886300322\n",
      "Epoch 51 / 100 - Batch 3000 - Loss: 0.00012414348561322873\n",
      "Epoch 51 / 100 - Batch 3500 - Loss: 0.00025686812458277194\n",
      "Epoch 51 / 100 - Batch 4000 - Loss: 0.0002302720816653876\n",
      "Epoch 51 / 100 - Batch 4500 - Loss: 0.00021857051640138765\n",
      "Epoch 51 / 100 - Batch 5000 - Loss: 0.00020359148167464492\n",
      "Epoch 51 / 100 - Batch 5500 - Loss: 0.00019122788999692753\n",
      "Epoch 51 / 100 - Batch 6000 - Loss: 0.00018093097396176572\n",
      "Epoch 51 / 100 - Batch 6500 - Loss: 0.0001782028486490338\n",
      "Epoch 52 / 100 - Batch 0 - Loss: 2.136367055483035e-10\n",
      "Epoch 52 / 100 - Batch 500 - Loss: 1.8342947576542326e-05\n",
      "Epoch 52 / 100 - Batch 1000 - Loss: 2.6779843032697876e-05\n",
      "Epoch 52 / 100 - Batch 1500 - Loss: 4.725591432423356e-05\n",
      "Epoch 52 / 100 - Batch 2000 - Loss: 9.203170936994863e-05\n",
      "Epoch 52 / 100 - Batch 2500 - Loss: 0.0001415622959162308\n",
      "Epoch 52 / 100 - Batch 3000 - Loss: 0.0001283973443434068\n",
      "Epoch 52 / 100 - Batch 3500 - Loss: 0.0003365389166310891\n",
      "Epoch 52 / 100 - Batch 4000 - Loss: 0.0003087946708696681\n",
      "Epoch 52 / 100 - Batch 4500 - Loss: 0.00027565725951512907\n",
      "Epoch 52 / 100 - Batch 5000 - Loss: 0.0002513131378215874\n",
      "Epoch 52 / 100 - Batch 5500 - Loss: 0.0002356075070132647\n",
      "Epoch 52 / 100 - Batch 6000 - Loss: 0.00021742386412757495\n",
      "Epoch 52 / 100 - Batch 6500 - Loss: 0.00020164959349064121\n",
      "Epoch 53 / 100 - Batch 0 - Loss: 8.54546822193214e-10\n",
      "Epoch 53 / 100 - Batch 500 - Loss: 4.333169381214e-05\n",
      "Epoch 53 / 100 - Batch 1000 - Loss: 4.986116397436894e-05\n",
      "Epoch 53 / 100 - Batch 1500 - Loss: 8.205734831516578e-05\n",
      "Epoch 53 / 100 - Batch 2000 - Loss: 0.00010508150975125888\n",
      "Epoch 53 / 100 - Batch 2500 - Loss: 8.99190672847373e-05\n",
      "Epoch 53 / 100 - Batch 3000 - Loss: 8.213028898286729e-05\n",
      "Epoch 53 / 100 - Batch 3500 - Loss: 0.00017189750733811763\n",
      "Epoch 53 / 100 - Batch 4000 - Loss: 0.00017158282936758948\n",
      "Epoch 53 / 100 - Batch 4500 - Loss: 0.00015601490304376954\n",
      "Epoch 53 / 100 - Batch 5000 - Loss: 0.0001409547468016678\n",
      "Epoch 53 / 100 - Batch 5500 - Loss: 0.00013094933661246235\n",
      "Epoch 53 / 100 - Batch 6000 - Loss: 0.00012230926972944424\n",
      "Epoch 53 / 100 - Batch 6500 - Loss: 0.00012359895914477968\n",
      "Epoch 54 / 100 - Batch 0 - Loss: 1.4954566474045805e-09\n",
      "Epoch 54 / 100 - Batch 500 - Loss: 4.417206421617959e-05\n",
      "Epoch 54 / 100 - Batch 1000 - Loss: 0.00032698141910214297\n",
      "Epoch 54 / 100 - Batch 1500 - Loss: 0.00024347460361509124\n",
      "Epoch 54 / 100 - Batch 2000 - Loss: 0.00019587933331211783\n",
      "Epoch 54 / 100 - Batch 2500 - Loss: 0.00017298196839260032\n",
      "Epoch 54 / 100 - Batch 3000 - Loss: 0.0001492768619356506\n",
      "Epoch 54 / 100 - Batch 3500 - Loss: 0.0001318136690237349\n",
      "Epoch 54 / 100 - Batch 4000 - Loss: 0.00011624127591541964\n",
      "Epoch 54 / 100 - Batch 4500 - Loss: 0.00011056497105274037\n",
      "Epoch 54 / 100 - Batch 5000 - Loss: 9.975124093788565e-05\n",
      "Epoch 54 / 100 - Batch 5500 - Loss: 9.401255995024357e-05\n",
      "Epoch 54 / 100 - Batch 6000 - Loss: 9.478775401598462e-05\n",
      "Epoch 54 / 100 - Batch 6500 - Loss: 0.00010721809389881219\n",
      "Epoch 55 / 100 - Batch 0 - Loss: 0.0\n",
      "Epoch 55 / 100 - Batch 500 - Loss: 9.775577007816974e-05\n",
      "Epoch 55 / 100 - Batch 1000 - Loss: 0.00012766620799713681\n",
      "Epoch 55 / 100 - Batch 1500 - Loss: 0.00011564080019170996\n",
      "Epoch 55 / 100 - Batch 2000 - Loss: 0.00011283704565344312\n",
      "Epoch 55 / 100 - Batch 2500 - Loss: 0.00010153084806889499\n",
      "Epoch 55 / 100 - Batch 3000 - Loss: 8.691569906619522e-05\n",
      "Epoch 55 / 100 - Batch 3500 - Loss: 8.29105484466231e-05\n",
      "Epoch 55 / 100 - Batch 4000 - Loss: 7.325707573099157e-05\n",
      "Epoch 55 / 100 - Batch 4500 - Loss: 6.876876256560904e-05\n",
      "Epoch 55 / 100 - Batch 5000 - Loss: 6.893595152586013e-05\n",
      "Epoch 55 / 100 - Batch 5500 - Loss: 7.014607637945009e-05\n",
      "Epoch 55 / 100 - Batch 6000 - Loss: 6.705368615555735e-05\n",
      "Epoch 55 / 100 - Batch 6500 - Loss: 8.531526276939688e-05\n",
      "Epoch 56 / 100 - Batch 0 - Loss: 2.0444609560854587e-07\n",
      "Epoch 56 / 100 - Batch 500 - Loss: 0.00015326438904191166\n",
      "Epoch 56 / 100 - Batch 1000 - Loss: 0.00010359956270750561\n",
      "Epoch 56 / 100 - Batch 1500 - Loss: 0.00010006356357667005\n",
      "Epoch 56 / 100 - Batch 2000 - Loss: 0.00011119618810298268\n",
      "Epoch 56 / 100 - Batch 2500 - Loss: 0.0001086684286208078\n",
      "Epoch 56 / 100 - Batch 3000 - Loss: 0.00011636265876808298\n",
      "Epoch 56 / 100 - Batch 3500 - Loss: 0.00011475773297355277\n",
      "Epoch 56 / 100 - Batch 4000 - Loss: 0.00011408062560976787\n",
      "Epoch 56 / 100 - Batch 4500 - Loss: 0.00010242272986238326\n",
      "Epoch 56 / 100 - Batch 5000 - Loss: 0.00014245062017260646\n",
      "Epoch 56 / 100 - Batch 5500 - Loss: 0.00013249487912238945\n",
      "Epoch 56 / 100 - Batch 6000 - Loss: 0.00012558699470747298\n",
      "Epoch 56 / 100 - Batch 6500 - Loss: 0.00011867797511900043\n",
      "Epoch 57 / 100 - Batch 0 - Loss: 1.2818202055342454e-09\n",
      "Epoch 57 / 100 - Batch 500 - Loss: 6.71888860296879e-05\n",
      "Epoch 57 / 100 - Batch 1000 - Loss: 4.523670631790614e-05\n",
      "Epoch 57 / 100 - Batch 1500 - Loss: 4.566299957813291e-05\n",
      "Epoch 57 / 100 - Batch 2000 - Loss: 5.792816049374808e-05\n",
      "Epoch 57 / 100 - Batch 2500 - Loss: 8.915064052140326e-05\n",
      "Epoch 57 / 100 - Batch 3000 - Loss: 0.00013282698167019127\n",
      "Epoch 57 / 100 - Batch 3500 - Loss: 0.0001718338660457647\n",
      "Epoch 57 / 100 - Batch 4000 - Loss: 0.0001552253825761495\n",
      "Epoch 57 / 100 - Batch 4500 - Loss: 0.0001426403914083787\n",
      "Epoch 57 / 100 - Batch 5000 - Loss: 0.00013340309770852696\n",
      "Epoch 57 / 100 - Batch 5500 - Loss: 0.00012190840495254762\n",
      "Epoch 57 / 100 - Batch 6000 - Loss: 0.00011533180473250018\n",
      "Epoch 57 / 100 - Batch 6500 - Loss: 0.0001382377701202489\n",
      "Epoch 58 / 100 - Batch 0 - Loss: 1.0681835416193053e-09\n",
      "Epoch 58 / 100 - Batch 500 - Loss: 3.6739085696139206e-05\n",
      "Epoch 58 / 100 - Batch 1000 - Loss: 3.318989424640299e-05\n",
      "Epoch 58 / 100 - Batch 1500 - Loss: 5.3616579918013245e-05\n",
      "Epoch 58 / 100 - Batch 2000 - Loss: 4.955758782015496e-05\n",
      "Epoch 58 / 100 - Batch 2500 - Loss: 0.00013233482640503938\n",
      "Epoch 58 / 100 - Batch 3000 - Loss: 0.00012189665117396558\n",
      "Epoch 58 / 100 - Batch 3500 - Loss: 0.00011107928572235215\n",
      "Epoch 58 / 100 - Batch 4000 - Loss: 0.00011999201340494808\n",
      "Epoch 58 / 100 - Batch 4500 - Loss: 0.00010786069350797075\n",
      "Epoch 58 / 100 - Batch 5000 - Loss: 0.0001075518075008163\n",
      "Epoch 58 / 100 - Batch 5500 - Loss: 0.00010681023360519354\n",
      "Epoch 58 / 100 - Batch 6000 - Loss: 0.00010009340958750431\n",
      "Epoch 58 / 100 - Batch 6500 - Loss: 0.00010089061328305858\n",
      "Epoch 59 / 100 - Batch 0 - Loss: 9.677704326804815e-08\n",
      "Epoch 59 / 100 - Batch 500 - Loss: 0.00010799583360636688\n",
      "Epoch 59 / 100 - Batch 1000 - Loss: 0.00010120457246085991\n",
      "Epoch 59 / 100 - Batch 1500 - Loss: 7.37458391139406e-05\n",
      "Epoch 59 / 100 - Batch 2000 - Loss: 9.590349713162648e-05\n",
      "Epoch 59 / 100 - Batch 2500 - Loss: 0.00011347028718769322\n",
      "Epoch 59 / 100 - Batch 3000 - Loss: 0.00010443216259011206\n",
      "Epoch 59 / 100 - Batch 3500 - Loss: 9.265316285111581e-05\n",
      "Epoch 59 / 100 - Batch 4000 - Loss: 9.836592697937229e-05\n",
      "Epoch 59 / 100 - Batch 4500 - Loss: 8.865615321244626e-05\n",
      "Epoch 59 / 100 - Batch 5000 - Loss: 9.436961192673124e-05\n",
      "Epoch 59 / 100 - Batch 5500 - Loss: 9.697910474201384e-05\n",
      "Epoch 59 / 100 - Batch 6000 - Loss: 9.063353575644128e-05\n",
      "Epoch 59 / 100 - Batch 6500 - Loss: 8.522821568590416e-05\n",
      "Epoch 60 / 100 - Batch 0 - Loss: 7.071781055856263e-06\n",
      "Epoch 60 / 100 - Batch 500 - Loss: 0.0002564627446891852\n",
      "Epoch 60 / 100 - Batch 1000 - Loss: 0.0001807361595980267\n",
      "Epoch 60 / 100 - Batch 1500 - Loss: 0.00012681265311389092\n",
      "Epoch 60 / 100 - Batch 2000 - Loss: 0.00010176147339680356\n",
      "Epoch 60 / 100 - Batch 2500 - Loss: 8.568730028446172e-05\n",
      "Epoch 60 / 100 - Batch 3000 - Loss: 0.00010511631539336453\n",
      "Epoch 60 / 100 - Batch 3500 - Loss: 9.250859099677557e-05\n",
      "Epoch 60 / 100 - Batch 4000 - Loss: 8.223739105560338e-05\n",
      "Epoch 60 / 100 - Batch 4500 - Loss: 8.05387133860243e-05\n",
      "Epoch 60 / 100 - Batch 5000 - Loss: 8.15706321544387e-05\n",
      "Epoch 60 / 100 - Batch 5500 - Loss: 8.176784056057009e-05\n",
      "Epoch 60 / 100 - Batch 6000 - Loss: 8.860779185496407e-05\n",
      "Epoch 60 / 100 - Batch 6500 - Loss: 8.50075468137415e-05\n",
      "Epoch 61 / 100 - Batch 0 - Loss: 1.4954566474045805e-09\n",
      "Epoch 61 / 100 - Batch 500 - Loss: 4.4060672228453e-05\n",
      "Epoch 61 / 100 - Batch 1000 - Loss: 8.568874195492743e-05\n",
      "Epoch 61 / 100 - Batch 1500 - Loss: 7.024174837641438e-05\n",
      "Epoch 61 / 100 - Batch 2000 - Loss: 5.771996604308297e-05\n",
      "Epoch 61 / 100 - Batch 2500 - Loss: 4.859479069287022e-05\n",
      "Epoch 61 / 100 - Batch 3000 - Loss: 4.1926458112891444e-05\n",
      "Epoch 61 / 100 - Batch 3500 - Loss: 4.1599820414576064e-05\n",
      "Epoch 61 / 100 - Batch 4000 - Loss: 7.215646215562549e-05\n",
      "Epoch 61 / 100 - Batch 4500 - Loss: 7.684306773500976e-05\n",
      "Epoch 61 / 100 - Batch 5000 - Loss: 7.063541801869949e-05\n",
      "Epoch 61 / 100 - Batch 5500 - Loss: 7.312943289330857e-05\n",
      "Epoch 61 / 100 - Batch 6000 - Loss: 7.591066274328718e-05\n",
      "Epoch 61 / 100 - Batch 6500 - Loss: 7.19978221063836e-05\n",
      "Epoch 62 / 100 - Batch 0 - Loss: 0.0\n",
      "Epoch 62 / 100 - Batch 500 - Loss: 1.112315435567496e-05\n",
      "Epoch 62 / 100 - Batch 1000 - Loss: 7.301229954857769e-05\n",
      "Epoch 62 / 100 - Batch 1500 - Loss: 0.0001222258022153125\n",
      "Epoch 62 / 100 - Batch 2000 - Loss: 0.0001049469582396335\n",
      "Epoch 62 / 100 - Batch 2500 - Loss: 9.021750930141511e-05\n",
      "Epoch 62 / 100 - Batch 3000 - Loss: 0.00010547275961408028\n",
      "Epoch 62 / 100 - Batch 3500 - Loss: 9.500531221222397e-05\n",
      "Epoch 62 / 100 - Batch 4000 - Loss: 0.00011631105629587689\n",
      "Epoch 62 / 100 - Batch 4500 - Loss: 0.00012093191453388211\n",
      "Epoch 62 / 100 - Batch 5000 - Loss: 0.0001151358489251608\n",
      "Epoch 62 / 100 - Batch 5500 - Loss: 0.00010757648880678136\n",
      "Epoch 62 / 100 - Batch 6000 - Loss: 9.906585100438365e-05\n",
      "Epoch 62 / 100 - Batch 6500 - Loss: 9.571631514016544e-05\n",
      "Epoch 63 / 100 - Batch 0 - Loss: 2.777276630894221e-09\n",
      "Epoch 63 / 100 - Batch 500 - Loss: 8.483137233135723e-05\n",
      "Epoch 63 / 100 - Batch 1000 - Loss: 9.307241361329526e-05\n",
      "Epoch 63 / 100 - Batch 1500 - Loss: 7.135818476391653e-05\n",
      "Epoch 63 / 100 - Batch 2000 - Loss: 8.96630375413436e-05\n",
      "Epoch 63 / 100 - Batch 2500 - Loss: 9.534608431138423e-05\n",
      "Epoch 63 / 100 - Batch 3000 - Loss: 9.777921198996164e-05\n",
      "Epoch 63 / 100 - Batch 3500 - Loss: 9.251982669226457e-05\n",
      "Epoch 63 / 100 - Batch 4000 - Loss: 0.00010174371446888591\n",
      "Epoch 63 / 100 - Batch 4500 - Loss: 0.00010369042674907704\n",
      "Epoch 63 / 100 - Batch 5000 - Loss: 0.00011759995650328292\n",
      "Epoch 63 / 100 - Batch 5500 - Loss: 0.00011196470536945017\n",
      "Epoch 63 / 100 - Batch 6000 - Loss: 0.0001037536466949963\n",
      "Epoch 63 / 100 - Batch 6500 - Loss: 0.00010201604333420053\n",
      "Epoch 64 / 100 - Batch 0 - Loss: 0.0\n",
      "Epoch 64 / 100 - Batch 500 - Loss: 0.00011114714904314492\n",
      "Epoch 64 / 100 - Batch 1000 - Loss: 6.833696469326825e-05\n",
      "Epoch 64 / 100 - Batch 1500 - Loss: 4.9512540517418756e-05\n",
      "Epoch 64 / 100 - Batch 2000 - Loss: 7.849025973793697e-05\n",
      "Epoch 64 / 100 - Batch 2500 - Loss: 7.787323368021654e-05\n",
      "Epoch 64 / 100 - Batch 3000 - Loss: 7.295195003195503e-05\n",
      "Epoch 64 / 100 - Batch 3500 - Loss: 0.00010335353218873836\n",
      "Epoch 64 / 100 - Batch 4000 - Loss: 0.00017754352544194693\n",
      "Epoch 64 / 100 - Batch 4500 - Loss: 0.00016180723887357847\n",
      "Epoch 64 / 100 - Batch 5000 - Loss: 0.0001507006457615385\n",
      "Epoch 64 / 100 - Batch 5500 - Loss: 0.00014100393709118154\n",
      "Epoch 64 / 100 - Batch 6000 - Loss: 0.00013080712360045635\n",
      "Epoch 64 / 100 - Batch 6500 - Loss: 0.00012111429281309341\n",
      "Epoch 65 / 100 - Batch 0 - Loss: 0.0\n",
      "Epoch 65 / 100 - Batch 500 - Loss: 9.152239797988415e-05\n",
      "Epoch 65 / 100 - Batch 1000 - Loss: 5.657551689458544e-05\n",
      "Epoch 65 / 100 - Batch 1500 - Loss: 3.958166329895874e-05\n",
      "Epoch 65 / 100 - Batch 2000 - Loss: 6.509097679133934e-05\n",
      "Epoch 65 / 100 - Batch 2500 - Loss: 9.141795356785727e-05\n",
      "Epoch 65 / 100 - Batch 3000 - Loss: 7.895619741234203e-05\n",
      "Epoch 65 / 100 - Batch 3500 - Loss: 7.686909628576316e-05\n",
      "Epoch 65 / 100 - Batch 4000 - Loss: 7.009167222739075e-05\n",
      "Epoch 65 / 100 - Batch 4500 - Loss: 7.93875824769126e-05\n",
      "Epoch 65 / 100 - Batch 5000 - Loss: 7.393389837383236e-05\n",
      "Epoch 65 / 100 - Batch 5500 - Loss: 6.851066526665922e-05\n",
      "Epoch 65 / 100 - Batch 6000 - Loss: 6.820043534970379e-05\n",
      "Epoch 65 / 100 - Batch 6500 - Loss: 6.356292808446723e-05\n",
      "Epoch 66 / 100 - Batch 0 - Loss: 6.409100472559714e-10\n",
      "Epoch 66 / 100 - Batch 500 - Loss: 0.00013845267539169812\n",
      "Epoch 66 / 100 - Batch 1000 - Loss: 0.00020493215764094924\n",
      "Epoch 66 / 100 - Batch 1500 - Loss: 0.00014994420320247947\n",
      "Epoch 66 / 100 - Batch 2000 - Loss: 0.00012765548616030194\n",
      "Epoch 66 / 100 - Batch 2500 - Loss: 0.00011566648939264194\n",
      "Epoch 66 / 100 - Batch 3000 - Loss: 9.711472006386644e-05\n",
      "Epoch 66 / 100 - Batch 3500 - Loss: 0.0001796123047876976\n",
      "Epoch 66 / 100 - Batch 4000 - Loss: 0.00016303814903351088\n",
      "Epoch 66 / 100 - Batch 4500 - Loss: 0.00015542945034918704\n",
      "Epoch 66 / 100 - Batch 5000 - Loss: 0.00015763342558940225\n",
      "Epoch 66 / 100 - Batch 5500 - Loss: 0.0001492660955012413\n",
      "Epoch 66 / 100 - Batch 6000 - Loss: 0.00014380179062235037\n",
      "Epoch 66 / 100 - Batch 6500 - Loss: 0.0001357007289169273\n",
      "Epoch 67 / 100 - Batch 0 - Loss: 0.0\n",
      "Epoch 67 / 100 - Batch 500 - Loss: 0.00016600319988782395\n",
      "Epoch 67 / 100 - Batch 1000 - Loss: 0.0001217333662308434\n",
      "Epoch 67 / 100 - Batch 1500 - Loss: 9.994300342884808e-05\n",
      "Epoch 67 / 100 - Batch 2000 - Loss: 9.014909589239576e-05\n",
      "Epoch 67 / 100 - Batch 2500 - Loss: 7.330111005006327e-05\n",
      "Epoch 67 / 100 - Batch 3000 - Loss: 6.725352406918748e-05\n",
      "Epoch 67 / 100 - Batch 3500 - Loss: 0.00010064517813168728\n",
      "Epoch 67 / 100 - Batch 4000 - Loss: 9.161992780144474e-05\n",
      "Epoch 67 / 100 - Batch 4500 - Loss: 8.851791724981845e-05\n",
      "Epoch 67 / 100 - Batch 5000 - Loss: 8.575668432494545e-05\n",
      "Epoch 67 / 100 - Batch 5500 - Loss: 0.000107193951324067\n",
      "Epoch 67 / 100 - Batch 6000 - Loss: 0.00011497780613024575\n",
      "Epoch 67 / 100 - Batch 6500 - Loss: 0.0001120875482843249\n",
      "Epoch 68 / 100 - Batch 0 - Loss: 0.0\n",
      "Epoch 68 / 100 - Batch 500 - Loss: 3.443148790972834e-05\n",
      "Epoch 68 / 100 - Batch 1000 - Loss: 2.68410191658921e-05\n",
      "Epoch 68 / 100 - Batch 1500 - Loss: 2.9113049464547334e-05\n",
      "Epoch 68 / 100 - Batch 2000 - Loss: 2.488452618994072e-05\n",
      "Epoch 68 / 100 - Batch 2500 - Loss: 9.109800873191747e-05\n",
      "Epoch 68 / 100 - Batch 3000 - Loss: 9.667921453300785e-05\n",
      "Epoch 68 / 100 - Batch 3500 - Loss: 0.0001274689961032803\n",
      "Epoch 68 / 100 - Batch 4000 - Loss: 0.00011752263475445893\n",
      "Epoch 68 / 100 - Batch 4500 - Loss: 0.00010752191143332374\n",
      "Epoch 68 / 100 - Batch 5000 - Loss: 0.0001022442854199009\n",
      "Epoch 68 / 100 - Batch 5500 - Loss: 9.627565481853185e-05\n",
      "Epoch 68 / 100 - Batch 6000 - Loss: 0.00012501691202337263\n",
      "Epoch 68 / 100 - Batch 6500 - Loss: 0.00012778149152642244\n",
      "Epoch 69 / 100 - Batch 0 - Loss: 0.0\n",
      "Epoch 69 / 100 - Batch 500 - Loss: 7.067026376635651e-05\n",
      "Epoch 69 / 100 - Batch 1000 - Loss: 7.670223720308307e-05\n",
      "Epoch 69 / 100 - Batch 1500 - Loss: 5.9810601477078814e-05\n",
      "Epoch 69 / 100 - Batch 2000 - Loss: 6.003744872371014e-05\n",
      "Epoch 69 / 100 - Batch 2500 - Loss: 8.540575854627343e-05\n",
      "Epoch 69 / 100 - Batch 3000 - Loss: 7.35154542107999e-05\n",
      "Epoch 69 / 100 - Batch 3500 - Loss: 6.469693154048561e-05\n",
      "Epoch 69 / 100 - Batch 4000 - Loss: 6.430168258926234e-05\n",
      "Epoch 69 / 100 - Batch 4500 - Loss: 7.084119812680971e-05\n",
      "Epoch 69 / 100 - Batch 5000 - Loss: 6.886889535995522e-05\n",
      "Epoch 69 / 100 - Batch 5500 - Loss: 6.323385576029593e-05\n",
      "Epoch 69 / 100 - Batch 6000 - Loss: 8.027617680872324e-05\n",
      "Epoch 69 / 100 - Batch 6500 - Loss: 8.20098434628365e-05\n",
      "Epoch 70 / 100 - Batch 0 - Loss: 0.0\n",
      "Epoch 70 / 100 - Batch 500 - Loss: 9.654470892740317e-05\n",
      "Epoch 70 / 100 - Batch 1000 - Loss: 7.823715959519333e-05\n",
      "Epoch 70 / 100 - Batch 1500 - Loss: 7.054038438308152e-05\n",
      "Epoch 70 / 100 - Batch 2000 - Loss: 5.76074237654153e-05\n",
      "Epoch 70 / 100 - Batch 2500 - Loss: 4.7324115115484564e-05\n",
      "Epoch 70 / 100 - Batch 3000 - Loss: 5.653955927437798e-05\n",
      "Epoch 70 / 100 - Batch 3500 - Loss: 4.9114622789895045e-05\n",
      "Epoch 70 / 100 - Batch 4000 - Loss: 4.55642964685871e-05\n",
      "Epoch 70 / 100 - Batch 4500 - Loss: 4.6911984918103894e-05\n",
      "Epoch 70 / 100 - Batch 5000 - Loss: 5.3723734609289835e-05\n",
      "Epoch 70 / 100 - Batch 5500 - Loss: 5.7248674124650136e-05\n",
      "Epoch 70 / 100 - Batch 6000 - Loss: 5.310422045633088e-05\n",
      "Epoch 70 / 100 - Batch 6500 - Loss: 4.9616761511656366e-05\n",
      "Epoch 71 / 100 - Batch 0 - Loss: 0.0\n",
      "Epoch 71 / 100 - Batch 500 - Loss: 4.274761633368453e-05\n",
      "Epoch 71 / 100 - Batch 1000 - Loss: 3.060863461130327e-05\n",
      "Epoch 71 / 100 - Batch 1500 - Loss: 9.489891765321031e-05\n",
      "Epoch 71 / 100 - Batch 2000 - Loss: 7.72307334752308e-05\n",
      "Epoch 71 / 100 - Batch 2500 - Loss: 6.949402679043814e-05\n",
      "Epoch 71 / 100 - Batch 3000 - Loss: 8.651548218344903e-05\n",
      "Epoch 71 / 100 - Batch 3500 - Loss: 8.64468042194111e-05\n",
      "Epoch 71 / 100 - Batch 4000 - Loss: 8.818639528960878e-05\n",
      "Epoch 71 / 100 - Batch 4500 - Loss: 8.986380723966338e-05\n",
      "Epoch 71 / 100 - Batch 5000 - Loss: 9.389860273008424e-05\n",
      "Epoch 71 / 100 - Batch 5500 - Loss: 9.03033348861716e-05\n",
      "Epoch 71 / 100 - Batch 6000 - Loss: 8.458388189924089e-05\n",
      "Epoch 71 / 100 - Batch 6500 - Loss: 8.102082528427905e-05\n",
      "Epoch 72 / 100 - Batch 0 - Loss: 0.0\n",
      "Epoch 72 / 100 - Batch 500 - Loss: 3.6837114621818045e-05\n",
      "Epoch 72 / 100 - Batch 1000 - Loss: 4.223489744659101e-05\n",
      "Epoch 72 / 100 - Batch 1500 - Loss: 0.00014803078981631029\n",
      "Epoch 72 / 100 - Batch 2000 - Loss: 0.00014406777798704848\n",
      "Epoch 72 / 100 - Batch 2500 - Loss: 0.00012498553035377786\n",
      "Epoch 72 / 100 - Batch 3000 - Loss: 0.00010558279973781545\n",
      "Epoch 72 / 100 - Batch 3500 - Loss: 9.094300070354864e-05\n",
      "Epoch 72 / 100 - Batch 4000 - Loss: 8.721839238984744e-05\n",
      "Epoch 72 / 100 - Batch 4500 - Loss: 8.343574024724029e-05\n",
      "Epoch 72 / 100 - Batch 5000 - Loss: 8.736584389965518e-05\n",
      "Epoch 72 / 100 - Batch 5500 - Loss: 7.980472674734924e-05\n",
      "Epoch 72 / 100 - Batch 6000 - Loss: 7.451935276017851e-05\n",
      "Epoch 72 / 100 - Batch 6500 - Loss: 6.902357839391544e-05\n",
      "Epoch 73 / 100 - Batch 0 - Loss: 0.0\n",
      "Epoch 73 / 100 - Batch 500 - Loss: 4.9039466029250966e-05\n",
      "Epoch 73 / 100 - Batch 1000 - Loss: 0.00013831269087600503\n",
      "Epoch 73 / 100 - Batch 1500 - Loss: 0.00010667606528962292\n",
      "Epoch 73 / 100 - Batch 2000 - Loss: 0.00010099852017024468\n",
      "Epoch 73 / 100 - Batch 2500 - Loss: 0.00011700366821653357\n",
      "Epoch 73 / 100 - Batch 3000 - Loss: 0.00010655393631494723\n",
      "Epoch 73 / 100 - Batch 3500 - Loss: 9.203654502753542e-05\n",
      "Epoch 73 / 100 - Batch 4000 - Loss: 8.747369681339512e-05\n",
      "Epoch 73 / 100 - Batch 4500 - Loss: 8.263665159091352e-05\n",
      "Epoch 73 / 100 - Batch 5000 - Loss: 7.610198699400273e-05\n",
      "Epoch 73 / 100 - Batch 5500 - Loss: 8.181072840750726e-05\n",
      "Epoch 73 / 100 - Batch 6000 - Loss: 7.771741530903124e-05\n",
      "Epoch 73 / 100 - Batch 6500 - Loss: 7.842343901053178e-05\n",
      "Epoch 74 / 100 - Batch 0 - Loss: 0.0\n",
      "Epoch 74 / 100 - Batch 500 - Loss: 0.00010820600846500492\n",
      "Epoch 74 / 100 - Batch 1000 - Loss: 7.367980046448565e-05\n",
      "Epoch 74 / 100 - Batch 1500 - Loss: 6.79106429897977e-05\n",
      "Epoch 74 / 100 - Batch 2000 - Loss: 6.6433124177306e-05\n",
      "Epoch 74 / 100 - Batch 2500 - Loss: 6.56065747051728e-05\n",
      "Epoch 74 / 100 - Batch 3000 - Loss: 7.712448067458553e-05\n",
      "Epoch 74 / 100 - Batch 3500 - Loss: 6.894987642039351e-05\n",
      "Epoch 74 / 100 - Batch 4000 - Loss: 6.901112258703877e-05\n",
      "Epoch 74 / 100 - Batch 4500 - Loss: 6.438824110146167e-05\n",
      "Epoch 74 / 100 - Batch 5000 - Loss: 6.66791161929418e-05\n",
      "Epoch 74 / 100 - Batch 5500 - Loss: 8.661138572059953e-05\n",
      "Epoch 74 / 100 - Batch 6000 - Loss: 8.05279998109402e-05\n",
      "Epoch 74 / 100 - Batch 6500 - Loss: 7.520639025746527e-05\n",
      "Epoch 75 / 100 - Batch 0 - Loss: 0.0\n",
      "Epoch 75 / 100 - Batch 500 - Loss: 3.455420806979594e-06\n",
      "Epoch 75 / 100 - Batch 1000 - Loss: 7.104275582297748e-06\n",
      "Epoch 75 / 100 - Batch 1500 - Loss: 1.727139083455819e-05\n",
      "Epoch 75 / 100 - Batch 2000 - Loss: 2.4929374519380848e-05\n",
      "Epoch 75 / 100 - Batch 2500 - Loss: 4.343985569101379e-05\n",
      "Epoch 75 / 100 - Batch 3000 - Loss: 4.787427672335657e-05\n",
      "Epoch 75 / 100 - Batch 3500 - Loss: 4.847053444586737e-05\n",
      "Epoch 75 / 100 - Batch 4000 - Loss: 4.8394674910566674e-05\n",
      "Epoch 75 / 100 - Batch 4500 - Loss: 8.327128860423346e-05\n",
      "Epoch 75 / 100 - Batch 5000 - Loss: 8.17932492725431e-05\n",
      "Epoch 75 / 100 - Batch 5500 - Loss: 8.533230986554176e-05\n",
      "Epoch 75 / 100 - Batch 6000 - Loss: 8.021036253202701e-05\n",
      "Epoch 75 / 100 - Batch 6500 - Loss: 7.65307228500301e-05\n",
      "Epoch 76 / 100 - Batch 0 - Loss: 4.27273411096607e-10\n",
      "Epoch 76 / 100 - Batch 500 - Loss: 3.524897648079961e-06\n",
      "Epoch 76 / 100 - Batch 1000 - Loss: 2.412081386551263e-05\n",
      "Epoch 76 / 100 - Batch 1500 - Loss: 5.840063209574446e-05\n",
      "Epoch 76 / 100 - Batch 2000 - Loss: 7.12404133862744e-05\n",
      "Epoch 76 / 100 - Batch 2500 - Loss: 6.690370351548951e-05\n",
      "Epoch 76 / 100 - Batch 3000 - Loss: 7.028429551097601e-05\n",
      "Epoch 76 / 100 - Batch 3500 - Loss: 8.677246781913285e-05\n",
      "Epoch 76 / 100 - Batch 4000 - Loss: 7.961757938179481e-05\n",
      "Epoch 76 / 100 - Batch 4500 - Loss: 8.10347006927146e-05\n",
      "Epoch 76 / 100 - Batch 5000 - Loss: 7.7707959558487e-05\n",
      "Epoch 76 / 100 - Batch 5500 - Loss: 8.046181251608192e-05\n",
      "Epoch 76 / 100 - Batch 6000 - Loss: 8.314370912338878e-05\n",
      "Epoch 76 / 100 - Batch 6500 - Loss: 8.217571079543953e-05\n",
      "Epoch 77 / 100 - Batch 0 - Loss: 0.0\n",
      "Epoch 77 / 100 - Batch 500 - Loss: 5.312590121137125e-06\n",
      "Epoch 77 / 100 - Batch 1000 - Loss: 5.283088251598786e-05\n",
      "Epoch 77 / 100 - Batch 1500 - Loss: 7.076285475243678e-05\n",
      "Epoch 77 / 100 - Batch 2000 - Loss: 0.00010058508618892188\n",
      "Epoch 77 / 100 - Batch 2500 - Loss: 0.00014633896591840725\n",
      "Epoch 77 / 100 - Batch 3000 - Loss: 0.00013282314561536093\n",
      "Epoch 77 / 100 - Batch 3500 - Loss: 0.00013992463441820082\n",
      "Epoch 77 / 100 - Batch 4000 - Loss: 0.00012730487380265834\n",
      "Epoch 77 / 100 - Batch 4500 - Loss: 0.00014687784755371022\n",
      "Epoch 77 / 100 - Batch 5000 - Loss: 0.0001507803424021977\n",
      "Epoch 77 / 100 - Batch 5500 - Loss: 0.0001381977915707624\n",
      "Epoch 77 / 100 - Batch 6000 - Loss: 0.0001281034471645486\n",
      "Epoch 77 / 100 - Batch 6500 - Loss: 0.00012204655449964109\n",
      "Epoch 78 / 100 - Batch 0 - Loss: 0.0\n",
      "Epoch 78 / 100 - Batch 500 - Loss: 2.72216793117035e-05\n",
      "Epoch 78 / 100 - Batch 1000 - Loss: 4.284467933538737e-05\n",
      "Epoch 78 / 100 - Batch 1500 - Loss: 9.967961911712475e-05\n",
      "Epoch 78 / 100 - Batch 2000 - Loss: 0.00014327082136801579\n",
      "Epoch 78 / 100 - Batch 2500 - Loss: 0.00017895518797386256\n",
      "Epoch 78 / 100 - Batch 3000 - Loss: 0.0001527541356004637\n",
      "Epoch 78 / 100 - Batch 3500 - Loss: 0.00015032952420273178\n",
      "Epoch 78 / 100 - Batch 4000 - Loss: 0.000141267486885706\n",
      "Epoch 78 / 100 - Batch 4500 - Loss: 0.00012953011158113646\n",
      "Epoch 78 / 100 - Batch 5000 - Loss: 0.00011987241423456159\n",
      "Epoch 78 / 100 - Batch 5500 - Loss: 0.00011138196589000122\n",
      "Epoch 78 / 100 - Batch 6000 - Loss: 0.00010376220551730909\n",
      "Epoch 78 / 100 - Batch 6500 - Loss: 9.879449110367153e-05\n",
      "Epoch 79 / 100 - Batch 0 - Loss: 4.913637496883894e-09\n",
      "Epoch 79 / 100 - Batch 500 - Loss: 1.3894104084695277e-05\n",
      "Epoch 79 / 100 - Batch 1000 - Loss: 0.00010711619381079727\n",
      "Epoch 79 / 100 - Batch 1500 - Loss: 0.00012467405272641372\n",
      "Epoch 79 / 100 - Batch 2000 - Loss: 0.0001498497113556668\n",
      "Epoch 79 / 100 - Batch 2500 - Loss: 0.00012223824913724566\n",
      "Epoch 79 / 100 - Batch 3000 - Loss: 0.00010780253139817344\n",
      "Epoch 79 / 100 - Batch 3500 - Loss: 9.319235613580856e-05\n",
      "Epoch 79 / 100 - Batch 4000 - Loss: 8.2377940454313e-05\n",
      "Epoch 79 / 100 - Batch 4500 - Loss: 7.335973737752345e-05\n",
      "Epoch 79 / 100 - Batch 5000 - Loss: 6.612029294428294e-05\n",
      "Epoch 79 / 100 - Batch 5500 - Loss: 6.0305036310277924e-05\n",
      "Epoch 79 / 100 - Batch 6000 - Loss: 6.567196860782211e-05\n",
      "Epoch 79 / 100 - Batch 6500 - Loss: 8.680940203161542e-05\n",
      "Epoch 80 / 100 - Batch 0 - Loss: 1.4954564253599756e-09\n",
      "Epoch 80 / 100 - Batch 500 - Loss: 7.718922489554662e-06\n",
      "Epoch 80 / 100 - Batch 1000 - Loss: 9.137315662964287e-05\n",
      "Epoch 80 / 100 - Batch 1500 - Loss: 6.315424391862004e-05\n",
      "Epoch 80 / 100 - Batch 2000 - Loss: 6.108328825566968e-05\n",
      "Epoch 80 / 100 - Batch 2500 - Loss: 5.096295932021829e-05\n",
      "Epoch 80 / 100 - Batch 3000 - Loss: 4.559200454909634e-05\n",
      "Epoch 80 / 100 - Batch 3500 - Loss: 3.9507489646205654e-05\n",
      "Epoch 80 / 100 - Batch 4000 - Loss: 3.605056780973098e-05\n",
      "Epoch 80 / 100 - Batch 4500 - Loss: 3.21495558875418e-05\n",
      "Epoch 80 / 100 - Batch 5000 - Loss: 2.9047544432613428e-05\n",
      "Epoch 80 / 100 - Batch 5500 - Loss: 4.155650560418931e-05\n",
      "Epoch 80 / 100 - Batch 6000 - Loss: 4.575604836990349e-05\n",
      "Epoch 80 / 100 - Batch 6500 - Loss: 4.417117439901246e-05\n",
      "Epoch 81 / 100 - Batch 0 - Loss: 7.370365295855663e-08\n",
      "Epoch 81 / 100 - Batch 500 - Loss: 5.501957270417549e-05\n",
      "Epoch 81 / 100 - Batch 1000 - Loss: 0.000191715134712426\n",
      "Epoch 81 / 100 - Batch 1500 - Loss: 0.00013066653390087228\n",
      "Epoch 81 / 100 - Batch 2000 - Loss: 0.00013968488092347712\n",
      "Epoch 81 / 100 - Batch 2500 - Loss: 0.00013730861604804167\n",
      "Epoch 81 / 100 - Batch 3000 - Loss: 0.00012670946566570697\n",
      "Epoch 81 / 100 - Batch 3500 - Loss: 0.00011471508365947232\n",
      "Epoch 81 / 100 - Batch 4000 - Loss: 0.00010069704510711187\n",
      "Epoch 81 / 100 - Batch 4500 - Loss: 8.982076293787976e-05\n",
      "Epoch 81 / 100 - Batch 5000 - Loss: 8.106033807970464e-05\n",
      "Epoch 81 / 100 - Batch 5500 - Loss: 9.634143050294605e-05\n",
      "Epoch 81 / 100 - Batch 6000 - Loss: 9.594175945000182e-05\n",
      "Epoch 81 / 100 - Batch 6500 - Loss: 9.149832758466458e-05\n",
      "Epoch 82 / 100 - Batch 0 - Loss: 2.136367055483035e-10\n",
      "Epoch 82 / 100 - Batch 500 - Loss: 0.00010923192523660968\n",
      "Epoch 82 / 100 - Batch 1000 - Loss: 5.874939706329431e-05\n",
      "Epoch 82 / 100 - Batch 1500 - Loss: 6.497035357262568e-05\n",
      "Epoch 82 / 100 - Batch 2000 - Loss: 7.194495678864822e-05\n",
      "Epoch 82 / 100 - Batch 2500 - Loss: 6.846105346621052e-05\n",
      "Epoch 82 / 100 - Batch 3000 - Loss: 5.813478732565746e-05\n",
      "Epoch 82 / 100 - Batch 3500 - Loss: 5.166523892313802e-05\n",
      "Epoch 82 / 100 - Batch 4000 - Loss: 5.9058480241949394e-05\n",
      "Epoch 82 / 100 - Batch 4500 - Loss: 6.620341510267993e-05\n",
      "Epoch 82 / 100 - Batch 5000 - Loss: 6.299827819516118e-05\n",
      "Epoch 82 / 100 - Batch 5500 - Loss: 8.676414134216598e-05\n",
      "Epoch 82 / 100 - Batch 6000 - Loss: 8.061907641918742e-05\n",
      "Epoch 82 / 100 - Batch 6500 - Loss: 7.932785205970114e-05\n",
      "Epoch 83 / 100 - Batch 0 - Loss: 0.0\n",
      "Epoch 83 / 100 - Batch 500 - Loss: 0.00012445504715022533\n",
      "Epoch 83 / 100 - Batch 1000 - Loss: 6.599082981593128e-05\n",
      "Epoch 83 / 100 - Batch 1500 - Loss: 5.026018121740949e-05\n",
      "Epoch 83 / 100 - Batch 2000 - Loss: 8.552404950499536e-05\n",
      "Epoch 83 / 100 - Batch 2500 - Loss: 9.155169056550217e-05\n",
      "Epoch 83 / 100 - Batch 3000 - Loss: 8.005021278218359e-05\n",
      "Epoch 83 / 100 - Batch 3500 - Loss: 8.312063055760174e-05\n",
      "Epoch 83 / 100 - Batch 4000 - Loss: 9.623368794779155e-05\n",
      "Epoch 83 / 100 - Batch 4500 - Loss: 9.478760668561705e-05\n",
      "Epoch 83 / 100 - Batch 5000 - Loss: 9.768565293225919e-05\n",
      "Epoch 83 / 100 - Batch 5500 - Loss: 9.436641648878343e-05\n",
      "Epoch 83 / 100 - Batch 6000 - Loss: 8.89510727651149e-05\n",
      "Epoch 83 / 100 - Batch 6500 - Loss: 9.385994388293492e-05\n",
      "Epoch 84 / 100 - Batch 0 - Loss: 9.421159319344952e-08\n",
      "Epoch 84 / 100 - Batch 500 - Loss: 0.0002650437712016398\n",
      "Epoch 84 / 100 - Batch 1000 - Loss: 0.0001373211269474354\n",
      "Epoch 84 / 100 - Batch 1500 - Loss: 0.0001028824059388888\n",
      "Epoch 84 / 100 - Batch 2000 - Loss: 8.523241538825014e-05\n",
      "Epoch 84 / 100 - Batch 2500 - Loss: 7.674799978893719e-05\n",
      "Epoch 84 / 100 - Batch 3000 - Loss: 6.881083204859013e-05\n",
      "Epoch 84 / 100 - Batch 3500 - Loss: 6.82042576619905e-05\n",
      "Epoch 84 / 100 - Batch 4000 - Loss: 7.77973726980549e-05\n",
      "Epoch 84 / 100 - Batch 4500 - Loss: 9.513388640959214e-05\n",
      "Epoch 84 / 100 - Batch 5000 - Loss: 8.693481865148497e-05\n",
      "Epoch 84 / 100 - Batch 5500 - Loss: 8.990312605106646e-05\n",
      "Epoch 84 / 100 - Batch 6000 - Loss: 9.723916213149507e-05\n",
      "Epoch 84 / 100 - Batch 6500 - Loss: 9.158295779296861e-05\n",
      "Epoch 85 / 100 - Batch 0 - Loss: 0.0\n",
      "Epoch 85 / 100 - Batch 500 - Loss: 0.00011844027489856616\n",
      "Epoch 85 / 100 - Batch 1000 - Loss: 0.00011214921315382318\n",
      "Epoch 85 / 100 - Batch 1500 - Loss: 9.339433737682452e-05\n",
      "Epoch 85 / 100 - Batch 2000 - Loss: 0.00010930143259395019\n",
      "Epoch 85 / 100 - Batch 2500 - Loss: 9.814172972318579e-05\n",
      "Epoch 85 / 100 - Batch 3000 - Loss: 8.747524485561827e-05\n",
      "Epoch 85 / 100 - Batch 3500 - Loss: 9.662357295734551e-05\n",
      "Epoch 85 / 100 - Batch 4000 - Loss: 0.00018276375997414442\n",
      "Epoch 85 / 100 - Batch 4500 - Loss: 0.000167626375372517\n",
      "Epoch 85 / 100 - Batch 5000 - Loss: 0.00015499423588508643\n",
      "Epoch 85 / 100 - Batch 5500 - Loss: 0.00014377693215413958\n",
      "Epoch 85 / 100 - Batch 6000 - Loss: 0.00014448329280086672\n",
      "Epoch 85 / 100 - Batch 6500 - Loss: 0.00013750744905379397\n",
      "Epoch 86 / 100 - Batch 0 - Loss: 1.2818202055342454e-09\n",
      "Epoch 86 / 100 - Batch 500 - Loss: 3.837598616458624e-06\n",
      "Epoch 86 / 100 - Batch 1000 - Loss: 2.0599777694426564e-05\n",
      "Epoch 86 / 100 - Batch 1500 - Loss: 2.2181102967412367e-05\n",
      "Epoch 86 / 100 - Batch 2000 - Loss: 0.00010590761055907415\n",
      "Epoch 86 / 100 - Batch 2500 - Loss: 0.00016894452970914097\n",
      "Epoch 86 / 100 - Batch 3000 - Loss: 0.0001492798177521545\n",
      "Epoch 86 / 100 - Batch 3500 - Loss: 0.00013170149331323067\n",
      "Epoch 86 / 100 - Batch 4000 - Loss: 0.00013695687536102575\n",
      "Epoch 86 / 100 - Batch 4500 - Loss: 0.00013543957381124464\n",
      "Epoch 86 / 100 - Batch 5000 - Loss: 0.00012647350884290856\n",
      "Epoch 86 / 100 - Batch 5500 - Loss: 0.00012019940911098958\n",
      "Epoch 86 / 100 - Batch 6000 - Loss: 0.00011098661676921523\n",
      "Epoch 86 / 100 - Batch 6500 - Loss: 0.00010819256773127093\n",
      "Epoch 87 / 100 - Batch 0 - Loss: 0.0\n",
      "Epoch 87 / 100 - Batch 500 - Loss: 3.1800655345742595e-06\n",
      "Epoch 87 / 100 - Batch 1000 - Loss: 0.00010932669788020561\n",
      "Epoch 87 / 100 - Batch 1500 - Loss: 7.4188791609539e-05\n",
      "Epoch 87 / 100 - Batch 2000 - Loss: 0.0001022385194573957\n",
      "Epoch 87 / 100 - Batch 2500 - Loss: 8.60841345361107e-05\n",
      "Epoch 87 / 100 - Batch 3000 - Loss: 8.92967296635448e-05\n",
      "Epoch 87 / 100 - Batch 3500 - Loss: 8.371424838398545e-05\n",
      "Epoch 87 / 100 - Batch 4000 - Loss: 0.00021525496167719804\n",
      "Epoch 87 / 100 - Batch 4500 - Loss: 0.00019652360267343426\n",
      "Epoch 87 / 100 - Batch 5000 - Loss: 0.00017862044530555935\n",
      "Epoch 87 / 100 - Batch 5500 - Loss: 0.0001632176437161696\n",
      "Epoch 87 / 100 - Batch 6000 - Loss: 0.0001503536383847991\n",
      "Epoch 87 / 100 - Batch 6500 - Loss: 0.00014038484119679887\n",
      "Epoch 88 / 100 - Batch 0 - Loss: 1.922729531145251e-09\n",
      "Epoch 88 / 100 - Batch 500 - Loss: 3.9936243302585366e-05\n",
      "Epoch 88 / 100 - Batch 1000 - Loss: 0.00014463827427344347\n",
      "Epoch 88 / 100 - Batch 1500 - Loss: 0.00010570019138293945\n",
      "Epoch 88 / 100 - Batch 2000 - Loss: 8.892916061706356e-05\n",
      "Epoch 88 / 100 - Batch 2500 - Loss: 7.936797191385159e-05\n",
      "Epoch 88 / 100 - Batch 3000 - Loss: 8.578518170315015e-05\n",
      "Epoch 88 / 100 - Batch 3500 - Loss: 8.505262483999712e-05\n",
      "Epoch 88 / 100 - Batch 4000 - Loss: 7.585016334999363e-05\n",
      "Epoch 88 / 100 - Batch 4500 - Loss: 7.823941062738358e-05\n",
      "Epoch 88 / 100 - Batch 5000 - Loss: 7.155908854676233e-05\n",
      "Epoch 88 / 100 - Batch 5500 - Loss: 7.35764502228665e-05\n",
      "Epoch 88 / 100 - Batch 6000 - Loss: 8.869020050860098e-05\n",
      "Epoch 88 / 100 - Batch 6500 - Loss: 8.348475774086158e-05\n",
      "Epoch 89 / 100 - Batch 0 - Loss: 0.0\n",
      "Epoch 89 / 100 - Batch 500 - Loss: 2.0560839631473436e-06\n",
      "Epoch 89 / 100 - Batch 1000 - Loss: 2.841926321770794e-06\n",
      "Epoch 89 / 100 - Batch 1500 - Loss: 6.9441049182168315e-06\n",
      "Epoch 89 / 100 - Batch 2000 - Loss: 5.84880291117075e-06\n",
      "Epoch 89 / 100 - Batch 2500 - Loss: 3.409668772266189e-05\n",
      "Epoch 89 / 100 - Batch 3000 - Loss: 6.564270432323083e-05\n",
      "Epoch 89 / 100 - Batch 3500 - Loss: 6.87110277957175e-05\n",
      "Epoch 89 / 100 - Batch 4000 - Loss: 6.832702877722649e-05\n",
      "Epoch 89 / 100 - Batch 4500 - Loss: 6.365554748673352e-05\n",
      "Epoch 89 / 100 - Batch 5000 - Loss: 5.758471317169892e-05\n",
      "Epoch 89 / 100 - Batch 5500 - Loss: 5.271915794904073e-05\n",
      "Epoch 89 / 100 - Batch 6000 - Loss: 5.1673031132864974e-05\n",
      "Epoch 89 / 100 - Batch 6500 - Loss: 6.07189884726471e-05\n",
      "Epoch 90 / 100 - Batch 0 - Loss: 4.27273411096607e-10\n",
      "Epoch 90 / 100 - Batch 500 - Loss: 7.247362473285956e-05\n",
      "Epoch 90 / 100 - Batch 1000 - Loss: 3.82031760577146e-05\n",
      "Epoch 90 / 100 - Batch 1500 - Loss: 2.6303093648276375e-05\n",
      "Epoch 90 / 100 - Batch 2000 - Loss: 2.047242296166952e-05\n",
      "Epoch 90 / 100 - Batch 2500 - Loss: 9.615329518484709e-05\n",
      "Epoch 90 / 100 - Batch 3000 - Loss: 0.0001940976163926321\n",
      "Epoch 90 / 100 - Batch 3500 - Loss: 0.00020047351422698034\n",
      "Epoch 90 / 100 - Batch 4000 - Loss: 0.0001784116585651411\n",
      "Epoch 90 / 100 - Batch 4500 - Loss: 0.00016632463540905177\n",
      "Epoch 90 / 100 - Batch 5000 - Loss: 0.00015114655855715282\n",
      "Epoch 90 / 100 - Batch 5500 - Loss: 0.00014634202934266655\n",
      "Epoch 90 / 100 - Batch 6000 - Loss: 0.00013586055594024793\n",
      "Epoch 90 / 100 - Batch 6500 - Loss: 0.00012929471729228732\n",
      "Epoch 91 / 100 - Batch 0 - Loss: 5.981818596012545e-09\n",
      "Epoch 91 / 100 - Batch 500 - Loss: 0.00013253761404332447\n",
      "Epoch 91 / 100 - Batch 1000 - Loss: 6.96908697007919e-05\n",
      "Epoch 91 / 100 - Batch 1500 - Loss: 7.442224939930617e-05\n",
      "Epoch 91 / 100 - Batch 2000 - Loss: 6.0939293830226426e-05\n",
      "Epoch 91 / 100 - Batch 2500 - Loss: 5.603137609392897e-05\n",
      "Epoch 91 / 100 - Batch 3000 - Loss: 4.94876972975497e-05\n",
      "Epoch 91 / 100 - Batch 3500 - Loss: 6.989667117895521e-05\n",
      "Epoch 91 / 100 - Batch 4000 - Loss: 6.259080140131767e-05\n",
      "Epoch 91 / 100 - Batch 4500 - Loss: 5.604616705010218e-05\n",
      "Epoch 91 / 100 - Batch 5000 - Loss: 5.7969617414349356e-05\n",
      "Epoch 91 / 100 - Batch 5500 - Loss: 5.7168752346933985e-05\n",
      "Epoch 91 / 100 - Batch 6000 - Loss: 5.302657820750673e-05\n",
      "Epoch 91 / 100 - Batch 6500 - Loss: 4.9308576115692915e-05\n",
      "Epoch 92 / 100 - Batch 0 - Loss: 0.0\n",
      "Epoch 92 / 100 - Batch 500 - Loss: 0.00021950485344080338\n",
      "Epoch 92 / 100 - Batch 1000 - Loss: 0.00012083681670737997\n",
      "Epoch 92 / 100 - Batch 1500 - Loss: 0.0001413827425568261\n",
      "Epoch 92 / 100 - Batch 2000 - Loss: 0.00010933181564244355\n",
      "Epoch 92 / 100 - Batch 2500 - Loss: 0.00010805726078697838\n",
      "Epoch 92 / 100 - Batch 3000 - Loss: 0.00010025372154881586\n",
      "Epoch 92 / 100 - Batch 3500 - Loss: 0.00012269876234545485\n",
      "Epoch 92 / 100 - Batch 4000 - Loss: 0.00011114816062267206\n",
      "Epoch 92 / 100 - Batch 4500 - Loss: 0.00010162176445353655\n",
      "Epoch 92 / 100 - Batch 5000 - Loss: 0.00011287184532842333\n",
      "Epoch 92 / 100 - Batch 5500 - Loss: 0.00010606080407442035\n",
      "Epoch 92 / 100 - Batch 6000 - Loss: 0.00010088261944651382\n",
      "Epoch 92 / 100 - Batch 6500 - Loss: 0.00010326269101990334\n",
      "Epoch 93 / 100 - Batch 0 - Loss: 0.0\n",
      "Epoch 93 / 100 - Batch 500 - Loss: 6.0122467634292764e-06\n",
      "Epoch 93 / 100 - Batch 1000 - Loss: 4.739446359819269e-05\n",
      "Epoch 93 / 100 - Batch 1500 - Loss: 9.710970188491572e-05\n",
      "Epoch 93 / 100 - Batch 2000 - Loss: 9.200868229796883e-05\n",
      "Epoch 93 / 100 - Batch 2500 - Loss: 9.446414745064225e-05\n",
      "Epoch 93 / 100 - Batch 3000 - Loss: 8.127598665511469e-05\n",
      "Epoch 93 / 100 - Batch 3500 - Loss: 7.602983978970432e-05\n",
      "Epoch 93 / 100 - Batch 4000 - Loss: 6.783430152846035e-05\n",
      "Epoch 93 / 100 - Batch 4500 - Loss: 6.0569461731431615e-05\n",
      "Epoch 93 / 100 - Batch 5000 - Loss: 5.5208872874919355e-05\n",
      "Epoch 93 / 100 - Batch 5500 - Loss: 5.0492899837137234e-05\n",
      "Epoch 93 / 100 - Batch 6000 - Loss: 4.9272308012883284e-05\n",
      "Epoch 93 / 100 - Batch 6500 - Loss: 4.988213746069784e-05\n",
      "Epoch 94 / 100 - Batch 0 - Loss: 0.0\n",
      "Epoch 94 / 100 - Batch 500 - Loss: 0.00018925879156478617\n",
      "Epoch 94 / 100 - Batch 1000 - Loss: 0.00011035127607150684\n",
      "Epoch 94 / 100 - Batch 1500 - Loss: 9.568257015167999e-05\n",
      "Epoch 94 / 100 - Batch 2000 - Loss: 0.00018161196783915496\n",
      "Epoch 94 / 100 - Batch 2500 - Loss: 0.00017080213232050284\n",
      "Epoch 94 / 100 - Batch 3000 - Loss: 0.00014405197911070743\n",
      "Epoch 94 / 100 - Batch 3500 - Loss: 0.00017221489340527664\n",
      "Epoch 94 / 100 - Batch 4000 - Loss: 0.00015234086450069176\n",
      "Epoch 94 / 100 - Batch 4500 - Loss: 0.00013874599813028452\n",
      "Epoch 94 / 100 - Batch 5000 - Loss: 0.00012677371370810076\n",
      "Epoch 94 / 100 - Batch 5500 - Loss: 0.00011554340133357845\n",
      "Epoch 94 / 100 - Batch 6000 - Loss: 0.00011607026885661153\n",
      "Epoch 94 / 100 - Batch 6500 - Loss: 0.00011060485801322685\n",
      "Epoch 95 / 100 - Batch 0 - Loss: 1.4099968304037702e-08\n",
      "Epoch 95 / 100 - Batch 500 - Loss: 0.00021298110307181118\n",
      "Epoch 95 / 100 - Batch 1000 - Loss: 0.00014534426871546188\n",
      "Epoch 95 / 100 - Batch 1500 - Loss: 0.0003943539227926694\n",
      "Epoch 95 / 100 - Batch 2000 - Loss: 0.0003121595765315818\n",
      "Epoch 95 / 100 - Batch 2500 - Loss: 0.0002808349293577189\n",
      "Epoch 95 / 100 - Batch 3000 - Loss: 0.00023615425882961296\n",
      "Epoch 95 / 100 - Batch 3500 - Loss: 0.00023538471495150186\n",
      "Epoch 95 / 100 - Batch 4000 - Loss: 0.0002165239453973206\n",
      "Epoch 95 / 100 - Batch 4500 - Loss: 0.00019597872703220858\n",
      "Epoch 95 / 100 - Batch 5000 - Loss: 0.00018310335124098841\n",
      "Epoch 95 / 100 - Batch 5500 - Loss: 0.00017234705589079158\n",
      "Epoch 95 / 100 - Batch 6000 - Loss: 0.0001761239842629708\n",
      "Epoch 95 / 100 - Batch 6500 - Loss: 0.0001781694441583147\n",
      "Epoch 96 / 100 - Batch 0 - Loss: 0.0\n",
      "Epoch 96 / 100 - Batch 500 - Loss: 3.684110025647718e-05\n",
      "Epoch 96 / 100 - Batch 1000 - Loss: 7.873815311075853e-05\n",
      "Epoch 96 / 100 - Batch 1500 - Loss: 6.01836577401267e-05\n",
      "Epoch 96 / 100 - Batch 2000 - Loss: 4.8252200960288366e-05\n",
      "Epoch 96 / 100 - Batch 2500 - Loss: 4.037457725133656e-05\n",
      "Epoch 96 / 100 - Batch 3000 - Loss: 3.651518772116672e-05\n",
      "Epoch 96 / 100 - Batch 3500 - Loss: 6.23991110971607e-05\n",
      "Epoch 96 / 100 - Batch 4000 - Loss: 5.871492628903613e-05\n",
      "Epoch 96 / 100 - Batch 4500 - Loss: 5.361217885681919e-05\n",
      "Epoch 96 / 100 - Batch 5000 - Loss: 5.398958214187012e-05\n",
      "Epoch 96 / 100 - Batch 5500 - Loss: 4.979447720775214e-05\n",
      "Epoch 96 / 100 - Batch 6000 - Loss: 7.600560237020613e-05\n",
      "Epoch 96 / 100 - Batch 6500 - Loss: 8.431061499852683e-05\n",
      "Epoch 97 / 100 - Batch 0 - Loss: 2.136367055483035e-10\n",
      "Epoch 97 / 100 - Batch 500 - Loss: 2.2885904743531455e-05\n",
      "Epoch 97 / 100 - Batch 1000 - Loss: 2.5844132333583296e-05\n",
      "Epoch 97 / 100 - Batch 1500 - Loss: 2.2284921254776572e-05\n",
      "Epoch 97 / 100 - Batch 2000 - Loss: 1.730705579870863e-05\n",
      "Epoch 97 / 100 - Batch 2500 - Loss: 1.9659627470765433e-05\n",
      "Epoch 97 / 100 - Batch 3000 - Loss: 3.681033737892026e-05\n",
      "Epoch 97 / 100 - Batch 3500 - Loss: 6.220380175439245e-05\n",
      "Epoch 97 / 100 - Batch 4000 - Loss: 5.836725501007459e-05\n",
      "Epoch 97 / 100 - Batch 4500 - Loss: 5.6273119725425064e-05\n",
      "Epoch 97 / 100 - Batch 5000 - Loss: 5.933343038207152e-05\n",
      "Epoch 97 / 100 - Batch 5500 - Loss: 5.7700720160112445e-05\n",
      "Epoch 97 / 100 - Batch 6000 - Loss: 0.00010724279450110961\n",
      "Epoch 97 / 100 - Batch 6500 - Loss: 0.00019070541028165386\n",
      "Epoch 98 / 100 - Batch 0 - Loss: 2.0486652374529513e-07\n",
      "Epoch 98 / 100 - Batch 500 - Loss: 1.2625488406663612e-05\n",
      "Epoch 98 / 100 - Batch 1000 - Loss: 1.6374498399292878e-05\n",
      "Epoch 98 / 100 - Batch 1500 - Loss: 1.8526740943890534e-05\n",
      "Epoch 98 / 100 - Batch 2000 - Loss: 1.743636351036818e-05\n",
      "Epoch 98 / 100 - Batch 2500 - Loss: 5.260483591235969e-05\n",
      "Epoch 98 / 100 - Batch 3000 - Loss: 5.9929143421889736e-05\n",
      "Epoch 98 / 100 - Batch 3500 - Loss: 5.565705489015169e-05\n",
      "Epoch 98 / 100 - Batch 4000 - Loss: 6.115394035458167e-05\n",
      "Epoch 98 / 100 - Batch 4500 - Loss: 5.840262945549313e-05\n",
      "Epoch 98 / 100 - Batch 5000 - Loss: 5.471237895618403e-05\n",
      "Epoch 98 / 100 - Batch 5500 - Loss: 5.623743766144253e-05\n",
      "Epoch 98 / 100 - Batch 6000 - Loss: 5.381855350018655e-05\n",
      "Epoch 98 / 100 - Batch 6500 - Loss: 5.3543322292916915e-05\n",
      "Epoch 99 / 100 - Batch 0 - Loss: 1.7090929782526132e-09\n",
      "Epoch 99 / 100 - Batch 500 - Loss: 1.77677164884767e-05\n",
      "Epoch 99 / 100 - Batch 1000 - Loss: 2.0807470697906964e-05\n",
      "Epoch 99 / 100 - Batch 1500 - Loss: 9.868309967488678e-05\n",
      "Epoch 99 / 100 - Batch 2000 - Loss: 0.00012687077204859222\n",
      "Epoch 99 / 100 - Batch 2500 - Loss: 0.00011677654990345305\n",
      "Epoch 99 / 100 - Batch 3000 - Loss: 0.00010557148852410432\n",
      "Epoch 99 / 100 - Batch 3500 - Loss: 9.18831068988452e-05\n",
      "Epoch 99 / 100 - Batch 4000 - Loss: 8.149927752112902e-05\n",
      "Epoch 99 / 100 - Batch 4500 - Loss: 0.00014603688351798436\n",
      "Epoch 99 / 100 - Batch 5000 - Loss: 0.00013390115869338804\n",
      "Epoch 99 / 100 - Batch 5500 - Loss: 0.00012217425301299804\n",
      "Epoch 99 / 100 - Batch 6000 - Loss: 0.0001137774735118734\n",
      "Epoch 99 / 100 - Batch 6500 - Loss: 0.00014490893816621435\n",
      "Epoch 100 / 100 - Batch 0 - Loss: 1.3886332972390392e-08\n",
      "Epoch 100 / 100 - Batch 500 - Loss: 0.00043591894623107996\n",
      "Epoch 100 / 100 - Batch 1000 - Loss: 0.0002557096798413076\n",
      "Epoch 100 / 100 - Batch 1500 - Loss: 0.0001872584122713667\n",
      "Epoch 100 / 100 - Batch 2000 - Loss: 0.00015273613798879972\n",
      "Epoch 100 / 100 - Batch 2500 - Loss: 0.00017424028363206078\n",
      "Epoch 100 / 100 - Batch 3000 - Loss: 0.0001570598790345267\n",
      "Epoch 100 / 100 - Batch 3500 - Loss: 0.00014432715596364386\n",
      "Epoch 100 / 100 - Batch 4000 - Loss: 0.00013018862874227728\n",
      "Epoch 100 / 100 - Batch 4500 - Loss: 0.0019977384915290776\n",
      "Epoch 100 / 100 - Batch 5000 - Loss: 0.0018085087683708144\n",
      "Epoch 100 / 100 - Batch 5500 - Loss: 0.0016483068314681684\n",
      "Epoch 100 / 100 - Batch 6000 - Loss: 0.0015156502004367478\n",
      "Epoch 100 / 100 - Batch 6500 - Loss: 0.0017403044368752498\n"
     ]
    }
   ],
   "source": [
    "model = ChordMixer().to(device)\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Adam(lr=0.0003, params=model.parameters())\n",
    "\n",
    "os.mkdir(\"checkpoints\")\n",
    "\n",
    "trainer = Trainer(model, train_dataloader, criterion, optimizer)\n",
    "for epoch in range(max_epochs):\n",
    "    trainer.train(epoch)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7500999,
     "sourceId": 66653,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30683,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 25972.266044,
   "end_time": "2024-04-22T03:02:46.444480",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-21T19:49:54.178436",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
