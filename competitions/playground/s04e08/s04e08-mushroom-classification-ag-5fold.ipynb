{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5857e6ab",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-07T04:22:35.097434Z",
     "iopub.status.busy": "2024-08-07T04:22:35.097151Z",
     "iopub.status.idle": "2024-08-07T04:23:21.487310Z",
     "shell.execute_reply": "2024-08-07T04:23:21.486081Z"
    },
    "papermill": {
     "duration": 46.39683,
     "end_time": "2024-08-07T04:23:21.489778",
     "exception": false,
     "start_time": "2024-08-07T04:22:35.092948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "aiobotocore 2.13.1 requires aiohttp<4.0.0,>=3.9.2, but you have aiohttp 3.9.1 which is incompatible.\r\n",
      "aiobotocore 2.13.1 requires botocore<1.34.132,>=1.34.70, but you have botocore 1.29.165 which is incompatible.\r\n",
      "spaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "spopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q autogluon.tabular\n",
    "!pip install -q ray==2.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c120d526",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T04:23:21.497607Z",
     "iopub.status.busy": "2024-08-07T04:23:21.497249Z",
     "iopub.status.idle": "2024-08-07T04:23:24.218097Z",
     "shell.execute_reply": "2024-08-07T04:23:24.217131Z"
    },
    "papermill": {
     "duration": 2.727274,
     "end_time": "2024-08-07T04:23:24.220441",
     "exception": false,
     "start_time": "2024-08-07T04:23:21.493167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.base import clone\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pickle\n",
    "import shutil\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad058a47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T04:23:24.228087Z",
     "iopub.status.busy": "2024-08-07T04:23:24.227590Z",
     "iopub.status.idle": "2024-08-07T04:23:24.231941Z",
     "shell.execute_reply": "2024-08-07T04:23:24.231136Z"
    },
    "papermill": {
     "duration": 0.010121,
     "end_time": "2024-08-07T04:23:24.233820",
     "exception": false,
     "start_time": "2024-08-07T04:23:24.223699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEED = 6\n",
    "N_FOLDS = 5\n",
    "TARGET = 'class'\n",
    "TIME_LIMIT = 3600 * 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30ee3788",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T04:23:24.241114Z",
     "iopub.status.busy": "2024-08-07T04:23:24.240635Z",
     "iopub.status.idle": "2024-08-07T04:23:39.047392Z",
     "shell.execute_reply": "2024-08-07T04:23:39.046606Z"
    },
    "papermill": {
     "duration": 14.812826,
     "end_time": "2024-08-07T04:23:39.049697",
     "exception": false,
     "start_time": "2024-08-07T04:23:24.236871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/playground-series-s4e8/train.csv', index_col='id')\n",
    "test = pd.read_csv('/kaggle/input/playground-series-s4e8/test.csv', index_col='id')\n",
    "\n",
    "train[TARGET] = train[TARGET].map({'e': 0, 'p': 1})\n",
    "\n",
    "train = TabularDataset(train)\n",
    "test = TabularDataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b18edb86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T04:23:39.057205Z",
     "iopub.status.busy": "2024-08-07T04:23:39.056927Z",
     "iopub.status.idle": "2024-08-07T04:23:39.062962Z",
     "shell.execute_reply": "2024-08-07T04:23:39.062100Z"
    },
    "papermill": {
     "duration": 0.01191,
     "end_time": "2024-08-07T04:23:39.064939",
     "exception": false,
     "start_time": "2024-08-07T04:23:39.053029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_pred_probs(pred_probs, cv_score, name):\n",
    "    with open(f'autogluon_{name}_pred_probs_{cv_score:.6f}.pkl', 'wb') as f:\n",
    "        pickle.dump(pred_probs, f)\n",
    "\n",
    "def save_submission(test_pred_probs, score):\n",
    "    sub = pd.read_csv('/kaggle/input/playground-series-s4e8/sample_submission.csv')\n",
    "    sub[TARGET] = np.argmax(test_pred_probs, axis=1)\n",
    "    sub[TARGET] = sub[TARGET].map({0: 'e', 1: 'p'})\n",
    "    sub.to_csv(f'sub_autogluon_{score:.6f}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25c0b707",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-08-07T04:23:39.072067Z",
     "iopub.status.busy": "2024-08-07T04:23:39.071831Z",
     "iopub.status.idle": "2024-08-07T15:32:34.128270Z",
     "shell.execute_reply": "2024-08-07T15:32:34.127375Z"
    },
    "papermill": {
     "duration": 40135.062829,
     "end_time": "2024-08-07T15:32:34.130826",
     "exception": false,
     "start_time": "2024-08-07T04:23:39.067997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240807_042340\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jun 27 20:43:36 UTC 2024\n",
      "CPU Count:          4\n",
      "Memory Avail:       28.15 GB / 31.36 GB (89.8%)\n",
      "Disk Space Avail:   19.50 GB / 19.52 GB (99.9%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 1620s of the 6480s of remaining time (25%).\n",
      "2024-08-07 04:23:41,173\tINFO util.py:124 -- Outdated packages:\n",
      "  ipywidgets==7.7.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2024-08-07 04:23:45,021\tINFO worker.py:1752 -- Started a local Ray instance.\n",
      "\t\tContext path: \"AutogluonModels/ag-20240807_042340/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m Beginning AutoGluon training ... Time limit = 1615s\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m AutoGluon will save models to \"AutogluonModels/ag-20240807_042340/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m Train Data Rows:    2216494\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m Train Data Columns: 20\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m Label Column:       class\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m Problem Type:       binary\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \tAvailable Memory:                    28451.40 MB\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \tTrain Data (Original)  Memory Usage: 1854.97 MB (6.5% of available memory)\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \tWarning: Data size prior to feature transformation consumes 6.5% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t\t('float', [])  :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t\t('object', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t\t('category', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t\t('float', [])    :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t30.7s = Fit runtime\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t20 features in original data used to generate 20 features in processed data.\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \tTrain Data (Processed) Memory Usage: 86.68 MB (0.3% of available memory)\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m Data preprocessing and feature engineering runtime = 32.85s ...\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'mcc'\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m Fitting 108 L1 models ...\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1054.23s of the 1581.72s of remaining time.\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=2.61%)\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m \tRan out of time, early stopping on iteration 361. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=587)\u001b[0m \t[361]\tvalid_set's binary_logloss: 0.0391058\n",
      "\u001b[36m(_ray_fit pid=628)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=628)\u001b[0m \tRan out of time, early stopping on iteration 388. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=628)\u001b[0m \t[388]\tvalid_set's binary_logloss: 0.0380976\n",
      "\u001b[36m(_ray_fit pid=669)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=669)\u001b[0m \tRan out of time, early stopping on iteration 382. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=669)\u001b[0m \t[382]\tvalid_set's binary_logloss: 0.0388503\n",
      "\u001b[36m(_ray_fit pid=710)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=710)\u001b[0m \tRan out of time, early stopping on iteration 389. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=710)\u001b[0m \t[389]\tvalid_set's binary_logloss: 0.0386634\n",
      "\u001b[36m(_ray_fit pid=751)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=751)\u001b[0m \tRan out of time, early stopping on iteration 389. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=751)\u001b[0m \t[389]\tvalid_set's binary_logloss: 0.0389992\n",
      "\u001b[36m(_ray_fit pid=793)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=793)\u001b[0m \tRan out of time, early stopping on iteration 386. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=793)\u001b[0m \t[386]\tvalid_set's binary_logloss: 0.0385856\n",
      "\u001b[36m(_ray_fit pid=834)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=834)\u001b[0m \tRan out of time, early stopping on iteration 392. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=834)\u001b[0m \t[392]\tvalid_set's binary_logloss: 0.0379418\n",
      "\u001b[36m(_ray_fit pid=875)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=875)\u001b[0m \tRan out of time, early stopping on iteration 385. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=875)\u001b[0m \t[385]\tvalid_set's binary_logloss: 0.0378883\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t0.9832\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t951.22s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t84.63s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 86.21s of the 613.7s of remaining time.\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=2.64%)\n",
      "\u001b[36m(_ray_fit pid=1144)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1144)\u001b[0m \tRan out of time, early stopping on iteration 18. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1144)\u001b[0m \t[18]\tvalid_set's binary_logloss: 0.337957\n",
      "\u001b[36m(_ray_fit pid=1185)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1185)\u001b[0m \tRan out of time, early stopping on iteration 17. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1185)\u001b[0m \t[17]\tvalid_set's binary_logloss: 0.350303\n",
      "\u001b[36m(_ray_fit pid=1226)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1226)\u001b[0m \tRan out of time, early stopping on iteration 17. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1226)\u001b[0m \t[17]\tvalid_set's binary_logloss: 0.351051\n",
      "\u001b[36m(_ray_fit pid=1267)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1267)\u001b[0m \tRan out of time, early stopping on iteration 17. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1267)\u001b[0m \t[17]\tvalid_set's binary_logloss: 0.34879\n",
      "\u001b[36m(_ray_fit pid=1308)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1308)\u001b[0m \tRan out of time, early stopping on iteration 18. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1308)\u001b[0m \t[18]\tvalid_set's binary_logloss: 0.341037\n",
      "\u001b[36m(_ray_fit pid=1349)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1349)\u001b[0m \tRan out of time, early stopping on iteration 17. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1349)\u001b[0m \t[17]\tvalid_set's binary_logloss: 0.348969\n",
      "\u001b[36m(_ray_fit pid=1390)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1390)\u001b[0m \tRan out of time, early stopping on iteration 16. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1390)\u001b[0m \t[16]\tvalid_set's binary_logloss: 0.360208\n",
      "\u001b[36m(_ray_fit pid=1431)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1431)\u001b[0m \tRan out of time, early stopping on iteration 17. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1431)\u001b[0m \t[17]\tvalid_set's binary_logloss: 0.350155\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t0.933\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t104.69s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t3.62s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 505.05s of remaining time.\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t0.9832\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t7.09s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t0.35s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m Fitting 108 L2 models ...\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 497.51s of the 497.37s of remaining time.\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=3.07%)\n",
      "\u001b[36m(_ray_fit pid=1706)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1706)\u001b[0m \tRan out of time, early stopping on iteration 211. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1706)\u001b[0m \t[211]\tvalid_set's binary_logloss: 0.0402687\n",
      "\u001b[36m(_ray_fit pid=1747)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1747)\u001b[0m \tRan out of time, early stopping on iteration 209. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1747)\u001b[0m \t[209]\tvalid_set's binary_logloss: 0.039754\n",
      "\u001b[36m(_ray_fit pid=1788)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1788)\u001b[0m \tRan out of time, early stopping on iteration 208. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1788)\u001b[0m \t[208]\tvalid_set's binary_logloss: 0.0382232\n",
      "\u001b[36m(_ray_fit pid=1829)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1829)\u001b[0m \tRan out of time, early stopping on iteration 203. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1829)\u001b[0m \t[203]\tvalid_set's binary_logloss: 0.0392315\n",
      "\u001b[36m(_ray_fit pid=1870)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1870)\u001b[0m \tRan out of time, early stopping on iteration 209. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1870)\u001b[0m \t[209]\tvalid_set's binary_logloss: 0.0399052\n",
      "\u001b[36m(_ray_fit pid=1911)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1911)\u001b[0m \tRan out of time, early stopping on iteration 206. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1911)\u001b[0m \t[206]\tvalid_set's binary_logloss: 0.0398705\n",
      "\u001b[36m(_ray_fit pid=1952)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1952)\u001b[0m \tRan out of time, early stopping on iteration 208. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1952)\u001b[0m \t[208]\tvalid_set's binary_logloss: 0.0382081\n",
      "\u001b[36m(_ray_fit pid=1993)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1993)\u001b[0m \tRan out of time, early stopping on iteration 204. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1993)\u001b[0m \t[204]\tvalid_set's binary_logloss: 0.0391129\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t0.9823\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t463.03s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t34.88s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 26.64s of the 26.51s of remaining time.\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=3.07%)\n",
      "\u001b[36m(_ray_fit pid=2268)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2268)\u001b[0m \tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=2268)\u001b[0m \t[1]\tvalid_set's binary_logloss: 0.641445\n",
      "\u001b[36m(_ray_fit pid=2309)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2309)\u001b[0m \tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=2309)\u001b[0m \t[1]\tvalid_set's binary_logloss: 0.641431\n",
      "\u001b[36m(_ray_fit pid=2350)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2350)\u001b[0m \tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=2350)\u001b[0m \t[1]\tvalid_set's binary_logloss: 0.641389\n",
      "\u001b[36m(_ray_fit pid=2391)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2391)\u001b[0m \tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=2391)\u001b[0m \t[1]\tvalid_set's binary_logloss: 0.641423\n",
      "\u001b[36m(_ray_fit pid=2432)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2432)\u001b[0m \tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=2432)\u001b[0m \t[1]\tvalid_set's binary_logloss: 0.641443\n",
      "\u001b[36m(_ray_fit pid=2473)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2473)\u001b[0m \tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=2473)\u001b[0m \t[1]\tvalid_set's binary_logloss: 0.641435\n",
      "\u001b[36m(_ray_fit pid=2514)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2514)\u001b[0m \tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=2514)\u001b[0m \t[1]\tvalid_set's binary_logloss: 0.641414\n",
      "\u001b[36m(_ray_fit pid=2555)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2555)\u001b[0m \tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=2555)\u001b[0m \t[1]\tvalid_set's binary_logloss: 0.641412\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t0.0\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t70.19s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t1.25s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the -48.24s of remaining time.\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.647, 'LightGBM_BAG_L2': 0.235, 'LightGBMXT_BAG_L2': 0.118}\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t0.9833\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t13.5s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m \t0.37s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m AutoGluon training complete, total runtime = 1677.03s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 2226.8 rows/s (277062 batch size)\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240807_042340/ds_sub_fit/sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=182)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                 model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0    LightGBMXT_BAG_L1       0.982915   0.983206         mcc       29.083969      84.632975   951.218076                29.083969               84.632975         951.218076            1       True          1\n",
      "1  WeightedEnsemble_L2       0.982915   0.983206         mcc       29.088422      84.983534   958.311121                 0.004452                0.350559           7.093045            2       True          3\n",
      "2  WeightedEnsemble_L3       0.982899   0.983281         mcc       44.530287     124.745533  1602.619760                 0.006722                0.367709          13.496524            3       True          6\n",
      "3    LightGBMXT_BAG_L2       0.982214   0.982273         mcc       43.637246     123.130504  1518.937567                12.814021               34.879348         463.031296            2       True          4\n",
      "4      LightGBM_BAG_L1       0.935038   0.932993         mcc        1.739256       3.618181   104.688195                 1.739256                3.618181         104.688195            1       True          2\n",
      "5      LightGBM_BAG_L2       0.000000   0.000000         mcc       31.709544      89.498476  1126.091940                 0.886319                1.247320          70.185669            2       True          5\n",
      "\t0\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n",
      "\t1740s\t = DyStack   runtime |\t4740s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=0.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\n",
      "Beginning AutoGluon training ... Time limit = 4740s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240807_042340\"\n",
      "Train Data Rows:    2493556\n",
      "Train Data Columns: 20\n",
      "Label Column:       class\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    29660.74 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2087.06 MB (7.0% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 7.0% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t\t('object', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\t\t('float', [])    :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t27.2s = Fit runtime\n",
      "\t20 features in original data used to generate 20 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 97.51 MB (0.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 28.57s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mcc'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 4710.94s of the 4710.92s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=2.83%)\n",
      "\t0.9847\t = Validation score   (mcc)\n",
      "\t4357.95s\t = Training   runtime\n",
      "\t624.38s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 271.13s of the 271.11s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=2.83%)\n",
      "\t0.9705\t = Validation score   (mcc)\n",
      "\t265.61s\t = Training   runtime\n",
      "\t16.54s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 471.09s of the -0.3s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t0.9847\t = Validation score   (mcc)\n",
      "\t7.05s\t = Training   runtime\n",
      "\t0.41s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 4747.64s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 499.2 rows/s (311695 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240807_042340\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240807_063940\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jun 27 20:43:36 UTC 2024\n",
      "CPU Count:          4\n",
      "Memory Avail:       26.74 GB / 31.36 GB (85.3%)\n",
      "Disk Space Avail:   19.44 GB / 19.52 GB (99.6%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 1620s of the 6480s of remaining time (25%).\n",
      "\t\tContext path: \"AutogluonModels/ag-20240807_063940/ds_sub_fit/sub_fit_ho\"\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                 model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L3       0.983537   0.983097         mcc       45.351909     126.380967  1609.119970                 0.007395                0.370913          13.672233            3       True          6\n",
      "1    LightGBMXT_BAG_L1       0.983524   0.983032         mcc       29.945627      86.257536   956.563704                29.945627               86.257536         956.563704            1       True          1\n",
      "2  WeightedEnsemble_L2       0.983524   0.983032         mcc       29.949653      86.608116   963.699994                 0.004026                0.350580           7.136290            2       True          3\n",
      "3    LightGBMXT_BAG_L2       0.982722   0.982137         mcc       44.314770     124.738758  1524.476115                12.523790               34.886894         463.034786            2       True          4\n",
      "4      LightGBM_BAG_L1       0.935536   0.928196         mcc        1.845353       3.594327   104.877625                 1.845353                3.594327         104.877625            1       True          2\n",
      "5      LightGBM_BAG_L2       0.000000   0.000000         mcc       32.820724      91.123160  1132.412950                 1.029744                1.271296          70.971621            2       True          5\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t1742s\t = DyStack   runtime |\t4738s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 4738s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240807_063940\"\n",
      "Train Data Rows:    2493556\n",
      "Train Data Columns: 20\n",
      "Label Column:       class\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    29054.00 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2086.98 MB (7.2% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 7.2% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t\t('object', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\t\t('float', [])    :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t27.2s = Fit runtime\n",
      "\t20 features in original data used to generate 20 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 97.51 MB (0.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 28.61s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mcc'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3139.04s of the 4709.72s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=2.89%)\n",
      "\t0.9844\t = Validation score   (mcc)\n",
      "\t2894.09s\t = Training   runtime\n",
      "\t395.03s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 191.06s of the 1761.74s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=2.89%)\n",
      "\t0.9609\t = Validation score   (mcc)\n",
      "\t196.1s\t = Training   runtime\n",
      "\t10.7s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 1560.64s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t0.9844\t = Validation score   (mcc)\n",
      "\t6.19s\t = Training   runtime\n",
      "\t0.41s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1553.92s of the 1553.78s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=3.36%)\n",
      "\t0.9842\t = Validation score   (mcc)\n",
      "\t1411.32s\t = Training   runtime\n",
      "\t151.1s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 119.31s of the 119.17s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=3.36%)\n",
      "\t0.9839\t = Validation score   (mcc)\n",
      "\t132.69s\t = Training   runtime\n",
      "\t3.44s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the -18.7s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L2': 0.5, 'LightGBMXT_BAG_L1': 0.318, 'LightGBM_BAG_L2': 0.182}\n",
      "\t0.9844\t = Validation score   (mcc)\n",
      "\t11.82s\t = Training   runtime\n",
      "\t0.42s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 4769.69s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 788.9 rows/s (311695 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240807_063940\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240807_084609\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jun 27 20:43:36 UTC 2024\n",
      "CPU Count:          4\n",
      "Memory Avail:       26.68 GB / 31.36 GB (85.1%)\n",
      "Disk Space Avail:   19.39 GB / 19.52 GB (99.3%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 1620s of the 6480s of remaining time (25%).\n",
      "\t\tContext path: \"AutogluonModels/ag-20240807_084609/ds_sub_fit/sub_fit_ho\"\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                 model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L3       0.983528   0.983105         mcc       45.669934     128.165965  1608.418921                 0.006828                0.364322          13.220601            3       True          6\n",
      "1    LightGBMXT_BAG_L1       0.983413   0.983035         mcc       29.828273      87.430119   956.227732                29.828273               87.430119         956.227732            1       True          1\n",
      "2  WeightedEnsemble_L2       0.983413   0.983035         mcc       29.832308      87.775932   963.305776                 0.004034                0.345813           7.078043            2       True          3\n",
      "3    LightGBMXT_BAG_L2       0.983005   0.982157         mcc       44.668299     126.543933  1524.754292                13.011672               35.542518         464.384877            2       True          4\n",
      "4      LightGBM_BAG_L1       0.937025   0.929856         mcc        1.828354       3.571295   104.141682                 1.828354                3.571295         104.141682            1       True          2\n",
      "5      LightGBM_BAG_L2       0.000000   0.000000         mcc       32.651434      92.259124  1130.813444                 0.994807                1.257710          70.444029            2       True          5\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t1742s\t = DyStack   runtime |\t4738s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 4738s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240807_084609\"\n",
      "Train Data Rows:    2493556\n",
      "Train Data Columns: 20\n",
      "Label Column:       class\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    29007.24 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2087.00 MB (7.2% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 7.2% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t\t('object', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\t\t('float', [])    :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t26.8s = Fit runtime\n",
      "\t20 features in original data used to generate 20 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 97.51 MB (0.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 28.19s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mcc'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3139.3s of the 4710.11s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=2.89%)\n",
      "\t0.9846\t = Validation score   (mcc)\n",
      "\t2891.89s\t = Training   runtime\n",
      "\t393.24s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 193.28s of the 1764.09s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=2.90%)\n",
      "\t0.9614\t = Validation score   (mcc)\n",
      "\t198.69s\t = Training   runtime\n",
      "\t10.77s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 1560.4s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t0.9846\t = Validation score   (mcc)\n",
      "\t6.22s\t = Training   runtime\n",
      "\t0.41s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1553.66s of the 1553.54s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=3.36%)\n",
      "\t0.9843\t = Validation score   (mcc)\n",
      "\t1414.47s\t = Training   runtime\n",
      "\t154.32s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 115.26s of the 115.14s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=3.36%)\n",
      "\t0.984\t = Validation score   (mcc)\n",
      "\t130.41s\t = Training   runtime\n",
      "\t3.39s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the -20.56s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.643, 'LightGBM_BAG_L2': 0.214, 'LightGBMXT_BAG_L2': 0.143}\n",
      "\t0.9846\t = Validation score   (mcc)\n",
      "\t12.18s\t = Training   runtime\n",
      "\t0.4s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 4771.82s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 554.9 rows/s (311695 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240807_084609\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240807_110109\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jun 27 20:43:36 UTC 2024\n",
      "CPU Count:          4\n",
      "Memory Avail:       26.54 GB / 31.36 GB (84.6%)\n",
      "Disk Space Avail:   19.34 GB / 19.52 GB (99.1%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 1620s of the 6480s of remaining time (25%).\n",
      "\t\tContext path: \"AutogluonModels/ag-20240807_110109/ds_sub_fit/sub_fit_ho\"\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                 model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0    LightGBMXT_BAG_L1       0.983677   0.983078         mcc       29.027697      86.380125   956.149778                29.027697               86.380125         956.149778            1       True          1\n",
      "1  WeightedEnsemble_L2       0.983677   0.983078         mcc       29.031931      86.727717   963.103917                 0.004234                0.347592           6.954139            2       True          3\n",
      "2  WeightedEnsemble_L3       0.983601   0.983219         mcc       44.293734     126.729285  1607.419398                 0.007130                0.361381          13.360509            3       True          6\n",
      "3    LightGBMXT_BAG_L2       0.982882   0.982124         mcc       43.294697     125.135584  1523.971910                12.461166               35.175291         463.496258            2       True          4\n",
      "4      LightGBM_BAG_L1       0.935093   0.932327         mcc        1.805834       3.580168   104.325875                 1.805834                3.580168         104.325875            1       True          2\n",
      "5      LightGBM_BAG_L2       0.000000   0.000000         mcc       31.825439      91.192614  1130.562632                 0.991908                1.232320          70.086979            2       True          5\n",
      "\t0\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n",
      "\t1739s\t = DyStack   runtime |\t4741s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=0.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\n",
      "Beginning AutoGluon training ... Time limit = 4741s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240807_110109\"\n",
      "Train Data Rows:    2493556\n",
      "Train Data Columns: 20\n",
      "Label Column:       class\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    28904.55 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2087.06 MB (7.2% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 7.2% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t\t('object', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\t\t('float', [])    :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t26.7s = Fit runtime\n",
      "\t20 features in original data used to generate 20 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 97.51 MB (0.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 28.11s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mcc'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 4712.65s of the 4712.62s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=2.89%)\n",
      "\t0.9847\t = Validation score   (mcc)\n",
      "\t4365.39s\t = Training   runtime\n",
      "\t633.0s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 264.95s of the 264.92s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=2.89%)\n",
      "\t0.9699\t = Validation score   (mcc)\n",
      "\t259.79s\t = Training   runtime\n",
      "\t15.97s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 471.26s of the -0.52s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.955, 'LightGBM_BAG_L1': 0.045}\n",
      "\t0.9847\t = Validation score   (mcc)\n",
      "\t6.19s\t = Training   runtime\n",
      "\t0.44s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 4748.3s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 480.3 rows/s (311695 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240807_110109\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240807_131811\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jun 27 20:43:36 UTC 2024\n",
      "CPU Count:          4\n",
      "Memory Avail:       26.53 GB / 31.36 GB (84.6%)\n",
      "Disk Space Avail:   19.28 GB / 19.52 GB (98.8%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 1620s of the 6480s of remaining time (25%).\n",
      "\t\tContext path: \"AutogluonModels/ag-20240807_131811/ds_sub_fit/sub_fit_ho\"\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                 model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L3       0.983516   0.983111         mcc       44.693913     124.867901  1610.403422                 0.007037                0.392071          14.003116            3       True          6\n",
      "1    LightGBMXT_BAG_L1       0.983410   0.983050         mcc       29.100660      85.222477   955.512064                29.100660               85.222477         955.512064            1       True          1\n",
      "2  WeightedEnsemble_L2       0.983410   0.983050         mcc       29.105553      85.595084   962.817731                 0.004894                0.372607           7.305667            2       True          3\n",
      "3    LightGBMXT_BAG_L2       0.982810   0.982082         mcc       43.675479     123.202235  1524.492876                12.483423               34.357306         462.865798            2       True          4\n",
      "4      LightGBM_BAG_L1       0.932047   0.928584         mcc        2.091396       3.622452   106.115014                 2.091396                3.622452         106.115014            1       True          2\n",
      "5      LightGBM_BAG_L2       0.000000   0.000000         mcc       32.203453      90.118524  1133.534507                 1.011397                1.273595          71.907429            2       True          5\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t1743s\t = DyStack   runtime |\t4737s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 4737s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240807_131811\"\n",
      "Train Data Rows:    2493556\n",
      "Train Data Columns: 20\n",
      "Label Column:       class\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    28894.89 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2086.98 MB (7.2% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 7.2% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t\t('object', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\t\t('float', [])    :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t27.6s = Fit runtime\n",
      "\t20 features in original data used to generate 20 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 97.51 MB (0.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 29.02s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mcc'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3137.86s of the 4707.94s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=2.89%)\n",
      "\t0.9845\t = Validation score   (mcc)\n",
      "\t2891.01s\t = Training   runtime\n",
      "\t391.44s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 195.12s of the 1765.21s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=2.90%)\n",
      "\t0.9616\t = Validation score   (mcc)\n",
      "\t199.72s\t = Training   runtime\n",
      "\t10.98s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 1560.53s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t0.9845\t = Validation score   (mcc)\n",
      "\t6.12s\t = Training   runtime\n",
      "\t0.36s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1553.93s of the 1553.81s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=3.36%)\n",
      "\t0.9843\t = Validation score   (mcc)\n",
      "\t1414.23s\t = Training   runtime\n",
      "\t153.27s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 116.5s of the 116.37s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=3.37%)\n",
      "\t0.9842\t = Validation score   (mcc)\n",
      "\t131.36s\t = Training   runtime\n",
      "\t3.66s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the -20.18s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.5, 'LightGBMXT_BAG_L2': 0.375, 'LightGBM_BAG_L2': 0.125}\n",
      "\t0.9845\t = Validation score   (mcc)\n",
      "\t12.07s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 4769.87s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 557.2 rows/s (311695 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240807_131811\")\n"
     ]
    }
   ],
   "source": [
    "scores = []    \n",
    "oof_pred_probs = np.zeros((train.shape[0], len(np.unique(train[TARGET]))))\n",
    "test_pred_probs = np.zeros((test.shape[0], len(np.unique(train[TARGET]))))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, random_state=SEED, shuffle=True)\n",
    "split = skf.split(train.drop(columns=TARGET), train[TARGET])\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(split):\n",
    "    _train, _val = train.iloc[train_idx], train.iloc[val_idx]   \n",
    "    \n",
    "    predictor = TabularPredictor(\n",
    "        label=TARGET,\n",
    "        eval_metric='mcc',\n",
    "        problem_type='binary',\n",
    "        verbosity=2\n",
    "    )\n",
    "    \n",
    "    predictor.fit(\n",
    "        train_data=_train,\n",
    "        time_limit=TIME_LIMIT // N_FOLDS,\n",
    "        presets='best_quality',\n",
    "        save_space=True,\n",
    "        excluded_model_types=['KNN'],\n",
    "        ag_args_fit={\n",
    "            'num_gpus': 1, \n",
    "            'stopping_metric': 'log_loss'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    y_pred_probs = predictor.predict_proba(_val)\n",
    "    oof_pred_probs[val_idx] = y_pred_probs            \n",
    "\n",
    "    temp_test_pred_probs = predictor.predict_proba(test)\n",
    "    test_pred_probs += temp_test_pred_probs / N_FOLDS\n",
    "\n",
    "    score = matthews_corrcoef(_val[TARGET], np.argmax(y_pred_probs, axis=1))\n",
    "    scores.append(score)\n",
    "\n",
    "    del predictor, _train, _val, y_pred_probs, temp_test_pred_probs\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee796e2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T15:32:34.322987Z",
     "iopub.status.busy": "2024-08-07T15:32:34.321692Z",
     "iopub.status.idle": "2024-08-07T15:32:34.327786Z",
     "shell.execute_reply": "2024-08-07T15:32:34.326882Z"
    },
    "papermill": {
     "duration": 0.054655,
     "end_time": "2024-08-07T15:32:34.330247",
     "exception": false,
     "start_time": "2024-08-07T15:32:34.275592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - MCC: 0.984550\n",
      "Fold 2 - MCC: 0.984842\n",
      "Fold 3 - MCC: 0.984530\n",
      "Fold 4 - MCC: 0.984742\n",
      "Fold 5 - MCC: 0.984701\n",
      "\n",
      "--- Average MCC: 0.984673  0.000118\n"
     ]
    }
   ],
   "source": [
    "for i in range(N_FOLDS):\n",
    "    print(f'Fold {i + 1} - MCC: {scores[i]:.6f}')\n",
    "    \n",
    "print(f'\\n--- Average MCC: {np.mean(scores):.6f}  {np.std(scores):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2b419f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T15:32:34.420101Z",
     "iopub.status.busy": "2024-08-07T15:32:34.419349Z",
     "iopub.status.idle": "2024-08-07T15:32:37.693885Z",
     "shell.execute_reply": "2024-08-07T15:32:37.692860Z"
    },
    "papermill": {
     "duration": 3.321886,
     "end_time": "2024-08-07T15:32:37.696290",
     "exception": false,
     "start_time": "2024-08-07T15:32:34.374404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_pred_probs(oof_pred_probs, np.mean(scores), 'oof')\n",
    "save_pred_probs(test_pred_probs, np.mean(scores), 'test')\n",
    "save_submission(test_pred_probs, np.mean(scores)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "195063c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T15:32:37.787457Z",
     "iopub.status.busy": "2024-08-07T15:32:37.787125Z",
     "iopub.status.idle": "2024-08-07T15:32:37.863033Z",
     "shell.execute_reply": "2024-08-07T15:32:37.862228Z"
    },
    "papermill": {
     "duration": 0.12372,
     "end_time": "2024-08-07T15:32:37.865222",
     "exception": false,
     "start_time": "2024-08-07T15:32:37.741502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "shutil.rmtree(\"AutogluonModels\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9045607,
     "sourceId": 76727,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 40210.83389,
   "end_time": "2024-08-07T15:32:43.209325",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-07T04:22:32.375435",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
