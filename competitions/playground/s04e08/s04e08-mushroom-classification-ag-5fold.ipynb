{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e085af9e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-07T15:39:00.035924Z",
     "iopub.status.busy": "2024-08-07T15:39:00.035583Z",
     "iopub.status.idle": "2024-08-07T15:39:46.697272Z",
     "shell.execute_reply": "2024-08-07T15:39:46.695992Z"
    },
    "papermill": {
     "duration": 46.668678,
     "end_time": "2024-08-07T15:39:46.699690",
     "exception": false,
     "start_time": "2024-08-07T15:39:00.031012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "aiobotocore 2.13.1 requires aiohttp<4.0.0,>=3.9.2, but you have aiohttp 3.9.1 which is incompatible.\r\n",
      "aiobotocore 2.13.1 requires botocore<1.34.132,>=1.34.70, but you have botocore 1.29.165 which is incompatible.\r\n",
      "spaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "spopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q autogluon.tabular\n",
    "!pip install -q ray==2.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8948c46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T15:39:46.707482Z",
     "iopub.status.busy": "2024-08-07T15:39:46.707184Z",
     "iopub.status.idle": "2024-08-07T15:39:49.373460Z",
     "shell.execute_reply": "2024-08-07T15:39:49.372652Z"
    },
    "papermill": {
     "duration": 2.672733,
     "end_time": "2024-08-07T15:39:49.375841",
     "exception": false,
     "start_time": "2024-08-07T15:39:46.703108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.base import clone\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pickle\n",
    "import shutil\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f651f9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T15:39:49.383529Z",
     "iopub.status.busy": "2024-08-07T15:39:49.383043Z",
     "iopub.status.idle": "2024-08-07T15:39:49.387448Z",
     "shell.execute_reply": "2024-08-07T15:39:49.386579Z"
    },
    "papermill": {
     "duration": 0.010182,
     "end_time": "2024-08-07T15:39:49.389292",
     "exception": false,
     "start_time": "2024-08-07T15:39:49.379110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEED = 6\n",
    "N_FOLDS = 5\n",
    "TARGET = 'class'\n",
    "TIME_LIMIT = 3600 * 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7be3bd33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T15:39:49.396331Z",
     "iopub.status.busy": "2024-08-07T15:39:49.396034Z",
     "iopub.status.idle": "2024-08-07T15:40:04.431780Z",
     "shell.execute_reply": "2024-08-07T15:40:04.430946Z"
    },
    "papermill": {
     "duration": 15.041791,
     "end_time": "2024-08-07T15:40:04.434204",
     "exception": false,
     "start_time": "2024-08-07T15:39:49.392413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/playground-series-s4e8/train.csv', index_col='id')\n",
    "test = pd.read_csv('/kaggle/input/playground-series-s4e8/test.csv', index_col='id')\n",
    "\n",
    "train[TARGET] = train[TARGET].map({'e': 0, 'p': 1})\n",
    "\n",
    "train = TabularDataset(train)\n",
    "test = TabularDataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95735c41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T15:40:04.441977Z",
     "iopub.status.busy": "2024-08-07T15:40:04.441697Z",
     "iopub.status.idle": "2024-08-07T15:40:04.447664Z",
     "shell.execute_reply": "2024-08-07T15:40:04.446830Z"
    },
    "papermill": {
     "duration": 0.011855,
     "end_time": "2024-08-07T15:40:04.449472",
     "exception": false,
     "start_time": "2024-08-07T15:40:04.437617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_pred_probs(pred_probs, cv_score, name):\n",
    "    with open(f'autogluon_{name}_pred_probs_{cv_score:.6f}.pkl', 'wb') as f:\n",
    "        pickle.dump(pred_probs, f)\n",
    "\n",
    "def save_submission(test_pred_probs, score):\n",
    "    sub = pd.read_csv('/kaggle/input/playground-series-s4e8/sample_submission.csv')\n",
    "    sub[TARGET] = np.argmax(test_pred_probs, axis=1)\n",
    "    sub[TARGET] = sub[TARGET].map({0: 'e', 1: 'p'})\n",
    "    sub.to_csv(f'sub_autogluon_{score:.6f}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "831b3b83",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-08-07T15:40:04.456609Z",
     "iopub.status.busy": "2024-08-07T15:40:04.456371Z",
     "iopub.status.idle": "2024-08-08T01:54:02.206623Z",
     "shell.execute_reply": "2024-08-08T01:54:02.205611Z"
    },
    "papermill": {
     "duration": 36837.757646,
     "end_time": "2024-08-08T01:54:02.210240",
     "exception": false,
     "start_time": "2024-08-07T15:40:04.452594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240807_154006\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jun 27 20:43:36 UTC 2024\n",
      "CPU Count:          4\n",
      "Memory Avail:       28.19 GB / 31.36 GB (89.9%)\n",
      "Disk Space Avail:   19.50 GB / 19.52 GB (99.9%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 1620s of the 6480s of remaining time (25%).\n",
      "2024-08-07 15:40:06,806\tINFO util.py:124 -- Outdated packages:\n",
      "  ipywidgets==7.7.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2024-08-07 15:40:09,597\tINFO worker.py:1752 -- Started a local Ray instance.\n",
      "\t\tContext path: \"AutogluonModels/ag-20240807_154006/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m Beginning AutoGluon training ... Time limit = 1615s\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m AutoGluon will save models to \"AutogluonModels/ag-20240807_154006/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m Train Data Rows:    2216494\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m Train Data Columns: 20\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m Label Column:       class\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m Problem Type:       binary\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \tAvailable Memory:                    28507.84 MB\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \tTrain Data (Original)  Memory Usage: 1854.97 MB (6.5% of available memory)\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \tWarning: Data size prior to feature transformation consumes 6.5% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t\t('float', [])  :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t\t('object', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t\t('category', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t\t('float', [])    :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t30.3s = Fit runtime\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t20 features in original data used to generate 20 features in processed data.\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \tTrain Data (Processed) Memory Usage: 86.68 MB (0.3% of available memory)\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m Data preprocessing and feature engineering runtime = 32.51s ...\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'mcc'\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m Fitting 108 L1 models ...\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1054.93s of the 1582.78s of remaining time.\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=2.61%)\n",
      "\u001b[36m(_ray_fit pid=581)\u001b[0m \tRan out of time, early stopping on iteration 160. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=581)\u001b[0m \t[160]\tvalid_set's binary_logloss: 0.0530795\tvalid_set's mcc: 0.979181\n",
      "\u001b[36m(_ray_fit pid=703)\u001b[0m \tRan out of time, early stopping on iteration 164. Best iteration is:\u001b[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=703)\u001b[0m \t[163]\tvalid_set's binary_logloss: 0.0518793\tvalid_set's mcc: 0.978882\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t0.9789\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t874.76s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t150.56s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 153.38s of the 681.22s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=804)\u001b[0m \tRan out of time, early stopping on iteration 163. Best iteration is:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=804)\u001b[0m \t[163]\tvalid_set's binary_logloss: 0.0517308\tvalid_set's mcc: 0.978919\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=2.64%)\n",
      "\u001b[36m(_ray_fit pid=1095)\u001b[0m \tRan out of time, early stopping on iteration 17. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1095)\u001b[0m \t[16]\tvalid_set's binary_logloss: 0.361575\tvalid_set's mcc: 0.935582\n",
      "\u001b[36m(_ray_fit pid=1096)\u001b[0m \tRan out of time, early stopping on iteration 17. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1096)\u001b[0m \t[17]\tvalid_set's binary_logloss: 0.350303\tvalid_set's mcc: 0.933908\n",
      "\u001b[36m(_ray_fit pid=1217)\u001b[0m \tRan out of time, early stopping on iteration 18. Best iteration is:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1217)\u001b[0m \t[17]\tvalid_set's binary_logloss: 0.35191\tvalid_set's mcc: 0.931132\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t0.9333\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t135.69s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t13.45s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 11.24s of the 539.09s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=1293)\u001b[0m \tRan out of time, early stopping on iteration 18. Best iteration is:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1293)\u001b[0m \t[16]\tvalid_set's binary_logloss: 0.362055\tvalid_set's mcc: 0.931776\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \tWarning: Model is expected to require 779.7s to train, which exceeds the maximum time limit of 11.2s, skipping model...\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \tTime limit exceeded... Skipping RandomForestGini_BAG_L1.\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 527.57s of remaining time.\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t0.9789\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t19.62s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t0.95s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m Fitting 108 L2 models ...\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 506.92s of the 506.82s of remaining time.\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=3.09%)\n",
      "\u001b[36m(_ray_fit pid=1623)\u001b[0m \tRan out of time, early stopping on iteration 84. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1623)\u001b[0m \t[83]\tvalid_set's binary_logloss: 0.0504094\tvalid_set's mcc: 0.979435\n",
      "\u001b[36m(_ray_fit pid=1744)\u001b[0m \tRan out of time, early stopping on iteration 85. Best iteration is:\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1744)\u001b[0m \t[84]\tvalid_set's binary_logloss: 0.0521784\tvalid_set's mcc: 0.977612\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t0.9785\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t422.71s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t44.51s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 73.09s of the 72.99s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=1818)\u001b[0m \tRan out of time, early stopping on iteration 87. Best iteration is:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1818)\u001b[0m \t[87]\tvalid_set's binary_logloss: 0.0493018\tvalid_set's mcc: 0.979264\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=3.09%)\n",
      "\u001b[36m(_ray_fit pid=2139)\u001b[0m \tRan out of time, early stopping on iteration 5. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=2139)\u001b[0m \t[5]\tvalid_set's binary_logloss: 0.493445\tvalid_set's mcc: 0.978636\n",
      "\u001b[36m(_ray_fit pid=2141)\u001b[0m \tRan out of time, early stopping on iteration 5. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=2141)\u001b[0m \t[5]\tvalid_set's binary_logloss: 0.493178\tvalid_set's mcc: 0.980291\n",
      "\u001b[36m(_ray_fit pid=2270)\u001b[0m \tRan out of time, early stopping on iteration 5. Best iteration is:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2270)\u001b[0m \t[5]\tvalid_set's binary_logloss: 0.49342\tvalid_set's mcc: 0.978585\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t0.9794\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t68.16s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t5.7s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the -2.16s of remaining time.\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \tEnsemble Weights: {'LightGBM_BAG_L2': 0.667, 'LightGBMXT_BAG_L2': 0.238, 'LightGBMXT_BAG_L1': 0.095}\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t0.9803\t = Validation score   (mcc)\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t39.19s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m \t0.96s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m AutoGluon training complete, total runtime = 1657.94s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 1292.6 rows/s (277062 batch size)\n",
      "\u001b[36m(_ray_fit pid=2286)\u001b[0m \tRan out of time, early stopping on iteration 5. Best iteration is:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2286)\u001b[0m \t[5]\tvalid_set's binary_logloss: 0.493276\tvalid_set's mcc: 0.97951\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240807_154006/ds_sub_fit/sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=166)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                 model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L3       0.980249   0.980266         mcc       28.709580     215.177575  1540.509410                 0.006397                0.956297          39.188761            3       True          6\n",
      "1      LightGBM_BAG_L2       0.979875   0.979365         mcc       22.848640     169.708772  1078.606573                 0.962065                5.704401          68.161219            2       True          5\n",
      "2    LightGBMXT_BAG_L1       0.979141   0.978930         mcc       19.960976     150.556115   874.758517                19.960976              150.556115         874.758517            1       True          1\n",
      "3  WeightedEnsemble_L2       0.979141   0.978930         mcc       19.964829     151.506042   894.379805                 0.003853                0.949927          19.621288            2       True          3\n",
      "4    LightGBMXT_BAG_L2       0.978815   0.978497         mcc       27.741119     208.516878  1433.159430                 5.854543               44.512507         422.714076            2       True          4\n",
      "5      LightGBM_BAG_L1       0.935188   0.933297         mcc        1.925600      13.448256   135.686837                 1.925600               13.448256         135.686837            1       True          2\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t1706s\t = DyStack   runtime |\t4774s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 4774s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240807_154006\"\n",
      "Train Data Rows:    2493556\n",
      "Train Data Columns: 20\n",
      "Label Column:       class\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    29700.58 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2087.06 MB (7.0% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 7.0% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t\t('object', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\t\t('float', [])    :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t27.2s = Fit runtime\n",
      "\t20 features in original data used to generate 20 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 97.51 MB (0.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 28.69s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mcc'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3162.5s of the 4744.92s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=2.82%)\n",
      "\t0.9836\t = Validation score   (mcc)\n",
      "\t2608.11s\t = Training   runtime\n",
      "\t512.2s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 482.2s of the 2064.62s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=2.82%)\n",
      "\t0.9679\t = Validation score   (mcc)\n",
      "\t406.07s\t = Training   runtime\n",
      "\t58.19s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 63.45s of the 1645.86s of remaining time.\n",
      "\tWarning: Model is expected to require 872.2s to train, which exceeds the maximum time limit of 63.4s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestGini_BAG_L1.\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 50.7s of the 1633.12s of remaining time.\n",
      "\tWarning: Model is expected to require 964.8s to train, which exceeds the maximum time limit of 50.7s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestEntr_BAG_L1.\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 36.82s of the 1619.23s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=2.87%)\n",
      "\t0.5353\t = Validation score   (mcc)\n",
      "\t61.74s\t = Training   runtime\n",
      "\t1.06s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 1551.9s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.857, 'CatBoost_BAG_L1': 0.143}\n",
      "\t0.9836\t = Validation score   (mcc)\n",
      "\t29.14s\t = Training   runtime\n",
      "\t1.07s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1521.57s of the 1521.42s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=3.56%)\n",
      "\t0.9832\t = Validation score   (mcc)\n",
      "\t1249.89s\t = Training   runtime\n",
      "\t188.99s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 238.76s of the 238.61s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=3.56%)\n",
      "\t0.9837\t = Validation score   (mcc)\n",
      "\t204.84s\t = Training   runtime\n",
      "\t11.33s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 26.36s of the 26.21s of remaining time.\n",
      "\tWarning: Model is expected to require 1171.0s to train, which exceeds the maximum time limit of 26.4s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestGini_BAG_L2.\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 8.0s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L2': 0.5, 'LightGBM_BAG_L2': 0.5}\n",
      "\t0.9838\t = Validation score   (mcc)\n",
      "\t48.04s\t = Training   runtime\n",
      "\t1.08s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 4815.13s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 403.8 rows/s (311695 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240807_154006\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240807_174254\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jun 27 20:43:36 UTC 2024\n",
      "CPU Count:          4\n",
      "Memory Avail:       26.46 GB / 31.36 GB (84.4%)\n",
      "Disk Space Avail:   19.46 GB / 19.52 GB (99.7%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 1620s of the 6480s of remaining time (25%).\n",
      "\t\tContext path: \"AutogluonModels/ag-20240807_174254/ds_sub_fit/sub_fit_ho\"\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                 model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L3       0.980743   0.980017         mcc       28.801713     219.580350  1544.010847                 0.006603                0.992720          40.350142            3       True          6\n",
      "1      LightGBM_BAG_L2       0.979984   0.979065         mcc       22.865928     173.805007  1080.462308                 0.973814                5.525897          69.430907            2       True          5\n",
      "2    LightGBMXT_BAG_L1       0.979538   0.978693         mcc       19.865302     153.824346   875.565876                19.865302              153.824346         875.565876            1       True          1\n",
      "3  WeightedEnsemble_L2       0.979538   0.978693         mcc       19.869217     154.795368   895.637959                 0.003916                0.971022          20.072083            2       True          3\n",
      "4    LightGBMXT_BAG_L2       0.979164   0.978468         mcc       27.821296     213.061733  1434.229797                 5.929183               44.782624         423.198396            2       True          4\n",
      "5      LightGBM_BAG_L1       0.935042   0.929638         mcc        2.026812      14.454764   135.465525                 2.026812               14.454764         135.465525            1       True          2\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t1712s\t = DyStack   runtime |\t4768s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 4768s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240807_174254\"\n",
      "Train Data Rows:    2493556\n",
      "Train Data Columns: 20\n",
      "Label Column:       class\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    28803.83 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2086.98 MB (7.2% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 7.2% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t\t('object', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\t\t('float', [])    :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t28.0s = Fit runtime\n",
      "\t20 features in original data used to generate 20 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 97.51 MB (0.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 29.62s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mcc'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3158.41s of the 4738.79s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=2.91%)\n",
      "\t0.9835\t = Validation score   (mcc)\n",
      "\t2605.23s\t = Training   runtime\n",
      "\t517.06s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 483.98s of the 2064.35s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=2.91%)\n",
      "\t0.9683\t = Validation score   (mcc)\n",
      "\t407.26s\t = Training   runtime\n",
      "\t59.14s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 64.48s of the 1644.86s of remaining time.\n",
      "\tWarning: Model is expected to require 886.7s to train, which exceeds the maximum time limit of 64.5s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestGini_BAG_L1.\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 51.65s of the 1632.02s of remaining time.\n",
      "\tWarning: Model is expected to require 988.8s to train, which exceeds the maximum time limit of 51.7s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestEntr_BAG_L1.\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 37.47s of the 1617.84s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=2.96%)\n",
      "\t0.5348\t = Validation score   (mcc)\n",
      "\t61.27s\t = Training   runtime\n",
      "\t1.12s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 1550.85s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t0.9835\t = Validation score   (mcc)\n",
      "\t28.67s\t = Training   runtime\n",
      "\t1.05s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1521.02s of the 1520.86s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=3.67%)\n",
      "\t0.9829\t = Validation score   (mcc)\n",
      "\t1249.23s\t = Training   runtime\n",
      "\t162.69s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 239.56s of the 239.4s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=3.67%)\n",
      "\t0.9836\t = Validation score   (mcc)\n",
      "\t206.3s\t = Training   runtime\n",
      "\t11.8s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 26.14s of the 25.99s of remaining time.\n",
      "\tWarning: Model is expected to require 1217.2s to train, which exceeds the maximum time limit of 26.1s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestGini_BAG_L2.\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 6.74s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L2': 0.5, 'LightGBMXT_BAG_L1': 0.25, 'LightGBMXT_BAG_L2': 0.25}\n",
      "\t0.9837\t = Validation score   (mcc)\n",
      "\t47.96s\t = Training   runtime\n",
      "\t1.13s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 4811.13s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 414.5 rows/s (311695 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240807_174254\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240807_194529\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jun 27 20:43:36 UTC 2024\n",
      "CPU Count:          4\n",
      "Memory Avail:       26.22 GB / 31.36 GB (83.6%)\n",
      "Disk Space Avail:   19.42 GB / 19.52 GB (99.5%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 1620s of the 6480s of remaining time (25%).\n",
      "\t\tContext path: \"AutogluonModels/ag-20240807_194529/ds_sub_fit/sub_fit_ho\"\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                 model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L3       0.980721   0.980025         mcc       28.697075     215.237567  1542.774687                 0.007086                0.984070          39.898824            3       True          6\n",
      "1      LightGBM_BAG_L2       0.980042   0.979072         mcc       23.109858     170.759160  1081.727793                 1.001718                5.396155          68.968599            2       True          5\n",
      "2    LightGBMXT_BAG_L2       0.979724   0.978677         mcc       27.688271     208.857343  1433.907264                 5.580131               43.494337         421.148070            2       True          4\n",
      "3    LightGBMXT_BAG_L1       0.979709   0.978714         mcc       19.662443     150.470427   874.472899                19.662443              150.470427         874.472899            1       True          1\n",
      "4  WeightedEnsemble_L2       0.979709   0.978714         mcc       19.666503     151.438387   894.394900                 0.004059                0.967960          19.922000            2       True          3\n",
      "5      LightGBM_BAG_L1       0.936874   0.932616         mcc        2.445697      14.892578   138.286295                 2.445697               14.892578         138.286295            1       True          2\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t1710s\t = DyStack   runtime |\t4770s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 4770s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240807_194529\"\n",
      "Train Data Rows:    2493556\n",
      "Train Data Columns: 20\n",
      "Label Column:       class\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    28448.54 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2087.00 MB (7.3% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 7.3% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t\t('object', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\t\t('float', [])    :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t26.5s = Fit runtime\n",
      "\t20 features in original data used to generate 20 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 97.51 MB (0.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 28.07s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mcc'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3160.63s of the 4742.11s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=2.95%)\n",
      "\t0.9836\t = Validation score   (mcc)\n",
      "\t2606.92s\t = Training   runtime\n",
      "\t512.25s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 484.81s of the 2066.29s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=2.94%)\n",
      "\t0.9684\t = Validation score   (mcc)\n",
      "\t408.03s\t = Training   runtime\n",
      "\t60.91s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 64.09s of the 1645.57s of remaining time.\n",
      "\tWarning: Model is expected to require 1015.1s to train, which exceeds the maximum time limit of 64.1s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestGini_BAG_L1.\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 49.28s of the 1630.77s of remaining time.\n",
      "\tWarning: Model is expected to require 1015.3s to train, which exceeds the maximum time limit of 49.3s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestEntr_BAG_L1.\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 34.7s of the 1616.18s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=2.99%)\n",
      "\t0.5359\t = Validation score   (mcc)\n",
      "\t61.93s\t = Training   runtime\n",
      "\t1.04s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 1548.52s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t0.9836\t = Validation score   (mcc)\n",
      "\t29.24s\t = Training   runtime\n",
      "\t1.11s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1518.05s of the 1517.89s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=3.70%)\n",
      "\t0.9831\t = Validation score   (mcc)\n",
      "\t1247.96s\t = Training   runtime\n",
      "\t170.38s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 239.5s of the 239.33s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 10.05% memory usage per fold, 40.19%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=10.05%)\n",
      "\t0.9836\t = Validation score   (mcc)\n",
      "\t204.82s\t = Training   runtime\n",
      "\t11.25s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 26.07s of the 25.91s of remaining time.\n",
      "\tWarning: Model is expected to require 1192.8s to train, which exceeds the maximum time limit of 26.1s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestGini_BAG_L2.\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 7.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.4, 'LightGBM_BAG_L2': 0.4, 'LightGBMXT_BAG_L2': 0.2}\n",
      "\t0.9838\t = Validation score   (mcc)\n",
      "\t48.96s\t = Training   runtime\n",
      "\t1.1s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 4813.54s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 412.3 rows/s (311695 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240807_194529\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240807_214832\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jun 27 20:43:36 UTC 2024\n",
      "CPU Count:          4\n",
      "Memory Avail:       25.76 GB / 31.36 GB (82.2%)\n",
      "Disk Space Avail:   19.38 GB / 19.52 GB (99.3%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 1620s of the 6480s of remaining time (25%).\n",
      "\t\tContext path: \"AutogluonModels/ag-20240807_214832/ds_sub_fit/sub_fit_ho\"\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                 model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L3       0.980648   0.980136         mcc       28.718530     214.874242  1541.978805                 0.006823                0.978198          40.662610            3       True          6\n",
      "1      LightGBM_BAG_L2       0.980129   0.979268         mcc       23.029531     169.682899  1081.126295                 1.098652                5.446770          69.041312            2       True          5\n",
      "2    LightGBMXT_BAG_L1       0.979496   0.978695         mcc       19.765471     148.645599   874.497336                19.765471              148.645599         874.497336            1       True          1\n",
      "3  WeightedEnsemble_L2       0.979496   0.978695         mcc       19.769720     149.601672   894.575157                 0.004249                0.956073          20.077821            2       True          3\n",
      "4    LightGBMXT_BAG_L2       0.979447   0.978699         mcc       27.613054     208.449274  1432.274883                 5.682175               44.213146         420.189900            2       True          4\n",
      "5      LightGBM_BAG_L1       0.935454   0.934504         mcc        2.165407      15.590529   137.587646                 2.165407               15.590529         137.587646            1       True          2\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t1713s\t = DyStack   runtime |\t4767s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 4767s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240807_214832\"\n",
      "Train Data Rows:    2493556\n",
      "Train Data Columns: 20\n",
      "Label Column:       class\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    28092.74 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2087.06 MB (7.4% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 7.4% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t\t('object', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\t\t('float', [])    :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t27.5s = Fit runtime\n",
      "\t20 features in original data used to generate 20 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 97.51 MB (0.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 29.05s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mcc'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3157.63s of the 4737.62s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=2.98%)\n",
      "\t0.9835\t = Validation score   (mcc)\n",
      "\t2603.18s\t = Training   runtime\n",
      "\t514.17s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 482.74s of the 2062.73s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=2.98%)\n",
      "\t0.9674\t = Validation score   (mcc)\n",
      "\t405.82s\t = Training   runtime\n",
      "\t60.23s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 63.38s of the 1643.36s of remaining time.\n",
      "\tWarning: Model is expected to require 979.5s to train, which exceeds the maximum time limit of 63.4s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestGini_BAG_L1.\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 48.97s of the 1628.96s of remaining time.\n",
      "\tWarning: Model is expected to require 1121.0s to train, which exceeds the maximum time limit of 49.0s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestEntr_BAG_L1.\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 33.03s of the 1613.01s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=3.03%)\n",
      "\t0.5348\t = Validation score   (mcc)\n",
      "\t70.09s\t = Training   runtime\n",
      "\t1.44s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 1536.95s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.824, 'CatBoost_BAG_L1': 0.176}\n",
      "\t0.9835\t = Validation score   (mcc)\n",
      "\t30.53s\t = Training   runtime\n",
      "\t1.12s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1505.17s of the 1505.0s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=3.74%)\n",
      "\t0.9829\t = Validation score   (mcc)\n",
      "\t1237.51s\t = Training   runtime\n",
      "\t160.03s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 240.89s of the 240.71s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=3.74%)\n",
      "\t0.9835\t = Validation score   (mcc)\n",
      "\t209.2s\t = Training   runtime\n",
      "\t12.27s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 24.1s of the 23.93s of remaining time.\n",
      "\tWarning: Model is expected to require 1367.0s to train, which exceeds the maximum time limit of 24.1s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestGini_BAG_L2.\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 2.96s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L2': 0.571, 'LightGBMXT_BAG_L1': 0.286, 'LightGBMXT_BAG_L2': 0.143}\n",
      "\t0.9838\t = Validation score   (mcc)\n",
      "\t47.98s\t = Training   runtime\n",
      "\t1.08s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 4813.15s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 416.6 rows/s (311695 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240807_214832\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240807_235122\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jun 27 20:43:36 UTC 2024\n",
      "CPU Count:          4\n",
      "Memory Avail:       25.64 GB / 31.36 GB (81.8%)\n",
      "Disk Space Avail:   19.34 GB / 19.52 GB (99.1%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 1620s of the 6480s of remaining time (25%).\n",
      "\t\tContext path: \"AutogluonModels/ag-20240807_235122/ds_sub_fit/sub_fit_ho\"\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                 model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L3       0.980868   0.980084         mcc       29.319010     218.876702  1541.077535                 0.007144                1.000135          40.780845            3       True          6\n",
      "1    LightGBMXT_BAG_L1       0.979964   0.978785         mcc       20.409923     152.534099   874.331491                20.409923              152.534099         874.331491            1       True          1\n",
      "2  WeightedEnsemble_L2       0.979964   0.978785         mcc       20.414206     153.514614   895.143074                 0.004282                0.980515          20.811583            2       True          3\n",
      "3      LightGBM_BAG_L2       0.979882   0.978500         mcc       23.666514     173.337105  1078.924405                 0.931011                5.707162          68.370680            2       True          5\n",
      "4    LightGBMXT_BAG_L2       0.979594   0.978613         mcc       28.380855     212.169405  1431.926010                 5.645352               44.539462         421.372286            2       True          4\n",
      "5      LightGBM_BAG_L1       0.932752   0.931068         mcc        2.325580      15.095843   136.222233                 2.325580               15.095843         136.222233            1       True          2\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t1713s\t = DyStack   runtime |\t4767s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 4767s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240807_235122\"\n",
      "Train Data Rows:    2493556\n",
      "Train Data Columns: 20\n",
      "Label Column:       class\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    27938.87 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2086.98 MB (7.5% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 7.5% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t\t('object', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\t\t('float', [])    :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t27.9s = Fit runtime\n",
      "\t20 features in original data used to generate 20 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 97.51 MB (0.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 29.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mcc'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3157.58s of the 4737.53s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=3.00%)\n",
      "\t0.9836\t = Validation score   (mcc)\n",
      "\t2604.05s\t = Training   runtime\n",
      "\t521.98s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 482.11s of the 2062.07s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=3.00%)\n",
      "\t0.9675\t = Validation score   (mcc)\n",
      "\t406.55s\t = Training   runtime\n",
      "\t59.74s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 63.14s of the 1643.1s of remaining time.\n",
      "\tWarning: Model is expected to require 1020.2s to train, which exceeds the maximum time limit of 63.1s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestGini_BAG_L1.\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 48.21s of the 1628.17s of remaining time.\n",
      "\tWarning: Model is expected to require 1114.8s to train, which exceeds the maximum time limit of 48.2s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestEntr_BAG_L1.\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 32.34s of the 1612.29s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=3.03%)\n",
      "\t0.536\t = Validation score   (mcc)\n",
      "\t64.2s\t = Training   runtime\n",
      "\t1.27s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 1542.27s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.9, 'CatBoost_BAG_L1': 0.1}\n",
      "\t0.9836\t = Validation score   (mcc)\n",
      "\t29.28s\t = Training   runtime\n",
      "\t1.32s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1511.46s of the 1511.29s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=3.76%)\n",
      "\t0.983\t = Validation score   (mcc)\n",
      "\t1242.52s\t = Training   runtime\n",
      "\t159.48s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 242.68s of the 242.5s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=3.76%)\n",
      "\t0.9836\t = Validation score   (mcc)\n",
      "\t206.92s\t = Training   runtime\n",
      "\t12.75s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 28.53s of the 28.35s of remaining time.\n",
      "\tWarning: Model is expected to require 1304.1s to train, which exceeds the maximum time limit of 28.5s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestGini_BAG_L2.\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 7.96s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L2': 0.5, 'LightGBMXT_BAG_L1': 0.2, 'LightGBMXT_BAG_L2': 0.2, 'CatBoost_BAG_L1': 0.1}\n",
      "\t0.9838\t = Validation score   (mcc)\n",
      "\t49.05s\t = Training   runtime\n",
      "\t1.08s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 4809.88s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 412.6 rows/s (311695 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240807_235122\")\n"
     ]
    }
   ],
   "source": [
    "scores = []    \n",
    "oof_pred_probs = np.zeros((train.shape[0], len(np.unique(train[TARGET]))))\n",
    "test_pred_probs = np.zeros((test.shape[0], len(np.unique(train[TARGET]))))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, random_state=SEED, shuffle=True)\n",
    "split = skf.split(train.drop(columns=TARGET), train[TARGET])\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(split):\n",
    "    _train, _val = train.iloc[train_idx], train.iloc[val_idx]   \n",
    "    \n",
    "    predictor = TabularPredictor(\n",
    "        label=TARGET,\n",
    "        eval_metric='mcc',\n",
    "        problem_type='binary',\n",
    "        verbosity=2\n",
    "    )\n",
    "    \n",
    "    predictor.fit(\n",
    "        train_data=_train,\n",
    "        time_limit=TIME_LIMIT // N_FOLDS,\n",
    "        presets='best_quality',\n",
    "        save_space=True,\n",
    "        excluded_model_types=['KNN'],\n",
    "        # ag_args_fit={\n",
    "        #     'num_gpus': 1\n",
    "        # }\n",
    "    )\n",
    "\n",
    "    y_pred_probs = predictor.predict_proba(_val)\n",
    "    oof_pred_probs[val_idx] = y_pred_probs            \n",
    "\n",
    "    temp_test_pred_probs = predictor.predict_proba(test)\n",
    "    test_pred_probs += temp_test_pred_probs / N_FOLDS\n",
    "\n",
    "    score = matthews_corrcoef(_val[TARGET], np.argmax(y_pred_probs, axis=1))\n",
    "    scores.append(score)\n",
    "\n",
    "    del predictor, _train, _val, y_pred_probs, temp_test_pred_probs\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef69caae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T01:54:02.309569Z",
     "iopub.status.busy": "2024-08-08T01:54:02.308702Z",
     "iopub.status.idle": "2024-08-08T01:54:02.315149Z",
     "shell.execute_reply": "2024-08-08T01:54:02.314263Z"
    },
    "papermill": {
     "duration": 0.058011,
     "end_time": "2024-08-08T01:54:02.317607",
     "exception": false,
     "start_time": "2024-08-08T01:54:02.259596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - MCC: 0.983594\n",
      "Fold 2 - MCC: 0.984139\n",
      "Fold 3 - MCC: 0.984031\n",
      "Fold 4 - MCC: 0.983975\n",
      "Fold 5 - MCC: 0.984015\n",
      "\n",
      "--- Average MCC: 0.983951  0.000186\n"
     ]
    }
   ],
   "source": [
    "for i in range(N_FOLDS):\n",
    "    print(f'Fold {i + 1} - MCC: {scores[i]:.6f}')\n",
    "    \n",
    "print(f'\\n--- Average MCC: {np.mean(scores):.6f}  {np.std(scores):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9b01806",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T01:54:02.414038Z",
     "iopub.status.busy": "2024-08-08T01:54:02.413526Z",
     "iopub.status.idle": "2024-08-08T01:54:05.577408Z",
     "shell.execute_reply": "2024-08-08T01:54:05.576372Z"
    },
    "papermill": {
     "duration": 3.214814,
     "end_time": "2024-08-08T01:54:05.579750",
     "exception": false,
     "start_time": "2024-08-08T01:54:02.364936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_pred_probs(oof_pred_probs, np.mean(scores), 'oof')\n",
    "save_pred_probs(test_pred_probs, np.mean(scores), 'test')\n",
    "save_submission(test_pred_probs, np.mean(scores)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8b780ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T01:54:05.677994Z",
     "iopub.status.busy": "2024-08-08T01:54:05.677143Z",
     "iopub.status.idle": "2024-08-08T01:54:05.778241Z",
     "shell.execute_reply": "2024-08-08T01:54:05.777088Z"
    },
    "papermill": {
     "duration": 0.152034,
     "end_time": "2024-08-08T01:54:05.780768",
     "exception": false,
     "start_time": "2024-08-08T01:54:05.628734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "shutil.rmtree(\"AutogluonModels\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9045607,
     "sourceId": 76727,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 36913.515099,
   "end_time": "2024-08-08T01:54:11.177002",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-07T15:38:57.661903",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
