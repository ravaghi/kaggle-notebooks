{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9020b92",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-10T18:04:49.476546Z",
     "iopub.status.busy": "2024-08-10T18:04:49.476171Z",
     "iopub.status.idle": "2024-08-10T18:05:35.437273Z",
     "shell.execute_reply": "2024-08-10T18:05:35.435783Z"
    },
    "papermill": {
     "duration": 45.96981,
     "end_time": "2024-08-10T18:05:35.440216",
     "exception": false,
     "start_time": "2024-08-10T18:04:49.470406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "aiobotocore 2.13.1 requires aiohttp<4.0.0,>=3.9.2, but you have aiohttp 3.9.1 which is incompatible.\r\n",
      "aiobotocore 2.13.1 requires botocore<1.34.132,>=1.34.70, but you have botocore 1.29.165 which is incompatible.\r\n",
      "spaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "spopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q autogluon.tabular\n",
    "!pip install -q ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "684e2b1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T18:05:35.449818Z",
     "iopub.status.busy": "2024-08-10T18:05:35.449432Z",
     "iopub.status.idle": "2024-08-10T18:05:39.008938Z",
     "shell.execute_reply": "2024-08-10T18:05:39.007857Z"
    },
    "papermill": {
     "duration": 3.567476,
     "end_time": "2024-08-10T18:05:39.011748",
     "exception": false,
     "start_time": "2024-08-10T18:05:35.444272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.base import clone\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pickle\n",
    "import shutil\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a563f582",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T18:05:39.021153Z",
     "iopub.status.busy": "2024-08-10T18:05:39.020511Z",
     "iopub.status.idle": "2024-08-10T18:05:39.026126Z",
     "shell.execute_reply": "2024-08-10T18:05:39.025025Z"
    },
    "papermill": {
     "duration": 0.013069,
     "end_time": "2024-08-10T18:05:39.028614",
     "exception": false,
     "start_time": "2024-08-10T18:05:39.015545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEED = 6\n",
    "N_FOLDS = 5\n",
    "TARGET = 'class'\n",
    "TIME_LIMIT = 3600 * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0336670",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T18:05:39.037712Z",
     "iopub.status.busy": "2024-08-10T18:05:39.037341Z",
     "iopub.status.idle": "2024-08-10T18:05:56.783745Z",
     "shell.execute_reply": "2024-08-10T18:05:56.782569Z"
    },
    "papermill": {
     "duration": 17.75396,
     "end_time": "2024-08-10T18:05:56.786547",
     "exception": false,
     "start_time": "2024-08-10T18:05:39.032587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/playground-series-s4e8/train.csv', index_col='id')\n",
    "test = pd.read_csv('/kaggle/input/playground-series-s4e8/test.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "438f18a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T18:05:56.796503Z",
     "iopub.status.busy": "2024-08-10T18:05:56.795525Z",
     "iopub.status.idle": "2024-08-10T18:05:56.803104Z",
     "shell.execute_reply": "2024-08-10T18:05:56.802015Z"
    },
    "papermill": {
     "duration": 0.014968,
     "end_time": "2024-08-10T18:05:56.805388",
     "exception": false,
     "start_time": "2024-08-10T18:05:56.790420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_pred_probs(pred_probs, cv_score, name):\n",
    "    with open(f'autogluon_{name}_pred_probs_{cv_score:.6f}.pkl', 'wb') as f:\n",
    "        pickle.dump(pred_probs, f)\n",
    "\n",
    "def save_submission(test_pred_probs, score):\n",
    "    sub = pd.read_csv('/kaggle/input/playground-series-s4e8/sample_submission.csv')\n",
    "    sub[TARGET] = np.argmax(test_pred_probs, axis=1)\n",
    "    sub[TARGET] = sub[TARGET].map({0: 'e', 1: 'p'})\n",
    "    sub.to_csv(f'sub_autogluon_{score:.6f}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d30aeff",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-08-10T18:05:56.815205Z",
     "iopub.status.busy": "2024-08-10T18:05:56.814483Z",
     "iopub.status.idle": "2024-08-11T05:10:01.156006Z",
     "shell.execute_reply": "2024-08-11T05:10:01.154954Z"
    },
    "papermill": {
     "duration": 39844.349551,
     "end_time": "2024-08-11T05:10:01.159030",
     "exception": false,
     "start_time": "2024-08-10T18:05:56.809479",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240810_180613\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jun 27 20:43:36 UTC 2024\n",
      "CPU Count:          4\n",
      "Memory Avail:       28.14 GB / 31.36 GB (89.7%)\n",
      "Disk Space Avail:   19.50 GB / 19.52 GB (99.9%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 1260s of the 5040s of remaining time (25%).\n",
      "2024-08-10 18:06:14,324\tINFO util.py:124 -- Outdated packages:\n",
      "  ipywidgets==7.7.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\t\tContext path: \"AutogluonModels/ag-20240810_180613/ds_sub_fit/sub_fit_ho\"\n",
      "Running DyStack sub-fit ...\n",
      "Beginning AutoGluon training ... Time limit = 1259s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240810_180613/ds_sub_fit/sub_fit_ho\"\n",
      "Train Data Rows:    2216494\n",
      "Train Data Columns: 20\n",
      "Label Column:       class\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = p, class 0 = e\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (p) vs negative (e) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    29862.08 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1854.97 MB (6.2% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 6.2% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t\t('object', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\t\t('float', [])    :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t28.8s = Fit runtime\n",
      "\t20 features in original data used to generate 20 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 86.68 MB (0.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 33.11s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mcc'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 817.05s of the 1225.86s of remaining time.\n",
      "Will use sequential fold fitting strategy because import of ray failed. Reason: ray==2.9.0 detected. 2.10.0 <= ray < 2.11.0 is required. You can use pip to install certain version of ray `pip install ray==2.10.0` \n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "\tRan out of time, early stopping on iteration 643. Best iteration is:\n",
      "\t[643]\tvalid_set's binary_logloss: 0.0373567\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 712. Best iteration is:\n",
      "\t[712]\tvalid_set's binary_logloss: 0.0365027\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 738. Best iteration is:\n",
      "\t[738]\tvalid_set's binary_logloss: 0.0370869\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 761. Best iteration is:\n",
      "\t[761]\tvalid_set's binary_logloss: 0.0369514\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 791. Best iteration is:\n",
      "\t[791]\tvalid_set's binary_logloss: 0.0374519\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 827. Best iteration is:\n",
      "\t[827]\tvalid_set's binary_logloss: 0.0366089\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 884. Best iteration is:\n",
      "\t[884]\tvalid_set's binary_logloss: 0.0362456\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 1000. Best iteration is:\n",
      "\t[1000]\tvalid_set's binary_logloss: 0.0358903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0358903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9842\t = Validation score   (mcc)\n",
      "\t719.63s\t = Training   runtime\n",
      "\t82.75s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 8.72s of the 417.53s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\t[1]\tvalid_set's binary_logloss: 0.654943\n",
      "\tTime limit exceeded... Skipping LightGBM_BAG_L1.\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 4.41s of the 413.22s of remaining time.\n",
      "\tWarning: Model is expected to require 922.1s to train, which exceeds the maximum time limit of 4.4s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestGini_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 399.83s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t0.9842\t = Validation score   (mcc)\n",
      "\t0.26s\t = Training   runtime\n",
      "\t0.34s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 399.08s of the 398.99s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 334. Best iteration is:\n",
      "\t[334]\tvalid_set's binary_logloss: 0.0388286\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 357. Best iteration is:\n",
      "\t[357]\tvalid_set's binary_logloss: 0.0382662\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 359. Best iteration is:\n",
      "\t[359]\tvalid_set's binary_logloss: 0.0364484\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 361. Best iteration is:\n",
      "\t[361]\tvalid_set's binary_logloss: 0.0374248\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 383. Best iteration is:\n",
      "\t[383]\tvalid_set's binary_logloss: 0.0378483\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 411. Best iteration is:\n",
      "\t[411]\tvalid_set's binary_logloss: 0.0376875\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 423. Best iteration is:\n",
      "\t[423]\tvalid_set's binary_logloss: 0.0365922\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 483. Best iteration is:\n",
      "\t[483]\tvalid_set's binary_logloss: 0.0369516\n",
      "\t0.9841\t = Validation score   (mcc)\n",
      "\t358.57s\t = Training   runtime\n",
      "\t31.83s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 4.72s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.773, 'LightGBMXT_BAG_L2': 0.227}\n",
      "\t0.9843\t = Validation score   (mcc)\n",
      "\t7.65s\t = Training   runtime\n",
      "\t0.34s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1262.63s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 2417.1 rows/s (277062 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240810_180613/ds_sub_fit/sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                 model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0    LightGBMXT_BAG_L1       0.983941   0.984240         mcc       84.818753      82.753882   719.632416                84.818753               82.753882         719.632416            1       True          1\n",
      "1  WeightedEnsemble_L2       0.983941   0.984240         mcc       84.823879      83.097963   719.891925                 0.005127                0.344081           0.259509            2       True          2\n",
      "2  WeightedEnsemble_L3       0.983934   0.984271         mcc      119.401390     114.922539  1085.859416                 0.006420                0.338950           7.653425            3       True          4\n",
      "3    LightGBMXT_BAG_L2       0.983896   0.984058         mcc      119.394970     114.583588  1078.205991                34.576218               31.829706         358.573574            2       True          3\n",
      "\t0\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n",
      "\t1396s\t = DyStack   runtime |\t3644s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=0.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\n",
      "Beginning AutoGluon training ... Time limit = 3644s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240810_180613\"\n",
      "Train Data Rows:    2493556\n",
      "Train Data Columns: 20\n",
      "Label Column:       class\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = p, class 0 = e\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (p) vs negative (e) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    29180.37 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2087.06 MB (7.2% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 7.2% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t\t('object', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\t\t('float', [])    :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t31.0s = Fit runtime\n",
      "\t20 features in original data used to generate 20 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 97.51 MB (0.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 34.15s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mcc'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3609.36s of the 3609.33s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0366087\n",
      "[2000]\tvalid_set's binary_logloss: 0.0362303\n",
      "[3000]\tvalid_set's binary_logloss: 0.0361472\n",
      "[4000]\tvalid_set's binary_logloss: 0.0361371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 4074. Best iteration is:\n",
      "\t[3245]\tvalid_set's binary_logloss: 0.036126\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0367157\n",
      "[2000]\tvalid_set's binary_logloss: 0.0361871\n",
      "[3000]\tvalid_set's binary_logloss: 0.0360904\n",
      "[4000]\tvalid_set's binary_logloss: 0.0360539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 4113. Best iteration is:\n",
      "\t[3631]\tvalid_set's binary_logloss: 0.036049\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0362963\n",
      "[2000]\tvalid_set's binary_logloss: 0.0358379\n",
      "[3000]\tvalid_set's binary_logloss: 0.0356913\n",
      "[4000]\tvalid_set's binary_logloss: 0.0356662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 4253. Best iteration is:\n",
      "\t[3701]\tvalid_set's binary_logloss: 0.0356598\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0368202\n",
      "[2000]\tvalid_set's binary_logloss: 0.0363285\n",
      "[3000]\tvalid_set's binary_logloss: 0.0362296\n",
      "[4000]\tvalid_set's binary_logloss: 0.0362138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 4336. Best iteration is:\n",
      "\t[3733]\tvalid_set's binary_logloss: 0.0362047\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0378753\n",
      "[2000]\tvalid_set's binary_logloss: 0.0374791\n",
      "[3000]\tvalid_set's binary_logloss: 0.0374221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0360276\n",
      "[2000]\tvalid_set's binary_logloss: 0.0355131\n",
      "[3000]\tvalid_set's binary_logloss: 0.0353627\n",
      "[4000]\tvalid_set's binary_logloss: 0.0353364\n",
      "[5000]\tvalid_set's binary_logloss: 0.0353866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 5009. Best iteration is:\n",
      "\t[3984]\tvalid_set's binary_logloss: 0.0353349\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0357538\n",
      "[2000]\tvalid_set's binary_logloss: 0.0353344\n",
      "[3000]\tvalid_set's binary_logloss: 0.0352017\n",
      "[4000]\tvalid_set's binary_logloss: 0.0351888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0360294\n",
      "[2000]\tvalid_set's binary_logloss: 0.035672\n",
      "[3000]\tvalid_set's binary_logloss: 0.035577\n",
      "[4000]\tvalid_set's binary_logloss: 0.0355839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9847\t = Validation score   (mcc)\n",
      "\t3015.99s\t = Training   runtime\n",
      "\t301.43s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 286.97s of the 286.95s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 169. Best iteration is:\n",
      "\t[169]\tvalid_set's binary_logloss: 0.0459487\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 174. Best iteration is:\n",
      "\t[174]\tvalid_set's binary_logloss: 0.0455523\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 177. Best iteration is:\n",
      "\t[177]\tvalid_set's binary_logloss: 0.0442343\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 182. Best iteration is:\n",
      "\t[182]\tvalid_set's binary_logloss: 0.0446006\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 191. Best iteration is:\n",
      "\t[191]\tvalid_set's binary_logloss: 0.0448332\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 197. Best iteration is:\n",
      "\t[197]\tvalid_set's binary_logloss: 0.0431482\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 210. Best iteration is:\n",
      "\t[210]\tvalid_set's binary_logloss: 0.0421149\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 246. Best iteration is:\n",
      "\t[246]\tvalid_set's binary_logloss: 0.0404177\n",
      "\t0.9808\t = Validation score   (mcc)\n",
      "\t258.42s\t = Training   runtime\n",
      "\t21.62s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.94s of the 4.53s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.958, 'LightGBM_BAG_L1': 0.042}\n",
      "\t0.9847\t = Validation score   (mcc)\n",
      "\t7.72s\t = Training   runtime\n",
      "\t0.36s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 3647.41s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 964.7 rows/s (311695 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240810_180613\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240810_202025\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jun 27 20:43:36 UTC 2024\n",
      "CPU Count:          4\n",
      "Memory Avail:       26.51 GB / 31.36 GB (84.5%)\n",
      "Disk Space Avail:   19.20 GB / 19.52 GB (98.4%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 1260s of the 5040s of remaining time (25%).\n",
      "\t\tContext path: \"AutogluonModels/ag-20240810_202025/ds_sub_fit/sub_fit_ho\"\n",
      "Running DyStack sub-fit ...\n",
      "Beginning AutoGluon training ... Time limit = 1260s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240810_202025/ds_sub_fit/sub_fit_ho\"\n",
      "Train Data Rows:    2216494\n",
      "Train Data Columns: 20\n",
      "Label Column:       class\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = p, class 0 = e\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (p) vs negative (e) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    28287.07 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1855.19 MB (6.6% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 6.6% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t\t('object', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\t\t('float', [])    :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t27.7s = Fit runtime\n",
      "\t20 features in original data used to generate 20 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 86.68 MB (0.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 31.98s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mcc'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 818.47s of the 1227.99s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 733. Best iteration is:\n",
      "\t[733]\tvalid_set's binary_logloss: 0.0378461\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 735. Best iteration is:\n",
      "\t[735]\tvalid_set's binary_logloss: 0.0371116\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 751. Best iteration is:\n",
      "\t[751]\tvalid_set's binary_logloss: 0.0366505\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 774. Best iteration is:\n",
      "\t[774]\tvalid_set's binary_logloss: 0.037364\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 799. Best iteration is:\n",
      "\t[799]\tvalid_set's binary_logloss: 0.0370109\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 842. Best iteration is:\n",
      "\t[842]\tvalid_set's binary_logloss: 0.0370234\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 889. Best iteration is:\n",
      "\t[889]\tvalid_set's binary_logloss: 0.0361453\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 993. Best iteration is:\n",
      "\t[992]\tvalid_set's binary_logloss: 0.0374517\n",
      "\t0.9841\t = Validation score   (mcc)\n",
      "\t718.45s\t = Training   runtime\n",
      "\t85.83s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 11.4s of the 420.92s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\t[1]\tvalid_set's binary_logloss: 0.655119\n",
      "\tTime limit exceeded... Skipping LightGBM_BAG_L1.\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 6.96s of the 416.47s of remaining time.\n",
      "\tWarning: Model is expected to require 895.7s to train, which exceeds the maximum time limit of 7.0s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestGini_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 403.49s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t0.9841\t = Validation score   (mcc)\n",
      "\t0.26s\t = Training   runtime\n",
      "\t0.36s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 402.72s of the 402.62s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 343. Best iteration is:\n",
      "\t[343]\tvalid_set's binary_logloss: 0.037701\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 356. Best iteration is:\n",
      "\t[356]\tvalid_set's binary_logloss: 0.0372472\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 338. Best iteration is:\n",
      "\t[338]\tvalid_set's binary_logloss: 0.0380212\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 388. Best iteration is:\n",
      "\t[388]\tvalid_set's binary_logloss: 0.0376426\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 374. Best iteration is:\n",
      "\t[374]\tvalid_set's binary_logloss: 0.038674\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 423. Best iteration is:\n",
      "\t[423]\tvalid_set's binary_logloss: 0.0384212\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 442. Best iteration is:\n",
      "\t[442]\tvalid_set's binary_logloss: 0.0375705\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 504. Best iteration is:\n",
      "\t[504]\tvalid_set's binary_logloss: 0.0378377\n",
      "\t0.9838\t = Validation score   (mcc)\n",
      "\t361.61s\t = Training   runtime\n",
      "\t31.61s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 5.58s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.52, 'LightGBMXT_BAG_L2': 0.48}\n",
      "\t0.9841\t = Validation score   (mcc)\n",
      "\t7.59s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1262.68s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 2358.2 rows/s (277062 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240810_202025/ds_sub_fit/sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                 model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0    LightGBMXT_BAG_L1       0.984536   0.984077         mcc       88.427707      85.833836   718.449358                88.427707               85.833836         718.449358            1       True          1\n",
      "1  WeightedEnsemble_L2       0.984536   0.984077         mcc       88.432640      86.195009   718.704880                 0.004933                0.361174           0.255522            2       True          2\n",
      "2  WeightedEnsemble_L3       0.984529   0.984127         mcc      122.915917     117.779093  1087.654930                 0.006685                0.331186           7.594453            3       True          4\n",
      "3    LightGBMXT_BAG_L2       0.984281   0.983835         mcc      122.909232     117.447907  1080.060477                34.481525               31.614071         361.611119            2       True          3\n",
      "\t0\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n",
      "\t1399s\t = DyStack   runtime |\t3641s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=0.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\n",
      "Beginning AutoGluon training ... Time limit = 3641s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240810_202025\"\n",
      "Train Data Rows:    2493556\n",
      "Train Data Columns: 20\n",
      "Label Column:       class\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = p, class 0 = e\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (p) vs negative (e) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    28622.13 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2086.98 MB (7.3% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 7.3% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t\t('object', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\t\t('float', [])    :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t31.0s = Fit runtime\n",
      "\t20 features in original data used to generate 20 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 97.51 MB (0.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 34.13s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mcc'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3607.36s of the 3607.34s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0362602\n",
      "[2000]\tvalid_set's binary_logloss: 0.03581\n",
      "[3000]\tvalid_set's binary_logloss: 0.0356978\n",
      "[4000]\tvalid_set's binary_logloss: 0.0356825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 4095. Best iteration is:\n",
      "\t[3467]\tvalid_set's binary_logloss: 0.0356753\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0362148\n",
      "[2000]\tvalid_set's binary_logloss: 0.0357517\n",
      "[3000]\tvalid_set's binary_logloss: 0.0356624\n",
      "[4000]\tvalid_set's binary_logloss: 0.0356706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 4154. Best iteration is:\n",
      "\t[3707]\tvalid_set's binary_logloss: 0.0356498\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.037229\n",
      "[2000]\tvalid_set's binary_logloss: 0.0368293\n",
      "[3000]\tvalid_set's binary_logloss: 0.0367038\n",
      "[4000]\tvalid_set's binary_logloss: 0.0366937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 4283. Best iteration is:\n",
      "\t[3577]\tvalid_set's binary_logloss: 0.036673\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.036251\n",
      "[2000]\tvalid_set's binary_logloss: 0.0358709\n",
      "[3000]\tvalid_set's binary_logloss: 0.0358025\n",
      "[4000]\tvalid_set's binary_logloss: 0.0358494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0371539\n",
      "[2000]\tvalid_set's binary_logloss: 0.0367277\n",
      "[3000]\tvalid_set's binary_logloss: 0.0366282\n",
      "[4000]\tvalid_set's binary_logloss: 0.0366278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 4567. Best iteration is:\n",
      "\t[3500]\tvalid_set's binary_logloss: 0.0366164\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0367359\n",
      "[2000]\tvalid_set's binary_logloss: 0.0362816\n",
      "[3000]\tvalid_set's binary_logloss: 0.0361802\n",
      "[4000]\tvalid_set's binary_logloss: 0.0361753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0367651\n",
      "[2000]\tvalid_set's binary_logloss: 0.0363998\n",
      "[3000]\tvalid_set's binary_logloss: 0.0363549\n",
      "[4000]\tvalid_set's binary_logloss: 0.0363516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0370022\n",
      "[2000]\tvalid_set's binary_logloss: 0.0365125\n",
      "[3000]\tvalid_set's binary_logloss: 0.0363535\n",
      "[4000]\tvalid_set's binary_logloss: 0.036331\n",
      "[5000]\tvalid_set's binary_logloss: 0.0363371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9846\t = Validation score   (mcc)\n",
      "\t3142.16s\t = Training   runtime\n",
      "\t314.65s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 145.55s of the 145.53s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 81. Best iteration is:\n",
      "\t[81]\tvalid_set's binary_logloss: 0.0844039\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 72. Best iteration is:\n",
      "\t[72]\tvalid_set's binary_logloss: 0.0968911\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 83. Best iteration is:\n",
      "\t[83]\tvalid_set's binary_logloss: 0.0819125\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 77. Best iteration is:\n",
      "\t[77]\tvalid_set's binary_logloss: 0.0896138\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 89. Best iteration is:\n",
      "\t[89]\tvalid_set's binary_logloss: 0.0755199\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 84. Best iteration is:\n",
      "\t[84]\tvalid_set's binary_logloss: 0.0808509\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 100. Best iteration is:\n",
      "\t[100]\tvalid_set's binary_logloss: 0.06673\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 107. Best iteration is:\n",
      "\t[107]\tvalid_set's binary_logloss: 0.0627895\n",
      "\t0.9741\t = Validation score   (mcc)\n",
      "\t132.6s\t = Training   runtime\n",
      "\t8.96s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.74s of the 1.61s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t0.9846\t = Validation score   (mcc)\n",
      "\t7.41s\t = Training   runtime\n",
      "\t0.42s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 3648.04s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 990.4 rows/s (311695 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240810_202025\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240810_223245\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jun 27 20:43:36 UTC 2024\n",
      "CPU Count:          4\n",
      "Memory Avail:       26.21 GB / 31.36 GB (83.6%)\n",
      "Disk Space Avail:   18.90 GB / 19.52 GB (96.8%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 1260s of the 5040s of remaining time (25%).\n",
      "\t\tContext path: \"AutogluonModels/ag-20240810_223245/ds_sub_fit/sub_fit_ho\"\n",
      "Running DyStack sub-fit ...\n",
      "Beginning AutoGluon training ... Time limit = 1260s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240810_223245/ds_sub_fit/sub_fit_ho\"\n",
      "Train Data Rows:    2216494\n",
      "Train Data Columns: 20\n",
      "Label Column:       class\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = p, class 0 = e\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (p) vs negative (e) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    28069.28 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1855.03 MB (6.6% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 6.6% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t\t('object', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\t\t('float', [])    :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t27.3s = Fit runtime\n",
      "\t20 features in original data used to generate 20 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 86.68 MB (0.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 31.43s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mcc'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 818.84s of the 1228.54s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 727. Best iteration is:\n",
      "\t[727]\tvalid_set's binary_logloss: 0.0365944\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 750. Best iteration is:\n",
      "\t[750]\tvalid_set's binary_logloss: 0.0365057\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 773. Best iteration is:\n",
      "\t[773]\tvalid_set's binary_logloss: 0.0380908\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 782. Best iteration is:\n",
      "\t[782]\tvalid_set's binary_logloss: 0.0368314\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 824. Best iteration is:\n",
      "\t[823]\tvalid_set's binary_logloss: 0.0364443\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 845. Best iteration is:\n",
      "\t[845]\tvalid_set's binary_logloss: 0.0371458\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 899. Best iteration is:\n",
      "\t[899]\tvalid_set's binary_logloss: 0.0363734\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 999. Best iteration is:\n",
      "\t[999]\tvalid_set's binary_logloss: 0.0371202\n",
      "\t0.9842\t = Validation score   (mcc)\n",
      "\t717.22s\t = Training   runtime\n",
      "\t87.35s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 11.65s of the 421.36s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\t[1]\tvalid_set's binary_logloss: 0.655081\n",
      "\tTime limit exceeded... Skipping LightGBM_BAG_L1.\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 6.69s of the 416.39s of remaining time.\n",
      "\tWarning: Model is expected to require 911.5s to train, which exceeds the maximum time limit of 6.7s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestGini_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 403.16s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t0.9842\t = Validation score   (mcc)\n",
      "\t0.25s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 402.43s of the 402.34s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 356. Best iteration is:\n",
      "\t[356]\tvalid_set's binary_logloss: 0.0380043\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 358. Best iteration is:\n",
      "\t[358]\tvalid_set's binary_logloss: 0.0378006\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 376. Best iteration is:\n",
      "\t[376]\tvalid_set's binary_logloss: 0.0376976\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 364. Best iteration is:\n",
      "\t[364]\tvalid_set's binary_logloss: 0.0381691\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 385. Best iteration is:\n",
      "\t[385]\tvalid_set's binary_logloss: 0.0377039\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 394. Best iteration is:\n",
      "\t[394]\tvalid_set's binary_logloss: 0.0374832\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 446. Best iteration is:\n",
      "\t[446]\tvalid_set's binary_logloss: 0.0381937\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 493. Best iteration is:\n",
      "\t[493]\tvalid_set's binary_logloss: 0.0366868\n",
      "\t0.984\t = Validation score   (mcc)\n",
      "\t360.28s\t = Training   runtime\n",
      "\t33.06s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 5.16s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.56, 'LightGBMXT_BAG_L2': 0.44}\n",
      "\t0.9843\t = Validation score   (mcc)\n",
      "\t7.79s\t = Training   runtime\n",
      "\t0.34s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1263.29s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 2300.2 rows/s (277062 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240810_223245/ds_sub_fit/sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                 model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L3       0.984514   0.984267         mcc      126.334083     120.748271  1085.295280                 0.006536                0.336803           7.792616            3       True          4\n",
      "1    LightGBMXT_BAG_L1       0.984499   0.984198         mcc       90.628971      87.348331   717.221286                90.628971               87.348331         717.221286            1       True          1\n",
      "2  WeightedEnsemble_L2       0.984499   0.984198         mcc       90.635071      87.681286   717.472470                 0.006100                0.332956           0.251184            2       True          2\n",
      "3    LightGBMXT_BAG_L2       0.984244   0.983977         mcc      126.327547     120.411468  1077.502663                35.698575               33.063137         360.281377            2       True          3\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t1402s\t = DyStack   runtime |\t3638s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 3638s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240810_223245\"\n",
      "Train Data Rows:    2493556\n",
      "Train Data Columns: 20\n",
      "Label Column:       class\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = p, class 0 = e\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (p) vs negative (e) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    28494.71 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2087.00 MB (7.3% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 7.3% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t\t('object', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\t\t('float', [])    :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t30.9s = Fit runtime\n",
      "\t20 features in original data used to generate 20 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 97.51 MB (0.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 33.91s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mcc'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 2401.83s of the 3603.63s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0352137\n",
      "[2000]\tvalid_set's binary_logloss: 0.03475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2590. Best iteration is:\n",
      "\t[2590]\tvalid_set's binary_logloss: 0.0346573\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0365834\n",
      "[2000]\tvalid_set's binary_logloss: 0.0361284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2587. Best iteration is:\n",
      "\t[2586]\tvalid_set's binary_logloss: 0.0360265\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0364161\n",
      "[2000]\tvalid_set's binary_logloss: 0.0360537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2661. Best iteration is:\n",
      "\t[2645]\tvalid_set's binary_logloss: 0.0359898\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0368605\n",
      "[2000]\tvalid_set's binary_logloss: 0.0364295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2700. Best iteration is:\n",
      "\t[2562]\tvalid_set's binary_logloss: 0.0363667\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0371248\n",
      "[2000]\tvalid_set's binary_logloss: 0.0366973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2816. Best iteration is:\n",
      "\t[2691]\tvalid_set's binary_logloss: 0.0366319\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0363685\n",
      "[2000]\tvalid_set's binary_logloss: 0.0358615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2958. Best iteration is:\n",
      "\t[2957]\tvalid_set's binary_logloss: 0.0357064\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0367584\n",
      "[2000]\tvalid_set's binary_logloss: 0.0362726\n",
      "[3000]\tvalid_set's binary_logloss: 0.0361542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 3109. Best iteration is:\n",
      "\t[3044]\tvalid_set's binary_logloss: 0.0361444\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0366921\n",
      "[2000]\tvalid_set's binary_logloss: 0.0362608\n",
      "[3000]\tvalid_set's binary_logloss: 0.0361576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 3448. Best iteration is:\n",
      "\t[3428]\tvalid_set's binary_logloss: 0.036152\n",
      "\t0.9847\t = Validation score   (mcc)\n",
      "\t2106.64s\t = Training   runtime\n",
      "\t253.99s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 36.83s of the 1238.62s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 7. Best iteration is:\n",
      "\t[7]\tvalid_set's binary_logloss: 0.506264\n",
      "\tTime limit exceeded... Skipping LightGBM_BAG_L1.\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 31.39s of the 1233.19s of remaining time.\n",
      "\tWarning: Model is expected to require 1054.2s to train, which exceeds the maximum time limit of 31.4s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestGini_BAG_L1.\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 16.3s of the 1218.1s of remaining time.\n",
      "\tWarning: Model is expected to require 1071.1s to train, which exceeds the maximum time limit of 16.3s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestEntr_BAG_L1.\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 1.0s of the 1202.8s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTime limit exceeded... Skipping CatBoost_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 1199.85s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t0.9847\t = Validation score   (mcc)\n",
      "\t0.25s\t = Training   runtime\n",
      "\t0.37s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1199.06s of the 1198.97s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0363412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1168. Best iteration is:\n",
      "\t[1130]\tvalid_set's binary_logloss: 0.0362775\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.035764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1180. Best iteration is:\n",
      "\t[1180]\tvalid_set's binary_logloss: 0.0357038\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0365541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1219. Best iteration is:\n",
      "\t[1215]\tvalid_set's binary_logloss: 0.0364872\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0361217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1181. Best iteration is:\n",
      "\t[1181]\tvalid_set's binary_logloss: 0.0360461\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0361388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1262. Best iteration is:\n",
      "\t[1262]\tvalid_set's binary_logloss: 0.0360408\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0369685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1316. Best iteration is:\n",
      "\t[1304]\tvalid_set's binary_logloss: 0.0368562\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.036191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1414. Best iteration is:\n",
      "\t[1411]\tvalid_set's binary_logloss: 0.0360162\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0366517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1535. Best iteration is:\n",
      "\t[1518]\tvalid_set's binary_logloss: 0.0364871\n",
      "\t0.9846\t = Validation score   (mcc)\n",
      "\t1053.98s\t = Training   runtime\n",
      "\t123.04s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 18.33s of the 18.24s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\t[1]\tvalid_set's binary_logloss: 0.641348\n",
      "\tTime limit exceeded... Skipping LightGBM_BAG_L2.\n",
      "Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 13.15s of the 13.06s of remaining time.\n",
      "\tWarning: Model is expected to require 1198.8s to train, which exceeds the maximum time limit of 13.2s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestGini_BAG_L2.\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the -5.51s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.615, 'LightGBMXT_BAG_L2': 0.385}\n",
      "\t0.9848\t = Validation score   (mcc)\n",
      "\t7.34s\t = Training   runtime\n",
      "\t0.38s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 3651.14s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 826.6 rows/s (311695 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240810_223245\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240811_005533\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jun 27 20:43:36 UTC 2024\n",
      "CPU Count:          4\n",
      "Memory Avail:       25.79 GB / 31.36 GB (82.2%)\n",
      "Disk Space Avail:   18.58 GB / 19.52 GB (95.2%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 1260s of the 5040s of remaining time (25%).\n",
      "\t\tContext path: \"AutogluonModels/ag-20240811_005533/ds_sub_fit/sub_fit_ho\"\n",
      "Running DyStack sub-fit ...\n",
      "Beginning AutoGluon training ... Time limit = 1260s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240811_005533/ds_sub_fit/sub_fit_ho\"\n",
      "Train Data Rows:    2216494\n",
      "Train Data Columns: 20\n",
      "Label Column:       class\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = p, class 0 = e\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (p) vs negative (e) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    27659.05 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1855.09 MB (6.7% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 6.7% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t\t('object', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\t\t('float', [])    :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t28.0s = Fit runtime\n",
      "\t20 features in original data used to generate 20 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 86.68 MB (0.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 32.21s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mcc'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 818.31s of the 1227.75s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 714. Best iteration is:\n",
      "\t[714]\tvalid_set's binary_logloss: 0.0374831\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 732. Best iteration is:\n",
      "\t[732]\tvalid_set's binary_logloss: 0.0368876\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 743. Best iteration is:\n",
      "\t[743]\tvalid_set's binary_logloss: 0.0361354\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 738. Best iteration is:\n",
      "\t[738]\tvalid_set's binary_logloss: 0.0376537\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 807. Best iteration is:\n",
      "\t[806]\tvalid_set's binary_logloss: 0.0367469\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 846. Best iteration is:\n",
      "\t[846]\tvalid_set's binary_logloss: 0.0372601\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 903. Best iteration is:\n",
      "\t[903]\tvalid_set's binary_logloss: 0.0370872\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0367151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1015. Best iteration is:\n",
      "\t[1014]\tvalid_set's binary_logloss: 0.0367009\n",
      "\t0.9841\t = Validation score   (mcc)\n",
      "\t719.57s\t = Training   runtime\n",
      "\t84.07s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 12.05s of the 421.49s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\t[1]\tvalid_set's binary_logloss: 0.655049\n",
      "\tTime limit exceeded... Skipping LightGBM_BAG_L1.\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 7.76s of the 417.2s of remaining time.\n",
      "\tWarning: Model is expected to require 958.1s to train, which exceeds the maximum time limit of 7.8s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestGini_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 403.39s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t0.9841\t = Validation score   (mcc)\n",
      "\t0.26s\t = Training   runtime\n",
      "\t0.35s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 402.63s of the 402.54s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 348. Best iteration is:\n",
      "\t[348]\tvalid_set's binary_logloss: 0.0390938\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 340. Best iteration is:\n",
      "\t[340]\tvalid_set's binary_logloss: 0.036626\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 366. Best iteration is:\n",
      "\t[366]\tvalid_set's binary_logloss: 0.0381803\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 358. Best iteration is:\n",
      "\t[358]\tvalid_set's binary_logloss: 0.0380024\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 393. Best iteration is:\n",
      "\t[393]\tvalid_set's binary_logloss: 0.0377471\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 392. Best iteration is:\n",
      "\t[392]\tvalid_set's binary_logloss: 0.0371648\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 436. Best iteration is:\n",
      "\t[436]\tvalid_set's binary_logloss: 0.0382189\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 499. Best iteration is:\n",
      "\t[499]\tvalid_set's binary_logloss: 0.0372511\n",
      "\t0.9839\t = Validation score   (mcc)\n",
      "\t361.44s\t = Training   runtime\n",
      "\t31.85s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 5.44s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.714, 'LightGBMXT_BAG_L2': 0.286}\n",
      "\t0.9842\t = Validation score   (mcc)\n",
      "\t7.9s\t = Training   runtime\n",
      "\t0.34s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1263.11s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 2389.3 rows/s (277062 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240811_005533/ds_sub_fit/sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                 model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0    LightGBMXT_BAG_L1       0.984652   0.984106         mcc       86.634000      84.065080   719.570284                86.634000               84.065080         719.570284            1       True          1\n",
      "1  WeightedEnsemble_L2       0.984652   0.984106         mcc       86.639150      84.418036   719.834962                 0.005151                0.352956           0.264678            2       True          2\n",
      "2  WeightedEnsemble_L3       0.984652   0.984167         mcc      121.399119     116.255976  1088.908339                 0.006459                0.340453           7.895908            3       True          4\n",
      "3    LightGBMXT_BAG_L2       0.984375   0.983920         mcc      121.392660     115.915523  1081.012430                34.758661               31.850442         361.442146            2       True          3\n",
      "\t0\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n",
      "\t1397s\t = DyStack   runtime |\t3643s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=0.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\n",
      "Beginning AutoGluon training ... Time limit = 3643s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240811_005533\"\n",
      "Train Data Rows:    2493556\n",
      "Train Data Columns: 20\n",
      "Label Column:       class\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = p, class 0 = e\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (p) vs negative (e) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    27996.67 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2087.06 MB (7.5% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 7.5% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t\t('object', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\t\t('float', [])    :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t31.5s = Fit runtime\n",
      "\t20 features in original data used to generate 20 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 97.51 MB (0.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 34.66s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mcc'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3607.89s of the 3607.87s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0366967\n",
      "[2000]\tvalid_set's binary_logloss: 0.0362309\n",
      "[3000]\tvalid_set's binary_logloss: 0.0361681\n",
      "[4000]\tvalid_set's binary_logloss: 0.0362063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 4007. Best iteration is:\n",
      "\t[3153]\tvalid_set's binary_logloss: 0.0361622\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0369972\n",
      "[2000]\tvalid_set's binary_logloss: 0.036578\n",
      "[3000]\tvalid_set's binary_logloss: 0.0365182\n",
      "[4000]\tvalid_set's binary_logloss: 0.0365484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0362415\n",
      "[2000]\tvalid_set's binary_logloss: 0.035798\n",
      "[3000]\tvalid_set's binary_logloss: 0.0356913\n",
      "[4000]\tvalid_set's binary_logloss: 0.0357058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.03571\n",
      "[2000]\tvalid_set's binary_logloss: 0.0353093\n",
      "[3000]\tvalid_set's binary_logloss: 0.0352292\n",
      "[4000]\tvalid_set's binary_logloss: 0.0352414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 4408. Best iteration is:\n",
      "\t[3477]\tvalid_set's binary_logloss: 0.0352083\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0365431\n",
      "[2000]\tvalid_set's binary_logloss: 0.0360376\n",
      "[3000]\tvalid_set's binary_logloss: 0.0359181\n",
      "[4000]\tvalid_set's binary_logloss: 0.0358896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 4625. Best iteration is:\n",
      "\t[4200]\tvalid_set's binary_logloss: 0.0358868\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.037465\n",
      "[2000]\tvalid_set's binary_logloss: 0.0369449\n",
      "[3000]\tvalid_set's binary_logloss: 0.0367873\n",
      "[4000]\tvalid_set's binary_logloss: 0.0367439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 4774. Best iteration is:\n",
      "\t[4002]\tvalid_set's binary_logloss: 0.0367434\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0360124\n",
      "[2000]\tvalid_set's binary_logloss: 0.0356132\n",
      "[3000]\tvalid_set's binary_logloss: 0.0355323\n",
      "[4000]\tvalid_set's binary_logloss: 0.0355522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0368175\n",
      "[2000]\tvalid_set's binary_logloss: 0.0364504\n",
      "[3000]\tvalid_set's binary_logloss: 0.0363729\n",
      "[4000]\tvalid_set's binary_logloss: 0.0363659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9847\t = Validation score   (mcc)\n",
      "\t3079.11s\t = Training   runtime\n",
      "\t304.53s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 219.27s of the 219.25s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 122. Best iteration is:\n",
      "\t[122]\tvalid_set's binary_logloss: 0.0554843\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 134. Best iteration is:\n",
      "\t[134]\tvalid_set's binary_logloss: 0.0524233\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 137. Best iteration is:\n",
      "\t[137]\tvalid_set's binary_logloss: 0.051045\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 132. Best iteration is:\n",
      "\t[132]\tvalid_set's binary_logloss: 0.0523096\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 136. Best iteration is:\n",
      "\t[136]\tvalid_set's binary_logloss: 0.0519635\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 142. Best iteration is:\n",
      "\t[142]\tvalid_set's binary_logloss: 0.050564\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 153. Best iteration is:\n",
      "\t[153]\tvalid_set's binary_logloss: 0.0483299\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 175. Best iteration is:\n",
      "\t[175]\tvalid_set's binary_logloss: 0.0449395\n",
      "\t0.9792\t = Validation score   (mcc)\n",
      "\t198.35s\t = Training   runtime\n",
      "\t15.24s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.79s of the 3.3s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t0.9847\t = Validation score   (mcc)\n",
      "\t7.39s\t = Training   runtime\n",
      "\t0.37s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 3647.34s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1023.4 rows/s (311695 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240811_005533\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240811_030711\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jun 27 20:43:36 UTC 2024\n",
      "CPU Count:          4\n",
      "Memory Avail:       25.80 GB / 31.36 GB (82.3%)\n",
      "Disk Space Avail:   18.28 GB / 19.52 GB (93.7%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 1260s of the 5040s of remaining time (25%).\n",
      "\t\tContext path: \"AutogluonModels/ag-20240811_030711/ds_sub_fit/sub_fit_ho\"\n",
      "Running DyStack sub-fit ...\n",
      "Beginning AutoGluon training ... Time limit = 1260s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240811_030711/ds_sub_fit/sub_fit_ho\"\n",
      "Train Data Rows:    2216494\n",
      "Train Data Columns: 20\n",
      "Label Column:       class\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = p, class 0 = e\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (p) vs negative (e) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    27612.64 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1855.12 MB (6.7% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 6.7% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t\t('object', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\t\t('float', [])    :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t27.4s = Fit runtime\n",
      "\t20 features in original data used to generate 20 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 86.68 MB (0.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 31.62s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mcc'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 818.71s of the 1228.34s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 758. Best iteration is:\n",
      "\t[758]\tvalid_set's binary_logloss: 0.0371915\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 757. Best iteration is:\n",
      "\t[757]\tvalid_set's binary_logloss: 0.0367031\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 765. Best iteration is:\n",
      "\t[764]\tvalid_set's binary_logloss: 0.037513\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 789. Best iteration is:\n",
      "\t[789]\tvalid_set's binary_logloss: 0.0371913\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 785. Best iteration is:\n",
      "\t[785]\tvalid_set's binary_logloss: 0.0361546\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 837. Best iteration is:\n",
      "\t[837]\tvalid_set's binary_logloss: 0.0357627\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 886. Best iteration is:\n",
      "\t[886]\tvalid_set's binary_logloss: 0.0367529\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0370522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1027. Best iteration is:\n",
      "\t[1027]\tvalid_set's binary_logloss: 0.037035\n",
      "\t0.9842\t = Validation score   (mcc)\n",
      "\t716.97s\t = Training   runtime\n",
      "\t87.94s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 11.06s of the 420.7s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\t[1]\tvalid_set's binary_logloss: 0.655011\n",
      "\tTime limit exceeded... Skipping LightGBM_BAG_L1.\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 6.73s of the 416.37s of remaining time.\n",
      "\tWarning: Model is expected to require 883.7s to train, which exceeds the maximum time limit of 6.7s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestGini_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 403.51s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t0.9842\t = Validation score   (mcc)\n",
      "\t0.25s\t = Training   runtime\n",
      "\t0.36s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 402.76s of the 402.66s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 356. Best iteration is:\n",
      "\t[356]\tvalid_set's binary_logloss: 0.0367282\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 368. Best iteration is:\n",
      "\t[368]\tvalid_set's binary_logloss: 0.0373446\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 367. Best iteration is:\n",
      "\t[367]\tvalid_set's binary_logloss: 0.0373054\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 390. Best iteration is:\n",
      "\t[390]\tvalid_set's binary_logloss: 0.038713\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 394. Best iteration is:\n",
      "\t[394]\tvalid_set's binary_logloss: 0.0373835\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 425. Best iteration is:\n",
      "\t[424]\tvalid_set's binary_logloss: 0.0370937\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 436. Best iteration is:\n",
      "\t[436]\tvalid_set's binary_logloss: 0.0376285\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 505. Best iteration is:\n",
      "\t[505]\tvalid_set's binary_logloss: 0.0378986\n",
      "\t0.984\t = Validation score   (mcc)\n",
      "\t361.1s\t = Training   runtime\n",
      "\t32.49s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 5.33s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.722, 'LightGBMXT_BAG_L2': 0.278}\n",
      "\t0.9842\t = Validation score   (mcc)\n",
      "\t7.6s\t = Training   runtime\n",
      "\t0.36s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1262.96s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 2299.6 rows/s (277062 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240811_030711/ds_sub_fit/sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                 model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L3       0.984515   0.984217         mcc      124.924891     120.795318  1085.669905                 0.006354                0.357354           7.601798            3       True          4\n",
      "1    LightGBMXT_BAG_L1       0.984501   0.984182         mcc       90.081970      87.943054   716.971624                90.081970               87.943054         716.971624            1       True          1\n",
      "2  WeightedEnsemble_L2       0.984501   0.984182         mcc       90.087037      88.305274   717.223325                 0.005067                0.362220           0.251701            2       True          2\n",
      "3    LightGBMXT_BAG_L2       0.984231   0.983966         mcc      124.918536     120.437964  1078.068107                34.836566               32.494910         361.096483            2       True          3\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t1401s\t = DyStack   runtime |\t3639s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 3639s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240811_030711\"\n",
      "Train Data Rows:    2493556\n",
      "Train Data Columns: 20\n",
      "Label Column:       class\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = p, class 0 = e\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (p) vs negative (e) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    28104.79 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2086.98 MB (7.4% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 7.4% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t\t('object', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\t\t('float', [])    :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t30.8s = Fit runtime\n",
      "\t20 features in original data used to generate 20 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 97.51 MB (0.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 33.93s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mcc'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 2403.0s of the 3605.38s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0367138\n",
      "[2000]\tvalid_set's binary_logloss: 0.0362761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2521. Best iteration is:\n",
      "\t[2521]\tvalid_set's binary_logloss: 0.0362052\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0359854\n",
      "[2000]\tvalid_set's binary_logloss: 0.0355019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2655. Best iteration is:\n",
      "\t[2646]\tvalid_set's binary_logloss: 0.0354039\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0372913\n",
      "[2000]\tvalid_set's binary_logloss: 0.0368405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2680. Best iteration is:\n",
      "\t[2645]\tvalid_set's binary_logloss: 0.0367448\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0375479\n",
      "[2000]\tvalid_set's binary_logloss: 0.0371335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2712. Best iteration is:\n",
      "\t[2703]\tvalid_set's binary_logloss: 0.0370351\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0362605\n",
      "[2000]\tvalid_set's binary_logloss: 0.0358215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2813. Best iteration is:\n",
      "\t[2808]\tvalid_set's binary_logloss: 0.0357659\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0354508\n",
      "[2000]\tvalid_set's binary_logloss: 0.0350475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2912. Best iteration is:\n",
      "\t[2875]\tvalid_set's binary_logloss: 0.034965\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0365709\n",
      "[2000]\tvalid_set's binary_logloss: 0.0360063\n",
      "[3000]\tvalid_set's binary_logloss: 0.0358692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 3070. Best iteration is:\n",
      "\t[3070]\tvalid_set's binary_logloss: 0.0358633\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.036228\n",
      "[2000]\tvalid_set's binary_logloss: 0.0358581\n",
      "[3000]\tvalid_set's binary_logloss: 0.0357947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 3427. Best iteration is:\n",
      "\t[2813]\tvalid_set's binary_logloss: 0.0357898\n",
      "\t0.9847\t = Validation score   (mcc)\n",
      "\t2106.22s\t = Training   runtime\n",
      "\t248.88s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 43.64s of the 1246.02s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 12. Best iteration is:\n",
      "\t[12]\tvalid_set's binary_logloss: 0.417877\n",
      "\tTime limit exceeded... Skipping LightGBM_BAG_L1.\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 37.55s of the 1239.93s of remaining time.\n",
      "\tWarning: Model is expected to require 1038.2s to train, which exceeds the maximum time limit of 37.6s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestGini_BAG_L1.\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 22.67s of the 1225.05s of remaining time.\n",
      "\tWarning: Model is expected to require 1074.1s to train, which exceeds the maximum time limit of 22.7s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestEntr_BAG_L1.\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 7.34s of the 1209.72s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTime limit exceeded... Skipping CatBoost_BAG_L1.\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 5.21s of the 1207.59s of remaining time.\n",
      "\tWarning: Model is expected to require 612.5s to train, which exceeds the maximum time limit of 5.2s, skipping model...\n",
      "\tTime limit exceeded... Skipping ExtraTreesGini_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 1198.29s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t0.9847\t = Validation score   (mcc)\n",
      "\t0.26s\t = Training   runtime\n",
      "\t0.38s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 108 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1197.49s of the 1197.4s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0372429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1191. Best iteration is:\n",
      "\t[1191]\tvalid_set's binary_logloss: 0.0371585\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0360429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1182. Best iteration is:\n",
      "\t[1181]\tvalid_set's binary_logloss: 0.035968\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0356614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1191. Best iteration is:\n",
      "\t[1190]\tvalid_set's binary_logloss: 0.0355847\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0357684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1245. Best iteration is:\n",
      "\t[1241]\tvalid_set's binary_logloss: 0.0357212\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0362705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1277. Best iteration is:\n",
      "\t[1267]\tvalid_set's binary_logloss: 0.0361679\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0363023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1348. Best iteration is:\n",
      "\t[1348]\tvalid_set's binary_logloss: 0.0361599\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0376473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1370. Best iteration is:\n",
      "\t[1369]\tvalid_set's binary_logloss: 0.0375542\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_logloss: 0.0358751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1553. Best iteration is:\n",
      "\t[1551]\tvalid_set's binary_logloss: 0.0356896\n",
      "\t0.9846\t = Validation score   (mcc)\n",
      "\t1051.68s\t = Training   runtime\n",
      "\t124.45s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 17.92s of the 17.82s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\t[1]\tvalid_set's binary_logloss: 0.641358\n",
      "\tTime limit exceeded... Skipping LightGBM_BAG_L2.\n",
      "Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 12.74s of the 12.65s of remaining time.\n",
      "\tWarning: Model is expected to require 1111.0s to train, which exceeds the maximum time limit of 12.7s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestGini_BAG_L2.\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the -4.74s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t0.9847\t = Validation score   (mcc)\n",
      "\t7.45s\t = Training   runtime\n",
      "\t0.36s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 3652.21s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1252.1 rows/s (311695 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240811_030711\")\n"
     ]
    }
   ],
   "source": [
    "scores = []    \n",
    "oof_pred_probs = np.zeros((train.shape[0], len(np.unique(train[TARGET]))))\n",
    "test_pred_probs = np.zeros((test.shape[0], len(np.unique(train[TARGET]))))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, random_state=SEED, shuffle=True)\n",
    "split = skf.split(train.drop(columns=TARGET), train[TARGET])\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(split):\n",
    "    _train, _val = train.iloc[train_idx], train.iloc[val_idx]   \n",
    "    \n",
    "    predictor = TabularPredictor(\n",
    "        label=TARGET,\n",
    "        eval_metric='mcc',\n",
    "        problem_type='binary',\n",
    "        verbosity=2\n",
    "    )\n",
    "    \n",
    "    predictor.fit(\n",
    "        train_data=_train,\n",
    "        time_limit=TIME_LIMIT // N_FOLDS,\n",
    "        presets='best_quality',\n",
    "        excluded_model_types=['KNN'],\n",
    "        ag_args_fit={\n",
    "            'num_gpus': 1, \n",
    "            'stopping_metric': 'log_loss'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    y_pred_probs = predictor.predict_proba(_val)\n",
    "    oof_pred_probs[val_idx] = y_pred_probs            \n",
    "\n",
    "    temp_test_pred_probs = predictor.predict_proba(test)\n",
    "    test_pred_probs += temp_test_pred_probs / N_FOLDS\n",
    "\n",
    "    score = matthews_corrcoef(_val[TARGET].map({'e': 0, 'p': 1}), np.argmax(y_pred_probs, axis=1))\n",
    "    scores.append(score)\n",
    "\n",
    "    del predictor, _train, _val, y_pred_probs, temp_test_pred_probs\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad2b2ecb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T05:10:01.407751Z",
     "iopub.status.busy": "2024-08-11T05:10:01.406968Z",
     "iopub.status.idle": "2024-08-11T05:10:01.413965Z",
     "shell.execute_reply": "2024-08-11T05:10:01.412921Z"
    },
    "papermill": {
     "duration": 0.133265,
     "end_time": "2024-08-11T05:10:01.416605",
     "exception": false,
     "start_time": "2024-08-11T05:10:01.283340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - MCC: 0.984637\n",
      "Fold 2 - MCC: 0.985046\n",
      "Fold 3 - MCC: 0.984650\n",
      "Fold 4 - MCC: 0.984817\n",
      "Fold 5 - MCC: 0.984847\n",
      "\n",
      "--- Average MCC: 0.984799  0.000150\n"
     ]
    }
   ],
   "source": [
    "for i in range(N_FOLDS):\n",
    "    print(f'Fold {i + 1} - MCC: {scores[i]:.6f}')\n",
    "    \n",
    "print(f'\\n--- Average MCC: {np.mean(scores):.6f}  {np.std(scores):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46274678",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T05:10:01.677275Z",
     "iopub.status.busy": "2024-08-11T05:10:01.676861Z",
     "iopub.status.idle": "2024-08-11T05:10:05.411211Z",
     "shell.execute_reply": "2024-08-11T05:10:05.410234Z"
    },
    "papermill": {
     "duration": 3.863346,
     "end_time": "2024-08-11T05:10:05.414018",
     "exception": false,
     "start_time": "2024-08-11T05:10:01.550672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_pred_probs(oof_pred_probs, np.mean(scores), 'oof')\n",
    "save_pred_probs(test_pred_probs, np.mean(scores), 'test')\n",
    "save_submission(test_pred_probs, np.mean(scores)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "252f647c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T05:10:05.663061Z",
     "iopub.status.busy": "2024-08-11T05:10:05.662683Z",
     "iopub.status.idle": "2024-08-11T05:10:06.004666Z",
     "shell.execute_reply": "2024-08-11T05:10:06.003493Z"
    },
    "papermill": {
     "duration": 0.470211,
     "end_time": "2024-08-11T05:10:06.007500",
     "exception": false,
     "start_time": "2024-08-11T05:10:05.537289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "shutil.rmtree(\"AutogluonModels\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9045607,
     "sourceId": 76727,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 39922.521561,
   "end_time": "2024-08-11T05:10:08.657730",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-10T18:04:46.136169",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
