{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "793de44c",
   "metadata": {
    "papermill": {
     "duration": 0.00405,
     "end_time": "2024-11-02T08:35:04.190754",
     "exception": false,
     "start_time": "2024-11-02T08:35:04.186704",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports and configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8be0f615",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-02T08:35:04.199360Z",
     "iopub.status.busy": "2024-11-02T08:35:04.198975Z",
     "iopub.status.idle": "2024-11-02T08:35:49.739434Z",
     "shell.execute_reply": "2024-11-02T08:35:49.738467Z"
    },
    "papermill": {
     "duration": 45.547414,
     "end_time": "2024-11-02T08:35:49.741860",
     "exception": false,
     "start_time": "2024-11-02T08:35:04.194446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cuml 24.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "aiobotocore 2.15.1 requires botocore<1.35.24,>=1.35.16, but you have botocore 1.29.165 which is incompatible.\r\n",
      "bigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\r\n",
      "bigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n",
      "bigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.2 which is incompatible.\r\n",
      "cesium 0.12.3 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\r\n",
      "dataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.9.2 which is incompatible.\r\n",
      "libpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\r\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "tsfresh 0.20.3 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.12.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q autogluon.tabular ray==2.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8755460",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T08:35:49.751456Z",
     "iopub.status.busy": "2024-11-02T08:35:49.750737Z",
     "iopub.status.idle": "2024-11-02T08:35:52.695762Z",
     "shell.execute_reply": "2024-11-02T08:35:52.694834Z"
    },
    "papermill": {
     "duration": 2.952854,
     "end_time": "2024-11-02T08:35:52.698448",
     "exception": false,
     "start_time": "2024-11-02T08:35:49.745594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import shutil\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37cfea6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T08:35:52.707705Z",
     "iopub.status.busy": "2024-11-02T08:35:52.707176Z",
     "iopub.status.idle": "2024-11-02T08:35:52.712333Z",
     "shell.execute_reply": "2024-11-02T08:35:52.711522Z"
    },
    "papermill": {
     "duration": 0.012107,
     "end_time": "2024-11-02T08:35:52.714540",
     "exception": false,
     "start_time": "2024-11-02T08:35:52.702433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    train_path = '/kaggle/input/playground-series-s4e11/train.csv'\n",
    "    test_path = '/kaggle/input/playground-series-s4e11/test.csv'\n",
    "    sample_sub_path = '/kaggle/input/playground-series-s4e11/sample_submission.csv'\n",
    "    original_data_path = '/kaggle/input/depression-surveydataset-for-analysis/final_depression_dataset_1.csv'\n",
    "    \n",
    "    target = 'Depression'\n",
    "    time_limit = 3600 * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbeb331",
   "metadata": {
    "papermill": {
     "duration": 0.003666,
     "end_time": "2024-11-02T08:35:52.722233",
     "exception": false,
     "start_time": "2024-11-02T08:35:52.718567",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "442c8709",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T08:35:52.732977Z",
     "iopub.status.busy": "2024-11-02T08:35:52.732061Z",
     "iopub.status.idle": "2024-11-02T08:35:53.693096Z",
     "shell.execute_reply": "2024-11-02T08:35:53.692014Z"
    },
    "papermill": {
     "duration": 0.969788,
     "end_time": "2024-11-02T08:35:53.695656",
     "exception": false,
     "start_time": "2024-11-02T08:35:52.725868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(CFG.train_path, index_col='id')\n",
    "test = pd.read_csv(CFG.test_path, index_col='id')\n",
    "\n",
    "original = pd.read_csv(CFG.original_data_path)\n",
    "original[CFG.target] = original[CFG.target].map({'Yes': 1, 'No': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecdac4b",
   "metadata": {
    "papermill": {
     "duration": 0.003776,
     "end_time": "2024-11-02T08:35:53.703778",
     "exception": false,
     "start_time": "2024-11-02T08:35:53.700002",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fitting the predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a81e659",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T08:35:53.713472Z",
     "iopub.status.busy": "2024-11-02T08:35:53.713027Z",
     "iopub.status.idle": "2024-11-02T08:35:53.719627Z",
     "shell.execute_reply": "2024-11-02T08:35:53.718670Z"
    },
    "papermill": {
     "duration": 0.014079,
     "end_time": "2024-11-02T08:35:53.721722",
     "exception": false,
     "start_time": "2024-11-02T08:35:53.707643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20241102_083553\"\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(\n",
    "    problem_type='binary',\n",
    "    eval_metric='accuracy',\n",
    "    label=CFG.target,\n",
    "    verbosity=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23cbd677",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-11-02T08:35:53.730909Z",
     "iopub.status.busy": "2024-11-02T08:35:53.730646Z",
     "iopub.status.idle": "2024-11-02T18:38:54.375201Z",
     "shell.execute_reply": "2024-11-02T18:38:54.374127Z"
    },
    "papermill": {
     "duration": 36180.651823,
     "end_time": "2024-11-02T18:38:54.377436",
     "exception": false,
     "start_time": "2024-11-02T08:35:53.725613",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predictor not fit prior to pseudolabeling. Fitting now...\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.10.14\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jun 27 20:43:36 UTC 2024\n",
      "CPU Count:          4\n",
      "Memory Avail:       30.20 GB / 31.36 GB (96.3%)\n",
      "Disk Space Avail:   19.50 GB / 19.52 GB (99.9%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 4500s of the 18000s of remaining time (25%).\n",
      "2024-11-02 08:35:54,122\tINFO util.py:124 -- Outdated packages:\n",
      "  ipywidgets==7.7.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2024-11-02 08:35:57,107\tINFO worker.py:1752 -- Started a local Ray instance.\n",
      "\t\tContext path: \"AutogluonModels/ag-20241102_083553/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Beginning AutoGluon training ... Time limit = 4495s\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m AutoGluon will save models to \"AutogluonModels/ag-20241102_083553/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Train Data Rows:    125066\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Train Data Columns: 18\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Label Column:       Depression\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Problem Type:       binary\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tAvailable Memory:                    30393.11 MB\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tTrain Data (Original)  Memory Usage: 83.58 MB (0.3% of available memory)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t\t('float', [])  :  8 | ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', ...]\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t\t('object', []) : 10 | ['Name', 'Gender', 'City', 'Working Professional or Student', 'Profession', ...]\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t\t('category', [])  : 6 | ['Name', 'City', 'Profession', 'Sleep Duration', 'Dietary Habits', ...]\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t\t('float', [])     : 8 | ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', ...]\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t\t('int', ['bool']) : 4 | ['Gender', 'Working Professional or Student', 'Have you ever had suicidal thoughts ?', 'Family History of Mental Illness']\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t1.2s = Fit runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t18 features in original data used to generate 18 features in processed data.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tTrain Data (Processed) Memory Usage: 8.95 MB (0.0% of available memory)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Data preprocessing and feature engineering runtime = 1.28s ...\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting 110 L1 models ...\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 2995.07s of the 4493.71s of remaining time.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.9092\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.88s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t1.19s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 2992.88s of the 4491.52s of remaining time.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.9025\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.22s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t1.22s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 2991.36s of the 4490.0s of remaining time.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.23%)\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=368)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=416)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=464)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=512)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=560)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=608)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=656)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=704)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.9399\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t63.64s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.69s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 2922.45s of the 4421.09s of remaining time.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.23%)\n",
      "\u001b[36m(_ray_fit pid=758)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=806)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=854)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=902)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=950)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=998)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1046)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1094)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.9389\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t55.38s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.63s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 2864.66s of the 4363.29s of remaining time.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.9361\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t21.7s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t4.15s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 2837.89s of the 4336.53s of remaining time.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.9362\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t20.83s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t3.86s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 2812.42s of the 4311.05s of remaining time.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.24%)\n",
      "\u001b[36m(_ray_fit pid=1176)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1176)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=1243)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1243)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=1310)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1310)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=1377)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1377)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=1444)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1444)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=1511)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1511)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=1578)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1578)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=1645)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1645)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.9381\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t127.58s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.2s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 2682.49s of the 4181.13s of remaining time.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.9355\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t13.59s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t4.36s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 2663.42s of the 4162.05s of remaining time.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.9355\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t15.2s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t4.45s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 2642.69s of the 4141.33s of remaining time.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.37%)\n",
      "\u001b[36m(_ray_fit pid=1746)\u001b[0m No improvement since epoch 3: early stopping\n",
      "\u001b[36m(_ray_fit pid=1746)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=1746)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=1798)\u001b[0m No improvement since epoch 4: early stopping\n",
      "\u001b[36m(_ray_fit pid=1798)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=1798)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=1850)\u001b[0m No improvement since epoch 2: early stopping\n",
      "\u001b[36m(_ray_fit pid=1850)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=1850)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=1902)\u001b[0m No improvement since epoch 4: early stopping\n",
      "\u001b[36m(_ray_fit pid=1902)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=1902)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=1954)\u001b[0m No improvement since epoch 4: early stopping\n",
      "\u001b[36m(_ray_fit pid=1954)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=1954)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=2006)\u001b[0m No improvement since epoch 2: early stopping\n",
      "\u001b[36m(_ray_fit pid=2006)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=2006)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=2058)\u001b[0m No improvement since epoch 4: early stopping\n",
      "\u001b[36m(_ray_fit pid=2058)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=2058)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=2110)\u001b[0m No improvement since epoch 2: early stopping\n",
      "\u001b[36m(_ray_fit pid=2110)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=2110)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.9385\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t1128.47s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t2.52s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 1511.52s of the 3010.15s of remaining time.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.32%)\n",
      "\u001b[36m(_ray_fit pid=2168)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:00:50] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=2168)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=2168)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:00:50] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=2168)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=2168)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=2168)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=2168)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=2168)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:00:51] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=2168)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=2168)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=2168)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=2168)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=2168)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:00:51] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=2168)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=2168)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=2168)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=2168)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=2168)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=2168)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=2168)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=2211)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:00:54] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=2211)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2211)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:00:55] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2211)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2211)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2254)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:00:59] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=2211)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:00:55] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=2211)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=2211)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=2211)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=2211)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=2254)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:00:59] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=2254)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=2254)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=2254)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=2254)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=2297)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2297)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:01:03] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2297)\u001b[0m \u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2297)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2297)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:01:03] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=2340)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:01:07] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=2297)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:01:04] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=2297)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=2297)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=2297)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=2297)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=2340)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:01:08] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=2340)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=2340)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=2340)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=2340)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=2340)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2340)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:01:08] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2340)\u001b[0m \u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2340)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2383)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:01:12] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=2426)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:01:16] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=2383)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:01:13] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=2383)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=2383)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=2383)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=2383)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=2426)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2426)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:01:16] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2426)\u001b[0m \u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2426)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2426)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:01:17] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=2426)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=2426)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=2426)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=2426)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.9396\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t33.28s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.49s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 1475.7s of the 2974.33s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=2469)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:01:21] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=2469)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2469)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:01:21] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2469)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2469)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.21%)\n",
      "\u001b[36m(_ray_fit pid=2518)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=2518)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=2469)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:01:21] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=2469)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=2469)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=2469)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=2469)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=2469)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2469)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:01:21] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=2469)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2469)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=2564)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=2564)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=2610)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=2610)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=2656)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=2656)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=2702)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=2702)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=2748)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=2748)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=2794)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=2794)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=2840)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=2840)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.9386\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t404.23s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.66s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 1069.06s of the 2567.7s of remaining time.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.27%)\n",
      "\u001b[36m(_ray_fit pid=2892)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2940)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2989)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3037)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3085)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3133)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3181)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3229)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.9388\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t87.83s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.91s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 978.77s of the 2477.4s of remaining time.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.24%)\n",
      "\u001b[36m(_ray_fit pid=3283)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3283)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=3350)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3350)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=3423)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3423)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=3496)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3496)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=3569)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3569)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=3642)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3642)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=3715)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3715)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=3788)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3788)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.9386\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t81.84s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.2s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 894.52s of the 2393.16s of remaining time.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.21%)\n",
      "\u001b[36m(_ray_fit pid=3867)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=3867)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=3913)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=3913)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=3959)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=3959)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=4005)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=4005)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=4051)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=4051)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=4097)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=4097)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=4143)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=4143)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=4189)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=4189)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.9391\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t533.17s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.75s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 358.79s of the 1857.43s of remaining time.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.25%)\n",
      "\u001b[36m(_ray_fit pid=4241)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=4288)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=4336)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=4384)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=4432)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=4481)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=4528)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=4576)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.8183\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t40.09s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.1s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 316.3s of the 1814.93s of remaining time.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.37%)\n",
      "\u001b[36m(_ray_fit pid=4629)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 4)\n",
      "\u001b[36m(_ray_fit pid=4629)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=4629)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=4682)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 4)\n",
      "\u001b[36m(_ray_fit pid=4682)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=4682)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=4734)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 4)\n",
      "\u001b[36m(_ray_fit pid=4734)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=4734)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=4786)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 4)\n",
      "\u001b[36m(_ray_fit pid=4786)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=4786)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=4838)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 4)\n",
      "\u001b[36m(_ray_fit pid=4838)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=4838)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=4890)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 4)\n",
      "\u001b[36m(_ray_fit pid=4890)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=4890)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=4942)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 4)\n",
      "\u001b[36m(_ray_fit pid=4942)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=4942)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=4994)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 4)\n",
      "\u001b[36m(_ray_fit pid=4994)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=4994)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.9383\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t276.45s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t2.47s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 37.12s of the 1535.76s of remaining time.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.32%)\n",
      "\u001b[36m(_ray_fit pid=5052)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=5052)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Failed to unpickle serialized exception\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/exceptions.py\", line 46, in from_ray_exception\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m     return pickle.loads(ray_exception.serialized_exception)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m ModuleNotFoundError: No module named '_catboost'\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \n",
      "\u001b[36m(_dystack pid=169)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/_private/serialization.py\", line 404, in deserialize_objects\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/_private/serialization.py\", line 293, in _deserialize_object\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m     return RayError.from_bytes(obj)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/exceptions.py\", line 40, in from_bytes\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m     return RayError.from_ray_exception(ray_exception)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/exceptions.py\", line 49, in from_ray_exception\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m     raise RuntimeError(msg) from e\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m RuntimeError: Failed to unpickle serialized exception\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tWarning: Exception caused CatBoost_r9_BAG_L1 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t\tSystem error: Failed to unpickle serialized exception\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m traceback: Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/exceptions.py\", line 46, in from_ray_exception\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m     return pickle.loads(ray_exception.serialized_exception)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m ModuleNotFoundError: No module named '_catboost'\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \n",
      "\u001b[36m(_dystack pid=169)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/_private/serialization.py\", line 404, in deserialize_objects\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/_private/serialization.py\", line 293, in _deserialize_object\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m     return RayError.from_bytes(obj)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/exceptions.py\", line 40, in from_bytes\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m     return RayError.from_ray_exception(ray_exception)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/exceptions.py\", line 49, in from_ray_exception\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m     raise RuntimeError(msg) from e\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m RuntimeError: Failed to unpickle serialized exception\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m     model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 288, in _fit\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m     self._fit_folds(\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 714, in _fit_folds\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m     fold_fitting_strategy.after_all_folds_scheduled()\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 668, in after_all_folds_scheduled\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m     self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 610, in _run_parallel\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m     self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 572, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m     raise processed_exception\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 537, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m     fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size = self.ray.get(finished)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m     return fn(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/_private/worker.py\", line 2667, in get\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m     raise value\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m ray.exceptions.RaySystemError: System error: Failed to unpickle serialized exception\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m traceback: Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/exceptions.py\", line 46, in from_ray_exception\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m     return pickle.loads(ray_exception.serialized_exception)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m ModuleNotFoundError: No module named '_catboost'\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \n",
      "\u001b[36m(_dystack pid=169)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/_private/serialization.py\", line 404, in deserialize_objects\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/_private/serialization.py\", line 293, in _deserialize_object\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m     return RayError.from_bytes(obj)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/exceptions.py\", line 40, in from_bytes\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m     return RayError.from_ray_exception(ray_exception)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/ray/exceptions.py\", line 49, in from_ray_exception\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m     raise RuntimeError(msg) from e\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m RuntimeError: Failed to unpickle serialized exception\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 28.89s of the 1527.53s of remaining time.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.23%)\n",
      "\u001b[36m(_ray_fit pid=5116)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=5210)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5257)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=5305)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=5353)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=5401)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=5449)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.8183\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t39.02s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.08s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 1485.93s of remaining time.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.9399\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t8.19s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.02s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting 108 L2 models ...\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1477.7s of the 1477.57s of remaining time.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.43%)\n",
      "\u001b[36m(_ray_fit pid=5510)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=5558)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=5606)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=5654)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=5702)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=5750)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=5798)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=5846)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.9402\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t47.6s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.25s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 1427.57s of the 1427.43s of remaining time.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.43%)\n",
      "\u001b[36m(_ray_fit pid=5906)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=5954)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=6002)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=6050)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=6098)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=6146)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=6194)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=6242)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.94\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t50.19s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.27s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 1374.86s of the 1374.73s of remaining time.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.9393\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t67.1s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t5.68s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 1301.46s of the 1301.33s of remaining time.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.9394\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t69.04s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t5.32s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 1226.56s of the 1226.43s of remaining time.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.45%)\n",
      "\u001b[36m(_ray_fit pid=6330)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=6330)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=6397)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=6397)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=6464)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=6464)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=6531)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=6531)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=6592)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=6592)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=6659)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=6659)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=6726)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=6726)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=6786)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=6786)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.9407\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t56.45s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.12s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 1167.62s of the 1167.49s of remaining time.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.9398\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t19.86s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t5.92s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 1140.99s of the 1140.86s of remaining time.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.9392\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t20.36s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t5.91s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 1113.91s of the 1113.77s of remaining time.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.70%)\n",
      "\u001b[36m(_ray_fit pid=6887)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=6887)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=6939)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=6939)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=6991)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=6991)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=7043)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=7043)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=7095)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=7095)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=7147)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=7147)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=7199)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=7199)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=7251)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=7251)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.9404\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t847.11s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t2.54s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: XGBoost_BAG_L2 ... Training model for up to 263.96s of the 263.83s of remaining time.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.60%)\n",
      "\u001b[36m(_ray_fit pid=7315)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:46:36] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=7315)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=7315)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:46:36] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=7315)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=7315)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=7315)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=7315)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=7315)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:46:37] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=7315)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=7315)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=7315)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=7315)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=7315)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:46:37] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=7315)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=7315)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=7315)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=7315)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=7315)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=7315)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=7315)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=7401)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:46:45] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7401)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7401)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:46:45] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7401)\u001b[0m \u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7401)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7358)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:46:41] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=7358)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=7358)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=7358)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=7358)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=7401)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:46:45] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=7401)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=7401)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=7401)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=7401)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=7487)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:46:53] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7487)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7487)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:46:53] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7487)\u001b[0m \u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7487)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7444)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:46:50] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=7444)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=7444)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=7444)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=7444)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=7487)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:46:54] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=7487)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=7487)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=7487)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=7487)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=7573)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:47:02] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7573)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7573)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:47:02] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7573)\u001b[0m \u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7573)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7530)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:46:58] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=7530)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=7530)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=7530)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=7530)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=7573)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:47:03] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=7573)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=7573)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=7573)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=7573)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.9402\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t32.64s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.54s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 228.84s of the 228.71s of remaining time.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.39%)\n",
      "\u001b[36m(_ray_fit pid=7616)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:47:07] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=7616)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7616)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:47:07] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7616)\u001b[0m \u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7616)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7671)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 11)\n",
      "\u001b[36m(_ray_fit pid=7671)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=7671)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=7616)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [09:47:07] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=7616)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=7616)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=7616)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=7616)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=7717)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 11)\n",
      "\u001b[36m(_ray_fit pid=7717)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=7717)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=7763)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 11)\n",
      "\u001b[36m(_ray_fit pid=7763)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=7763)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=7809)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 11)\n",
      "\u001b[36m(_ray_fit pid=7809)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=7809)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=7855)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 11)\n",
      "\u001b[36m(_ray_fit pid=7855)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=7855)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=7901)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 11)\n",
      "\u001b[36m(_ray_fit pid=7901)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=7901)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=7947)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 11)\n",
      "\u001b[36m(_ray_fit pid=7947)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=7947)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_ray_fit pid=7993)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 11)\n",
      "\u001b[36m(_ray_fit pid=7993)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=7993)\u001b[0m   self.model = torch.load(net_filename)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.9396\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t206.16s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t1.37s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 20.08s of the 19.95s of remaining time.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.52%)\n",
      "\u001b[36m(_ray_fit pid=8051)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=8051)\u001b[0m \tRan out of time, early stopping on iteration 51. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=8051)\u001b[0m \t[50]\tvalid_set's binary_error: 0.0633235\n",
      "\u001b[36m(_ray_fit pid=8099)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=8099)\u001b[0m \tRan out of time, early stopping on iteration 52. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=8099)\u001b[0m \t[45]\tvalid_set's binary_error: 0.0599974\n",
      "\u001b[36m(_ray_fit pid=8147)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=8147)\u001b[0m \tRan out of time, early stopping on iteration 57. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=8147)\u001b[0m \t[57]\tvalid_set's binary_error: 0.0599373\n",
      "\u001b[36m(_ray_fit pid=8195)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=8195)\u001b[0m \tRan out of time, early stopping on iteration 53. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=8195)\u001b[0m \t[53]\tvalid_set's binary_error: 0.0619843\n",
      "\u001b[36m(_ray_fit pid=8243)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=8243)\u001b[0m \tRan out of time, early stopping on iteration 49. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=8243)\u001b[0m \t[48]\tvalid_set's binary_error: 0.0617284\n",
      "\u001b[36m(_ray_fit pid=8291)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=8291)\u001b[0m \tRan out of time, early stopping on iteration 57. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=8291)\u001b[0m \t[55]\tvalid_set's binary_error: 0.0617284\n",
      "\u001b[36m(_ray_fit pid=8339)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=8339)\u001b[0m \tRan out of time, early stopping on iteration 58. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=8339)\u001b[0m \t[51]\tvalid_set's binary_error: 0.0613446\n",
      "\u001b[36m(_ray_fit pid=8387)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=8387)\u001b[0m \tRan out of time, early stopping on iteration 56. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=8387)\u001b[0m \t[54]\tvalid_set's binary_error: 0.0592976\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.9388\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t49.16s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.23s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the -32.82s of remaining time.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \tEnsemble Weights: {'CatBoost_BAG_L2': 1.0}\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.9407\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t10.59s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m \t0.02s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m AutoGluon training complete, total runtime = 4538.48s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 1278.5 rows/s (15634 batch size)\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241102_083553/ds_sub_fit/sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(_dystack pid=169)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                          model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0                XGBoost_BAG_L2       0.942817   0.940160    accuracy       20.656769      29.472358  2976.027204                 0.622251                0.537258          32.637366            2       True         28\n",
      "1               CatBoost_BAG_L2       0.942561   0.940695    accuracy       20.101078      29.058074  2999.844089                 0.066560                0.122974          56.454251            2       True         24\n",
      "2           WeightedEnsemble_L3       0.942561   0.940695    accuracy       20.103190      29.075265  3010.438696                 0.002112                0.017191          10.594607            3       True         31\n",
      "3       RandomForestGini_BAG_L2       0.942049   0.939280    accuracy       20.658025      34.614960  3010.485278                 0.623507                5.679860          67.095440            2       True         22\n",
      "4             LightGBMXT_BAG_L2       0.941921   0.940240    accuracy       20.395432      29.189993  2990.990567                 0.360914                0.254894          47.600729            2       True         20\n",
      "5       RandomForestEntr_BAG_L2       0.941921   0.939416    accuracy       20.580648      34.251168  3012.431082                 0.546130                5.316069          69.041244            2       True         23\n",
      "6         ExtraTreesEntr_BAG_L2       0.941921   0.939152    accuracy       20.898827      34.849515  2963.752817                 0.864309                5.914416          20.362979            2       True         26\n",
      "7        NeuralNetFastAI_BAG_L2       0.941921   0.940423    accuracy       22.917664      31.479882  3790.500968                 2.883146                2.544783         847.111130            2       True         27\n",
      "8          LightGBMLarge_BAG_L2       0.941474   0.938832    accuracy       20.326392      29.160491  2992.549948                 0.291874                0.225391          49.160110            2       True         30\n",
      "9               LightGBM_BAG_L2       0.941474   0.940016    accuracy       20.378696      29.205959  2993.578222                 0.344178                0.270859          50.188384            2       True         21\n",
      "10        ExtraTreesGini_BAG_L2       0.941474   0.939808    accuracy       20.940002      34.855188  2963.248359                 0.905484                5.920088          19.858521            2       True         25\n",
      "11              LightGBM_BAG_L1       0.941154   0.938944    accuracy        0.669904       0.631293    55.381303                 0.669904                0.631293          55.381303            1       True          4\n",
      "12  NeuralNetFastAI_r191_BAG_L1       0.941090   0.938297    accuracy        3.253181       2.474037   276.446043                 3.253181                2.474037         276.446043            1       True         17\n",
      "13        NeuralNetTorch_BAG_L2       0.941090   0.939568    accuracy       21.463061      30.301698  3149.548506                 1.428543                1.366598         206.158668            2       True         29\n",
      "14               XGBoost_BAG_L1       0.940898   0.939648    accuracy        0.996718       0.490223    33.279439                 0.996718                0.490223          33.279439            1       True         11\n",
      "15       NeuralNetFastAI_BAG_L1       0.940898   0.938528    accuracy        5.764746       2.515585  1128.474686                 5.764746                2.515585        1128.474686            1       True         10\n",
      "16            LightGBMXT_BAG_L1       0.940578   0.939904    accuracy        0.747061       0.688182    63.642568                 0.747061                0.688182          63.642568            1       True          3\n",
      "17          WeightedEnsemble_L2       0.940578   0.939904    accuracy        0.748942       0.707066    71.830024                 0.001881                0.018884           8.187455            2       True         19\n",
      "18        NeuralNetTorch_BAG_L1       0.940578   0.938560    accuracy        0.864812       0.661920   404.230451                 0.864812                0.661920         404.230451            1       True         12\n",
      "19    NeuralNetTorch_r79_BAG_L1       0.940514   0.939120    accuracy        0.934886       0.750780   533.169709                 0.934886                0.750780         533.169709            1       True         15\n",
      "20         CatBoost_r177_BAG_L1       0.940386   0.938640    accuracy        0.160003       0.197495    81.838303                 0.160003                0.197495          81.838303            1       True         14\n",
      "21              CatBoost_BAG_L1       0.940130   0.938113    accuracy        0.515944       0.195101   127.583037                 0.515944                0.195101         127.583037            1       True          7\n",
      "22         LightGBMLarge_BAG_L1       0.940130   0.938840    accuracy        1.435225       0.914651    87.828556                 1.435225                0.914651          87.828556            1       True         13\n",
      "23        ExtraTreesEntr_BAG_L1       0.938979   0.935466    accuracy        1.162118       4.451702    15.195222                 1.162118                4.451702          15.195222            1       True          9\n",
      "24      RandomForestGini_BAG_L1       0.938467   0.936106    accuracy        0.976318       4.153341    21.704825                 0.976318                4.153341          21.704825            1       True          5\n",
      "25      RandomForestEntr_BAG_L1       0.938020   0.936234    accuracy        0.860696       3.863355    20.828608                 0.860696                3.863355          20.828608            1       True          6\n",
      "26        ExtraTreesGini_BAG_L1       0.937572   0.935474    accuracy        1.197336       4.357104    13.587989                 1.197336                4.357104          13.587989            1       True          8\n",
      "27        KNeighborsUnif_BAG_L1       0.913394   0.909192    accuracy        0.185311       1.194564     0.881196                 0.185311                1.194564           0.881196            1       True          1\n",
      "28        KNeighborsDist_BAG_L1       0.902264   0.902499    accuracy        0.157389       1.216246     0.216328                 0.157389                1.216246           0.216328            1       True          2\n",
      "29          LightGBM_r96_BAG_L1       0.818281   0.818288    accuracy        0.074991       0.083063    39.015565                 0.074991                0.083063          39.015565            1       True         18\n",
      "30         LightGBM_r131_BAG_L1       0.818281   0.818288    accuracy        0.077879       0.096456    40.086010                 0.077879                0.096456          40.086010            1       True         16\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t4576s\t = DyStack   runtime |\t13424s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 13424s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20241102_083553\"\n",
      "Train Data Rows:    140700\n",
      "Train Data Columns: 18\n",
      "Label Column:       Depression\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    29305.05 MB\n",
      "\tTrain Data (Original)  Memory Usage: 94.04 MB (0.3% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  8 | ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', ...]\n",
      "\t\t('object', []) : 10 | ['Name', 'Gender', 'City', 'Working Professional or Student', 'Profession', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 6 | ['Name', 'City', 'Profession', 'Sleep Duration', 'Dietary Habits', ...]\n",
      "\t\t('float', [])     : 8 | ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', ...]\n",
      "\t\t('int', ['bool']) : 4 | ['Gender', 'Working Professional or Student', 'Have you ever had suicidal thoughts ?', 'Family History of Mental Illness']\n",
      "\t1.5s = Fit runtime\n",
      "\t18 features in original data used to generate 18 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 10.07 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.63s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 110 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 8946.08s of the 13422.45s of remaining time.\n",
      "\t0.9098\t = Validation score   (accuracy)\n",
      "\t0.47s\t = Training   runtime\n",
      "\t1.42s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 8944.06s of the 13420.44s of remaining time.\n",
      "\t0.9029\t = Validation score   (accuracy)\n",
      "\t0.29s\t = Training   runtime\n",
      "\t1.72s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 8941.94s of the 13418.32s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.25%)\n",
      "\t0.9397\t = Validation score   (accuracy)\n",
      "\t60.59s\t = Training   runtime\n",
      "\t0.85s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 8876.95s of the 13353.32s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.26%)\n",
      "\t0.9391\t = Validation score   (accuracy)\n",
      "\t62.29s\t = Training   runtime\n",
      "\t0.87s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 8812.0s of the 13288.37s of remaining time.\n",
      "\t0.9372\t = Validation score   (accuracy)\n",
      "\t24.89s\t = Training   runtime\n",
      "\t5.33s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 8780.84s of the 13257.22s of remaining time.\n",
      "\t0.9368\t = Validation score   (accuracy)\n",
      "\t24.2s\t = Training   runtime\n",
      "\t5.23s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 8750.48s of the 13226.86s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.27%)\n",
      "\t0.9394\t = Validation score   (accuracy)\n",
      "\t87.86s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 8660.15s of the 13136.53s of remaining time.\n",
      "\t0.936\t = Validation score   (accuracy)\n",
      "\t16.96s\t = Training   runtime\n",
      "\t5.72s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 8636.16s of the 13112.54s of remaining time.\n",
      "\t0.936\t = Validation score   (accuracy)\n",
      "\t17.69s\t = Training   runtime\n",
      "\t5.62s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 8611.66s of the 13088.03s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.42%)\n",
      "\t0.939\t = Validation score   (accuracy)\n",
      "\t1289.36s\t = Training   runtime\n",
      "\t2.71s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 7319.54s of the 11795.91s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.36%)\n",
      "\t0.9396\t = Validation score   (accuracy)\n",
      "\t33.39s\t = Training   runtime\n",
      "\t0.54s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 7283.57s of the 11759.94s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.23%)\n",
      "\t0.9382\t = Validation score   (accuracy)\n",
      "\t436.92s\t = Training   runtime\n",
      "\t0.72s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 6844.06s of the 11320.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.30%)\n",
      "\t0.9388\t = Validation score   (accuracy)\n",
      "\t85.37s\t = Training   runtime\n",
      "\t1.1s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 6756.2s of the 11232.57s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.27%)\n",
      "\t0.9398\t = Validation score   (accuracy)\n",
      "\t82.32s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 6671.32s of the 11147.69s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.23%)\n",
      "\t0.9391\t = Validation score   (accuracy)\n",
      "\t574.29s\t = Training   runtime\n",
      "\t0.8s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 6094.42s of the 10570.79s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.27%)\n",
      "\t0.8183\t = Validation score   (accuracy)\n",
      "\t39.52s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 6052.31s of the 10528.68s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.42%)\n",
      "\t0.9401\t = Validation score   (accuracy)\n",
      "\t1981.96s\t = Training   runtime\n",
      "\t2.76s\t = Validation runtime\n",
      "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 4067.08s of the 8543.45s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.35%)\n",
      "\t0.9395\t = Validation score   (accuracy)\n",
      "\t107.0s\t = Training   runtime\n",
      "\t0.57s\t = Validation runtime\n",
      "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 3957.48s of the 8433.86s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.26%)\n",
      "\t0.8183\t = Validation score   (accuracy)\n",
      "\t38.03s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 3916.79s of the 8393.16s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.23%)\n",
      "\t0.9389\t = Validation score   (accuracy)\n",
      "\t536.26s\t = Training   runtime\n",
      "\t0.74s\t = Validation runtime\n",
      "Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 3377.99s of the 7854.37s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.52%)\n",
      "\t0.8183\t = Validation score   (accuracy)\n",
      "\t30.74s\t = Training   runtime\n",
      "\t0.5s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 3344.65s of the 7821.03s of remaining time.\n",
      "\t0.9352\t = Validation score   (accuracy)\n",
      "\t28.4s\t = Training   runtime\n",
      "\t5.37s\t = Validation runtime\n",
      "Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 3309.98s of the 7786.36s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.25%)\n",
      "\t0.9389\t = Validation score   (accuracy)\n",
      "\t69.91s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r102_BAG_L1 ... Training model for up to 3237.68s of the 7714.05s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.42%)\n",
      "\t0.9385\t = Validation score   (accuracy)\n",
      "\t241.36s\t = Training   runtime\n",
      "\t0.71s\t = Validation runtime\n",
      "Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 2993.71s of the 7470.08s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.35%)\n",
      "\t0.9391\t = Validation score   (accuracy)\n",
      "\t162.51s\t = Training   runtime\n",
      "\t0.3s\t = Validation runtime\n",
      "Fitting model: RandomForest_r195_BAG_L1 ... Training model for up to 2828.69s of the 7305.07s of remaining time.\n",
      "\t0.9351\t = Validation score   (accuracy)\n",
      "\t58.36s\t = Training   runtime\n",
      "\t4.97s\t = Validation runtime\n",
      "Fitting model: LightGBM_r188_BAG_L1 ... Training model for up to 2764.8s of the 7241.17s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.29%)\n",
      "\t0.9403\t = Validation score   (accuracy)\n",
      "\t95.14s\t = Training   runtime\n",
      "\t1.47s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r145_BAG_L1 ... Training model for up to 2667.26s of the 7143.63s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.42%)\n",
      "\t0.94\t = Validation score   (accuracy)\n",
      "\t2147.94s\t = Training   runtime\n",
      "\t5.32s\t = Validation runtime\n",
      "Fitting model: XGBoost_r89_BAG_L1 ... Training model for up to 515.88s of the 4992.26s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.36%)\n",
      "\t0.94\t = Validation score   (accuracy)\n",
      "\t34.08s\t = Training   runtime\n",
      "\t0.57s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r30_BAG_L1 ... Training model for up to 479.15s of the 4955.53s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.23%)\n",
      "\t0.9397\t = Validation score   (accuracy)\n",
      "\t406.1s\t = Training   runtime\n",
      "\t0.91s\t = Validation runtime\n",
      "Fitting model: LightGBM_r130_BAG_L1 ... Training model for up to 70.41s of the 4546.79s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.29%)\n",
      "\t0.9397\t = Validation score   (accuracy)\n",
      "\t78.59s\t = Training   runtime\n",
      "\t1.09s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 894.61s of the 4465.19s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI_r191_BAG_L1': 0.261, 'LightGBM_r188_BAG_L1': 0.261, 'NeuralNetFastAI_BAG_L1': 0.13, 'NeuralNetFastAI_r145_BAG_L1': 0.13, 'XGBoost_BAG_L1': 0.087, 'CatBoost_r137_BAG_L1': 0.043, 'NeuralNetTorch_r30_BAG_L1': 0.043, 'LightGBM_r130_BAG_L1': 0.043}\n",
      "\t0.9408\t = Validation score   (accuracy)\n",
      "\t13.25s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting 108 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 4451.88s of the 4451.69s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.64%)\n",
      "\t0.9404\t = Validation score   (accuracy)\n",
      "\t54.05s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 4395.04s of the 4394.84s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.65%)\n",
      "\t0.9404\t = Validation score   (accuracy)\n",
      "\t53.34s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 4338.73s of the 4338.53s of remaining time.\n",
      "\t0.9402\t = Validation score   (accuracy)\n",
      "\t116.64s\t = Training   runtime\n",
      "\t7.59s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 4213.73s of the 4213.53s of remaining time.\n",
      "\t0.9407\t = Validation score   (accuracy)\n",
      "\t111.62s\t = Training   runtime\n",
      "\t7.55s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 4093.9s of the 4093.7s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.68%)\n",
      "\t0.9411\t = Validation score   (accuracy)\n",
      "\t58.72s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 4032.46s of the 4032.27s of remaining time.\n",
      "\t0.9402\t = Validation score   (accuracy)\n",
      "\t26.75s\t = Training   runtime\n",
      "\t8.14s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 3996.51s of the 3996.31s of remaining time.\n",
      "\t0.9399\t = Validation score   (accuracy)\n",
      "\t26.24s\t = Training   runtime\n",
      "\t8.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 3961.15s of the 3960.95s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=1.05%)\n",
      "\t0.9407\t = Validation score   (accuracy)\n",
      "\t1383.99s\t = Training   runtime\n",
      "\t2.9s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 2573.93s of the 2573.74s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.90%)\n",
      "\t0.941\t = Validation score   (accuracy)\n",
      "\t37.69s\t = Training   runtime\n",
      "\t0.74s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 2533.39s of the 2533.19s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.58%)\n",
      "\t0.9396\t = Validation score   (accuracy)\n",
      "\t439.52s\t = Training   runtime\n",
      "\t1.92s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 2090.89s of the 2090.69s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.76%)\n",
      "\t0.94\t = Validation score   (accuracy)\n",
      "\t67.26s\t = Training   runtime\n",
      "\t0.39s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 2020.83s of the 2020.63s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.69%)\n",
      "\t0.9411\t = Validation score   (accuracy)\n",
      "\t55.08s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L2 ... Training model for up to 1963.03s of the 1962.83s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.57%)\n",
      "\t0.9403\t = Validation score   (accuracy)\n",
      "\t637.66s\t = Training   runtime\n",
      "\t2.1s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L2 ... Training model for up to 1322.5s of the 1322.3s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.69%)\n",
      "\t0.8183\t = Validation score   (accuracy)\n",
      "\t45.58s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L2 ... Training model for up to 1273.99s of the 1273.79s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=1.06%)\n",
      "\t0.9407\t = Validation score   (accuracy)\n",
      "\t1054.36s\t = Training   runtime\n",
      "\t2.86s\t = Validation runtime\n",
      "Fitting model: CatBoost_r9_BAG_L2 ... Training model for up to 216.37s of the 216.17s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.89%)\n",
      "\t0.9413\t = Validation score   (accuracy)\n",
      "\t60.31s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Fitting model: LightGBM_r96_BAG_L2 ... Training model for up to 153.01s of the 152.81s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.64%)\n",
      "\t0.8183\t = Validation score   (accuracy)\n",
      "\t45.02s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L2 ... Training model for up to 105.05s of the 104.85s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.58%)\n",
      "\t0.9388\t = Validation score   (accuracy)\n",
      "\t115.44s\t = Training   runtime\n",
      "\t2.04s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 445.19s of the -15.38s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost_r9_BAG_L2': 1.0}\n",
      "\t0.9413\t = Validation score   (accuracy)\n",
      "\t12.76s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 13452.33s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 625.5 rows/s (17588 batch size)\n",
      "Deleting model WeightedEnsemble_L2. All files under AutogluonModels/ag-20241102_083553/models/WeightedEnsemble_L2 will be removed.\n",
      "Deleting model LightGBMXT_BAG_L2. All files under AutogluonModels/ag-20241102_083553/models/LightGBMXT_BAG_L2 will be removed.\n",
      "Deleting model LightGBM_BAG_L2. All files under AutogluonModels/ag-20241102_083553/models/LightGBM_BAG_L2 will be removed.\n",
      "Deleting model RandomForestGini_BAG_L2. All files under AutogluonModels/ag-20241102_083553/models/RandomForestGini_BAG_L2 will be removed.\n",
      "Deleting model RandomForestEntr_BAG_L2. All files under AutogluonModels/ag-20241102_083553/models/RandomForestEntr_BAG_L2 will be removed.\n",
      "Deleting model CatBoost_BAG_L2. All files under AutogluonModels/ag-20241102_083553/models/CatBoost_BAG_L2 will be removed.\n",
      "Deleting model ExtraTreesGini_BAG_L2. All files under AutogluonModels/ag-20241102_083553/models/ExtraTreesGini_BAG_L2 will be removed.\n",
      "Deleting model ExtraTreesEntr_BAG_L2. All files under AutogluonModels/ag-20241102_083553/models/ExtraTreesEntr_BAG_L2 will be removed.\n",
      "Deleting model NeuralNetFastAI_BAG_L2. All files under AutogluonModels/ag-20241102_083553/models/NeuralNetFastAI_BAG_L2 will be removed.\n",
      "Deleting model XGBoost_BAG_L2. All files under AutogluonModels/ag-20241102_083553/models/XGBoost_BAG_L2 will be removed.\n",
      "Deleting model NeuralNetTorch_BAG_L2. All files under AutogluonModels/ag-20241102_083553/models/NeuralNetTorch_BAG_L2 will be removed.\n",
      "Deleting model LightGBMLarge_BAG_L2. All files under AutogluonModels/ag-20241102_083553/models/LightGBMLarge_BAG_L2 will be removed.\n",
      "Deleting model CatBoost_r177_BAG_L2. All files under AutogluonModels/ag-20241102_083553/models/CatBoost_r177_BAG_L2 will be removed.\n",
      "Deleting model NeuralNetTorch_r79_BAG_L2. All files under AutogluonModels/ag-20241102_083553/models/NeuralNetTorch_r79_BAG_L2 will be removed.\n",
      "Deleting model LightGBM_r131_BAG_L2. All files under AutogluonModels/ag-20241102_083553/models/LightGBM_r131_BAG_L2 will be removed.\n",
      "Deleting model NeuralNetFastAI_r191_BAG_L2. All files under AutogluonModels/ag-20241102_083553/models/NeuralNetFastAI_r191_BAG_L2 will be removed.\n",
      "Deleting model LightGBM_r96_BAG_L2. All files under AutogluonModels/ag-20241102_083553/models/LightGBM_r96_BAG_L2 will be removed.\n",
      "Deleting model NeuralNetTorch_r22_BAG_L2. All files under AutogluonModels/ag-20241102_083553/models/NeuralNetTorch_r22_BAG_L2 will be removed.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241102_083553\")\n",
      "Fitting predictor using the provided pseudolabeled examples as extra training data...\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 110 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1_PSEUDO ... Training model for up to 11997.0s of the 17999.97s of remaining time.\n",
      "\t0.9103\t = Validation score   (accuracy)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t1.5s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1_PSEUDO ... Training model for up to 11995.15s of the 17998.12s of remaining time.\n",
      "\t0.9026\t = Validation score   (accuracy)\n",
      "\t0.25s\t = Training   runtime\n",
      "\t1.5s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1_PSEUDO ... Training model for up to 11993.29s of the 17996.27s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.26%)\n",
      "\t0.9397\t = Validation score   (accuracy)\n",
      "\t60.76s\t = Training   runtime\n",
      "\t0.8s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1_PSEUDO ... Training model for up to 11929.99s of the 17932.97s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.26%)\n",
      "\t0.9392\t = Validation score   (accuracy)\n",
      "\t59.7s\t = Training   runtime\n",
      "\t0.7s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1_PSEUDO ... Training model for up to 11867.58s of the 17870.55s of remaining time.\n",
      "\t0.9367\t = Validation score   (accuracy)\n",
      "\t25.34s\t = Training   runtime\n",
      "\t5.56s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1_PSEUDO ... Training model for up to 11835.79s of the 17838.76s of remaining time.\n",
      "\t0.937\t = Validation score   (accuracy)\n",
      "\t24.87s\t = Training   runtime\n",
      "\t5.34s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1_PSEUDO ... Training model for up to 11804.68s of the 17807.65s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.27%)\n",
      "\t0.9398\t = Validation score   (accuracy)\n",
      "\t98.57s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1_PSEUDO ... Training model for up to 11703.63s of the 17706.6s of remaining time.\n",
      "\t0.9365\t = Validation score   (accuracy)\n",
      "\t16.94s\t = Training   runtime\n",
      "\t5.95s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1_PSEUDO ... Training model for up to 11679.36s of the 17682.33s of remaining time.\n",
      "\t0.9364\t = Validation score   (accuracy)\n",
      "\t17.99s\t = Training   runtime\n",
      "\t6.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1_PSEUDO ... Training model for up to 11653.85s of the 17656.83s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.43%)\n",
      "\t0.9388\t = Validation score   (accuracy)\n",
      "\t1456.43s\t = Training   runtime\n",
      "\t2.96s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1_PSEUDO ... Training model for up to 10194.38s of the 16197.35s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.36%)\n",
      "\t0.9399\t = Validation score   (accuracy)\n",
      "\t35.99s\t = Training   runtime\n",
      "\t0.55s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1_PSEUDO ... Training model for up to 10155.47s of the 16158.44s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.24%)\n",
      "\t0.9386\t = Validation score   (accuracy)\n",
      "\t482.68s\t = Training   runtime\n",
      "\t0.76s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1_PSEUDO ... Training model for up to 9670.01s of the 15672.99s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.31%)\n",
      "\t0.9392\t = Validation score   (accuracy)\n",
      "\t90.68s\t = Training   runtime\n",
      "\t0.98s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L1_PSEUDO ... Training model for up to 9576.47s of the 15579.44s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.28%)\n",
      "\t0.9398\t = Validation score   (accuracy)\n",
      "\t84.51s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1_PSEUDO ... Training model for up to 9489.22s of the 15492.2s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.24%)\n",
      "\t0.9394\t = Validation score   (accuracy)\n",
      "\t649.03s\t = Training   runtime\n",
      "\t0.86s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L1_PSEUDO ... Training model for up to 8837.33s of the 14840.31s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.28%)\n",
      "\t0.8183\t = Validation score   (accuracy)\n",
      "\t42.92s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L1_PSEUDO ... Training model for up to 8791.8s of the 14794.77s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.42%)\n",
      "\t0.9402\t = Validation score   (accuracy)\n",
      "\t1803.1s\t = Training   runtime\n",
      "\t3.1s\t = Validation runtime\n",
      "Fitting model: CatBoost_r9_BAG_L1_PSEUDO ... Training model for up to 6985.24s of the 12988.21s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.35%)\n",
      "\t0.9397\t = Validation score   (accuracy)\n",
      "\t116.59s\t = Training   runtime\n",
      "\t0.68s\t = Validation runtime\n",
      "Fitting model: LightGBM_r96_BAG_L1_PSEUDO ... Training model for up to 6865.77s of the 12868.74s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.26%)\n",
      "\t0.8183\t = Validation score   (accuracy)\n",
      "\t40.47s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L1_PSEUDO ... Training model for up to 6822.65s of the 12825.63s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.23%)\n",
      "\t0.939\t = Validation score   (accuracy)\n",
      "\t601.73s\t = Training   runtime\n",
      "\t0.79s\t = Validation runtime\n",
      "Fitting model: XGBoost_r33_BAG_L1_PSEUDO ... Training model for up to 6218.13s of the 12221.11s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.52%)\n",
      "\t0.8183\t = Validation score   (accuracy)\n",
      "\t32.26s\t = Training   runtime\n",
      "\t0.48s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r42_BAG_L1_PSEUDO ... Training model for up to 6183.3s of the 12186.28s of remaining time.\n",
      "\t0.9357\t = Validation score   (accuracy)\n",
      "\t30.49s\t = Training   runtime\n",
      "\t5.28s\t = Validation runtime\n",
      "Fitting model: CatBoost_r137_BAG_L1_PSEUDO ... Training model for up to 6146.63s of the 12149.6s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.26%)\n",
      "\t0.9394\t = Validation score   (accuracy)\n",
      "\t76.11s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r102_BAG_L1_PSEUDO ... Training model for up to 6067.91s of the 12070.89s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.43%)\n",
      "\t0.9381\t = Validation score   (accuracy)\n",
      "\t257.38s\t = Training   runtime\n",
      "\t0.74s\t = Validation runtime\n",
      "Fitting model: CatBoost_r13_BAG_L1_PSEUDO ... Training model for up to 5807.75s of the 11810.73s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.35%)\n",
      "\t0.9393\t = Validation score   (accuracy)\n",
      "\t175.6s\t = Training   runtime\n",
      "\t0.36s\t = Validation runtime\n",
      "Fitting model: RandomForest_r195_BAG_L1_PSEUDO ... Training model for up to 5629.46s of the 11632.44s of remaining time.\n",
      "\t0.9351\t = Validation score   (accuracy)\n",
      "\t60.72s\t = Training   runtime\n",
      "\t5.15s\t = Validation runtime\n",
      "Fitting model: LightGBM_r188_BAG_L1_PSEUDO ... Training model for up to 5562.99s of the 11565.96s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.30%)\n",
      "\t0.94\t = Validation score   (accuracy)\n",
      "\t98.52s\t = Training   runtime\n",
      "\t1.69s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r145_BAG_L1_PSEUDO ... Training model for up to 5461.88s of the 11464.86s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.43%)\n",
      "\t0.9401\t = Validation score   (accuracy)\n",
      "\t3537.83s\t = Training   runtime\n",
      "\t5.65s\t = Validation runtime\n",
      "Fitting model: XGBoost_r89_BAG_L1_PSEUDO ... Training model for up to 1920.31s of the 7923.29s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.36%)\n",
      "\t0.9401\t = Validation score   (accuracy)\n",
      "\t35.04s\t = Training   runtime\n",
      "\t0.53s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r30_BAG_L1_PSEUDO ... Training model for up to 1882.56s of the 7885.54s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.24%)\n",
      "\t0.9397\t = Validation score   (accuracy)\n",
      "\t682.41s\t = Training   runtime\n",
      "\t0.87s\t = Validation runtime\n",
      "Fitting model: LightGBM_r130_BAG_L1_PSEUDO ... Training model for up to 1197.59s of the 7200.56s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.29%)\n",
      "\t0.9396\t = Validation score   (accuracy)\n",
      "\t70.74s\t = Training   runtime\n",
      "\t0.93s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r86_BAG_L1_PSEUDO ... Training model for up to 1124.35s of the 7127.32s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.24%)\n",
      "\t0.9383\t = Validation score   (accuracy)\n",
      "\t457.66s\t = Training   runtime\n",
      "\t0.75s\t = Validation runtime\n",
      "Fitting model: CatBoost_r50_BAG_L1_PSEUDO ... Training model for up to 664.15s of the 6667.12s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.26%)\n",
      "\t0.9395\t = Validation score   (accuracy)\n",
      "\t65.93s\t = Training   runtime\n",
      "\t0.39s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r11_BAG_L1_PSEUDO ... Training model for up to 595.78s of the 6598.76s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.43%)\n",
      "\t0.9366\t = Validation score   (accuracy)\n",
      "\t480.61s\t = Training   runtime\n",
      "\t4.71s\t = Validation runtime\n",
      "Fitting model: XGBoost_r194_BAG_L1_PSEUDO ... Training model for up to 112.0s of the 6114.97s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.39%)\n",
      "\t0.9379\t = Validation score   (accuracy)\n",
      "\t30.33s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r172_BAG_L1_PSEUDO ... Training model for up to 79.19s of the 6082.16s of remaining time.\n",
      "\t0.9366\t = Validation score   (accuracy)\n",
      "\t31.38s\t = Training   runtime\n",
      "\t4.66s\t = Validation runtime\n",
      "Fitting model: CatBoost_r69_BAG_L1_PSEUDO ... Training model for up to 42.73s of the 6045.71s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.26%)\n",
      "\t0.9282\t = Validation score   (accuracy)\n",
      "\t34.1s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r103_BAG_L1_PSEUDO ... Training model for up to 6.45s of the 6009.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.43%)\n",
      "\tTime limit exceeded... Skipping NeuralNetFastAI_r103_BAG_L1_PSEUDO.\n",
      "Fitting model: NeuralNetTorch_r14_BAG_L1_PSEUDO ... Training model for up to 0.77s of the 6003.75s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.24%)\n",
      "\tTime limit exceeded... Skipping NeuralNetTorch_r14_BAG_L1_PSEUDO.\n",
      "Fitting model: WeightedEnsemble_L2_PSEUDO ... Training model for up to 1199.7s of the 5997.69s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI_r191_BAG_L1_PSEUDO': 0.3, 'LightGBM_r188_BAG_L1_PSEUDO': 0.15, 'NeuralNetFastAI_r145_BAG_L1_PSEUDO': 0.15, 'XGBoost_BAG_L1_PSEUDO': 0.1, 'LightGBMXT_BAG_L1_PSEUDO': 0.05, 'RandomForestEntr_BAG_L1_PSEUDO': 0.05, 'NeuralNetFastAI_BAG_L1_PSEUDO': 0.05, 'CatBoost_r9_BAG_L1_PSEUDO': 0.05, 'NeuralNetTorch_r30_BAG_L1_PSEUDO': 0.05, 'LightGBM_r130_BAG_L1_PSEUDO': 0.05}\n",
      "\t0.941\t = Validation score   (accuracy)\n",
      "\t10.97s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting 108 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2_PSEUDO ... Training model for up to 5986.66s of the 5986.45s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.72%)\n",
      "\t0.9407\t = Validation score   (accuracy)\n",
      "\t50.33s\t = Training   runtime\n",
      "\t0.36s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2_PSEUDO ... Training model for up to 5933.72s of the 5933.51s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.73%)\n",
      "\t0.9406\t = Validation score   (accuracy)\n",
      "\t51.06s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L2_PSEUDO ... Training model for up to 5880.08s of the 5879.86s of remaining time.\n",
      "\t0.94\t = Validation score   (accuracy)\n",
      "\t111.1s\t = Training   runtime\n",
      "\t7.02s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L2_PSEUDO ... Training model for up to 5761.29s of the 5761.07s of remaining time.\n",
      "\t0.9402\t = Validation score   (accuracy)\n",
      "\t106.27s\t = Training   runtime\n",
      "\t7.2s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2_PSEUDO ... Training model for up to 5647.2s of the 5646.99s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.76%)\n",
      "\t0.9411\t = Validation score   (accuracy)\n",
      "\t53.36s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L2_PSEUDO ... Training model for up to 5591.31s of the 5591.09s of remaining time.\n",
      "\t0.9402\t = Validation score   (accuracy)\n",
      "\t23.95s\t = Training   runtime\n",
      "\t8.13s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L2_PSEUDO ... Training model for up to 5558.26s of the 5558.05s of remaining time.\n",
      "\t0.94\t = Validation score   (accuracy)\n",
      "\t25.29s\t = Training   runtime\n",
      "\t7.91s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2_PSEUDO ... Training model for up to 5524.1s of the 5523.89s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=1.17%)\n",
      "\t0.9407\t = Validation score   (accuracy)\n",
      "\t1233.77s\t = Training   runtime\n",
      "\t2.61s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2_PSEUDO ... Training model for up to 4287.32s of the 4287.1s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=1.01%)\n",
      "\t0.9411\t = Validation score   (accuracy)\n",
      "\t36.5s\t = Training   runtime\n",
      "\t0.72s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2_PSEUDO ... Training model for up to 4248.13s of the 4247.91s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.65%)\n",
      "\t0.9399\t = Validation score   (accuracy)\n",
      "\t436.96s\t = Training   runtime\n",
      "\t2.09s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2_PSEUDO ... Training model for up to 3808.32s of the 3808.11s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.86%)\n",
      "\t0.9405\t = Validation score   (accuracy)\n",
      "\t68.16s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L2_PSEUDO ... Training model for up to 3737.59s of the 3737.37s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.77%)\n",
      "\t0.941\t = Validation score   (accuracy)\n",
      "\t54.72s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L2_PSEUDO ... Training model for up to 3680.28s of the 3680.06s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.65%)\n",
      "\t0.9403\t = Validation score   (accuracy)\n",
      "\t572.53s\t = Training   runtime\n",
      "\t2.11s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L2_PSEUDO ... Training model for up to 3104.86s of the 3104.65s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.77%)\n",
      "\t0.8183\t = Validation score   (accuracy)\n",
      "\t41.78s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L2_PSEUDO ... Training model for up to 3060.51s of the 3060.29s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=1.18%)\n",
      "\t0.9408\t = Validation score   (accuracy)\n",
      "\t1446.25s\t = Training   runtime\n",
      "\t2.58s\t = Validation runtime\n",
      "Fitting model: CatBoost_r9_BAG_L2_PSEUDO ... Training model for up to 1611.07s of the 1610.86s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=1.00%)\n",
      "\t0.941\t = Validation score   (accuracy)\n",
      "\t62.12s\t = Training   runtime\n",
      "\t0.3s\t = Validation runtime\n",
      "Fitting model: LightGBM_r96_BAG_L2_PSEUDO ... Training model for up to 1546.13s of the 1545.92s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.72%)\n",
      "\t0.8183\t = Validation score   (accuracy)\n",
      "\t40.82s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L2_PSEUDO ... Training model for up to 1502.59s of the 1502.37s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.65%)\n",
      "\t0.9403\t = Validation score   (accuracy)\n",
      "\t442.72s\t = Training   runtime\n",
      "\t2.03s\t = Validation runtime\n",
      "Fitting model: XGBoost_r33_BAG_L2_PSEUDO ... Training model for up to 1057.09s of the 1056.87s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=1.51%)\n",
      "\t0.8183\t = Validation score   (accuracy)\n",
      "\t36.22s\t = Training   runtime\n",
      "\t0.69s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r42_BAG_L2_PSEUDO ... Training model for up to 1018.25s of the 1018.03s of remaining time.\n",
      "\t0.9398\t = Validation score   (accuracy)\n",
      "\t73.52s\t = Training   runtime\n",
      "\t7.04s\t = Validation runtime\n",
      "Fitting model: CatBoost_r137_BAG_L2_PSEUDO ... Training model for up to 937.01s of the 936.79s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=0.71%)\n",
      "\t0.9408\t = Validation score   (accuracy)\n",
      "\t45.41s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r102_BAG_L2_PSEUDO ... Training model for up to 889.13s of the 888.92s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=1.18%)\n",
      "\t0.9406\t = Validation score   (accuracy)\n",
      "\t223.61s\t = Training   runtime\n",
      "\t0.75s\t = Validation runtime\n",
      "Fitting model: CatBoost_r13_BAG_L2_PSEUDO ... Training model for up to 662.89s of the 662.67s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=1, memory=1.00%)\n",
      "\t0.941\t = Validation score   (accuracy)\n",
      "\t65.22s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: RandomForest_r195_BAG_L2_PSEUDO ... Training model for up to 595.11s of the 594.89s of remaining time.\n",
      "\t0.9394\t = Validation score   (accuracy)\n",
      "\t725.31s\t = Training   runtime\n",
      "\t7.12s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3_PSEUDO ... Training model for up to 598.66s of the -139.69s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost_r9_BAG_L2': 1.0}\n",
      "\t0.9413\t = Validation score   (accuracy)\n",
      "\t10.98s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Deleting model KNeighborsUnif_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/KNeighborsUnif_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model KNeighborsDist_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/KNeighborsDist_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model LightGBMXT_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/LightGBMXT_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model LightGBM_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/LightGBM_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model RandomForestGini_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/RandomForestGini_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model RandomForestEntr_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/RandomForestEntr_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model CatBoost_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/CatBoost_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model ExtraTreesGini_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/ExtraTreesGini_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model ExtraTreesEntr_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/ExtraTreesEntr_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model NeuralNetFastAI_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/NeuralNetFastAI_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model XGBoost_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/XGBoost_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model NeuralNetTorch_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/NeuralNetTorch_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model LightGBMLarge_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/LightGBMLarge_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model CatBoost_r177_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/CatBoost_r177_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model NeuralNetTorch_r79_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/NeuralNetTorch_r79_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model LightGBM_r131_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/LightGBM_r131_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model NeuralNetFastAI_r191_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/NeuralNetFastAI_r191_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model CatBoost_r9_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/CatBoost_r9_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model LightGBM_r96_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/LightGBM_r96_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model NeuralNetTorch_r22_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/NeuralNetTorch_r22_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model XGBoost_r33_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/XGBoost_r33_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model ExtraTrees_r42_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/ExtraTrees_r42_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model CatBoost_r137_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/CatBoost_r137_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model NeuralNetFastAI_r102_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/NeuralNetFastAI_r102_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model CatBoost_r13_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/CatBoost_r13_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model RandomForest_r195_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/RandomForest_r195_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model LightGBM_r188_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/LightGBM_r188_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model NeuralNetFastAI_r145_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/NeuralNetFastAI_r145_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model XGBoost_r89_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/XGBoost_r89_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model NeuralNetTorch_r30_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/NeuralNetTorch_r30_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model LightGBM_r130_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/LightGBM_r130_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model NeuralNetTorch_r86_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/NeuralNetTorch_r86_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model CatBoost_r50_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/CatBoost_r50_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model NeuralNetFastAI_r11_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/NeuralNetFastAI_r11_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model XGBoost_r194_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/XGBoost_r194_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model ExtraTrees_r172_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/ExtraTrees_r172_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model CatBoost_r69_BAG_L1_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/CatBoost_r69_BAG_L1_PSEUDO will be removed.\n",
      "Deleting model WeightedEnsemble_L2_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/WeightedEnsemble_L2_PSEUDO will be removed.\n",
      "Deleting model LightGBMXT_BAG_L2_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/LightGBMXT_BAG_L2_PSEUDO will be removed.\n",
      "Deleting model LightGBM_BAG_L2_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/LightGBM_BAG_L2_PSEUDO will be removed.\n",
      "Deleting model RandomForestGini_BAG_L2_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/RandomForestGini_BAG_L2_PSEUDO will be removed.\n",
      "Deleting model RandomForestEntr_BAG_L2_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/RandomForestEntr_BAG_L2_PSEUDO will be removed.\n",
      "Deleting model CatBoost_BAG_L2_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/CatBoost_BAG_L2_PSEUDO will be removed.\n",
      "Deleting model ExtraTreesGini_BAG_L2_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/ExtraTreesGini_BAG_L2_PSEUDO will be removed.\n",
      "Deleting model ExtraTreesEntr_BAG_L2_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/ExtraTreesEntr_BAG_L2_PSEUDO will be removed.\n",
      "Deleting model NeuralNetFastAI_BAG_L2_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/NeuralNetFastAI_BAG_L2_PSEUDO will be removed.\n",
      "Deleting model XGBoost_BAG_L2_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/XGBoost_BAG_L2_PSEUDO will be removed.\n",
      "Deleting model NeuralNetTorch_BAG_L2_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/NeuralNetTorch_BAG_L2_PSEUDO will be removed.\n",
      "Deleting model LightGBMLarge_BAG_L2_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/LightGBMLarge_BAG_L2_PSEUDO will be removed.\n",
      "Deleting model CatBoost_r177_BAG_L2_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/CatBoost_r177_BAG_L2_PSEUDO will be removed.\n",
      "Deleting model NeuralNetTorch_r79_BAG_L2_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/NeuralNetTorch_r79_BAG_L2_PSEUDO will be removed.\n",
      "Deleting model LightGBM_r131_BAG_L2_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/LightGBM_r131_BAG_L2_PSEUDO will be removed.\n",
      "Deleting model NeuralNetFastAI_r191_BAG_L2_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/NeuralNetFastAI_r191_BAG_L2_PSEUDO will be removed.\n",
      "Deleting model CatBoost_r9_BAG_L2_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/CatBoost_r9_BAG_L2_PSEUDO will be removed.\n",
      "Deleting model LightGBM_r96_BAG_L2_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/LightGBM_r96_BAG_L2_PSEUDO will be removed.\n",
      "Deleting model NeuralNetTorch_r22_BAG_L2_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/NeuralNetTorch_r22_BAG_L2_PSEUDO will be removed.\n",
      "Deleting model XGBoost_r33_BAG_L2_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/XGBoost_r33_BAG_L2_PSEUDO will be removed.\n",
      "Deleting model ExtraTrees_r42_BAG_L2_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/ExtraTrees_r42_BAG_L2_PSEUDO will be removed.\n",
      "Deleting model CatBoost_r137_BAG_L2_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/CatBoost_r137_BAG_L2_PSEUDO will be removed.\n",
      "Deleting model NeuralNetFastAI_r102_BAG_L2_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/NeuralNetFastAI_r102_BAG_L2_PSEUDO will be removed.\n",
      "Deleting model CatBoost_r13_BAG_L2_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/CatBoost_r13_BAG_L2_PSEUDO will be removed.\n",
      "Deleting model RandomForest_r195_BAG_L2_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/RandomForest_r195_BAG_L2_PSEUDO will be removed.\n",
      "Deleting model WeightedEnsemble_L3_PSEUDO. All files under AutogluonModels/ag-20241102_083553/models/WeightedEnsemble_L3_PSEUDO will be removed.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241102_083553\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogluon.tabular.predictor.predictor.TabularPredictor at 0x7e9fa48f3a90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.fit_pseudolabel(\n",
    "    train_data=train,\n",
    "    pseudo_data=original,\n",
    "    time_limit=CFG.time_limit,\n",
    "    keep_only_best=True,\n",
    "    presets='best_quality',\n",
    "    ag_args_fit={\n",
    "        'num_gpus': 1, \n",
    "        'num_cpus': 4\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "455ff13c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T18:38:54.539130Z",
     "iopub.status.busy": "2024-11-02T18:38:54.537580Z",
     "iopub.status.idle": "2024-11-02T18:38:54.618776Z",
     "shell.execute_reply": "2024-11-02T18:38:54.617875Z"
    },
    "papermill": {
     "duration": 0.163568,
     "end_time": "2024-11-02T18:38:54.621017",
     "exception": false,
     "start_time": "2024-11-02T18:38:54.457449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a31e2_row0_col1, #T_a31e2_row1_col1 {\n",
       "  background-color: #006837;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a31e2_row2_col1, #T_a31e2_row3_col1, #T_a31e2_row4_col1, #T_a31e2_row5_col1 {\n",
       "  background-color: #026c39;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a31e2_row6_col1, #T_a31e2_row7_col1, #T_a31e2_row8_col1, #T_a31e2_row9_col1, #T_a31e2_row10_col1, #T_a31e2_row11_col1, #T_a31e2_row12_col1 {\n",
       "  background-color: #036e3a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a31e2_row13_col1, #T_a31e2_row14_col1, #T_a31e2_row15_col1, #T_a31e2_row16_col1 {\n",
       "  background-color: #04703b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a31e2_row17_col1, #T_a31e2_row18_col1, #T_a31e2_row19_col1, #T_a31e2_row20_col1 {\n",
       "  background-color: #05713c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a31e2_row21_col1 {\n",
       "  background-color: #06733d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a31e2_row22_col1 {\n",
       "  background-color: #08773f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a31e2_row23_col1 {\n",
       "  background-color: #097940;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a31e2_row24_col1, #T_a31e2_row25_col1 {\n",
       "  background-color: #0b7d42;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a31e2_row26_col1 {\n",
       "  background-color: #0c7f43;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a31e2_row27_col1 {\n",
       "  background-color: #0d8044;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a31e2_row28_col1 {\n",
       "  background-color: #89cc67;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a31e2_row29_col1 {\n",
       "  background-color: #abdb6d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a31e2_row30_col1, #T_a31e2_row31_col1, #T_a31e2_row32_col1 {\n",
       "  background-color: #a50026;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a31e2\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a31e2_level0_col0\" class=\"col_heading level0 col0\" >model</th>\n",
       "      <th id=\"T_a31e2_level0_col1\" class=\"col_heading level0 col1\" >score_val</th>\n",
       "      <th id=\"T_a31e2_level0_col2\" class=\"col_heading level0 col2\" >eval_metric</th>\n",
       "      <th id=\"T_a31e2_level0_col3\" class=\"col_heading level0 col3\" >pred_time_val</th>\n",
       "      <th id=\"T_a31e2_level0_col4\" class=\"col_heading level0 col4\" >fit_time</th>\n",
       "      <th id=\"T_a31e2_level0_col5\" class=\"col_heading level0 col5\" >pred_time_val_marginal</th>\n",
       "      <th id=\"T_a31e2_level0_col6\" class=\"col_heading level0 col6\" >fit_time_marginal</th>\n",
       "      <th id=\"T_a31e2_level0_col7\" class=\"col_heading level0 col7\" >stack_level</th>\n",
       "      <th id=\"T_a31e2_level0_col8\" class=\"col_heading level0 col8\" >can_infer</th>\n",
       "      <th id=\"T_a31e2_level0_col9\" class=\"col_heading level0 col9\" >fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_a31e2_row0_col0\" class=\"data row0 col0\" >CatBoost_r9_BAG_L2</td>\n",
       "      <td id=\"T_a31e2_row0_col1\" class=\"data row0 col1\" >0.941336</td>\n",
       "      <td id=\"T_a31e2_row0_col2\" class=\"data row0 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row0_col3\" class=\"data row0 col3\" >59.081315</td>\n",
       "      <td id=\"T_a31e2_row0_col4\" class=\"data row0 col4\" >8913.083765</td>\n",
       "      <td id=\"T_a31e2_row0_col5\" class=\"data row0 col5\" >0.272509</td>\n",
       "      <td id=\"T_a31e2_row0_col6\" class=\"data row0 col6\" >60.309260</td>\n",
       "      <td id=\"T_a31e2_row0_col7\" class=\"data row0 col7\" >2</td>\n",
       "      <td id=\"T_a31e2_row0_col8\" class=\"data row0 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row0_col9\" class=\"data row0 col9\" >32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_a31e2_row1_col0\" class=\"data row1 col0\" >WeightedEnsemble_L3</td>\n",
       "      <td id=\"T_a31e2_row1_col1\" class=\"data row1 col1\" >0.941336</td>\n",
       "      <td id=\"T_a31e2_row1_col2\" class=\"data row1 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row1_col3\" class=\"data row1 col3\" >59.110605</td>\n",
       "      <td id=\"T_a31e2_row1_col4\" class=\"data row1 col4\" >8925.838868</td>\n",
       "      <td id=\"T_a31e2_row1_col5\" class=\"data row1 col5\" >0.029291</td>\n",
       "      <td id=\"T_a31e2_row1_col6\" class=\"data row1 col6\" >12.755102</td>\n",
       "      <td id=\"T_a31e2_row1_col7\" class=\"data row1 col7\" >3</td>\n",
       "      <td id=\"T_a31e2_row1_col8\" class=\"data row1 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row1_col9\" class=\"data row1 col9\" >33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_a31e2_row2_col0\" class=\"data row2 col0\" >LightGBM_r188_BAG_L1</td>\n",
       "      <td id=\"T_a31e2_row2_col1\" class=\"data row2 col1\" >0.940299</td>\n",
       "      <td id=\"T_a31e2_row2_col2\" class=\"data row2 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row2_col3\" class=\"data row2 col3\" >1.469552</td>\n",
       "      <td id=\"T_a31e2_row2_col4\" class=\"data row2 col4\" >95.139189</td>\n",
       "      <td id=\"T_a31e2_row2_col5\" class=\"data row2 col5\" >1.469552</td>\n",
       "      <td id=\"T_a31e2_row2_col6\" class=\"data row2 col6\" >95.139189</td>\n",
       "      <td id=\"T_a31e2_row2_col7\" class=\"data row2 col7\" >1</td>\n",
       "      <td id=\"T_a31e2_row2_col8\" class=\"data row2 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row2_col9\" class=\"data row2 col9\" >27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_a31e2_row3_col0\" class=\"data row3 col0\" >NeuralNetFastAI_r191_BAG_L1</td>\n",
       "      <td id=\"T_a31e2_row3_col1\" class=\"data row3 col1\" >0.940100</td>\n",
       "      <td id=\"T_a31e2_row3_col2\" class=\"data row3 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row3_col3\" class=\"data row3 col3\" >2.758621</td>\n",
       "      <td id=\"T_a31e2_row3_col4\" class=\"data row3 col4\" >1981.956502</td>\n",
       "      <td id=\"T_a31e2_row3_col5\" class=\"data row3 col5\" >2.758621</td>\n",
       "      <td id=\"T_a31e2_row3_col6\" class=\"data row3 col6\" >1981.956502</td>\n",
       "      <td id=\"T_a31e2_row3_col7\" class=\"data row3 col7\" >1</td>\n",
       "      <td id=\"T_a31e2_row3_col8\" class=\"data row3 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row3_col9\" class=\"data row3 col9\" >17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_a31e2_row4_col0\" class=\"data row4 col0\" >XGBoost_r89_BAG_L1</td>\n",
       "      <td id=\"T_a31e2_row4_col1\" class=\"data row4 col1\" >0.939979</td>\n",
       "      <td id=\"T_a31e2_row4_col2\" class=\"data row4 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row4_col3\" class=\"data row4 col3\" >0.567835</td>\n",
       "      <td id=\"T_a31e2_row4_col4\" class=\"data row4 col4\" >34.075950</td>\n",
       "      <td id=\"T_a31e2_row4_col5\" class=\"data row4 col5\" >0.567835</td>\n",
       "      <td id=\"T_a31e2_row4_col6\" class=\"data row4 col6\" >34.075950</td>\n",
       "      <td id=\"T_a31e2_row4_col7\" class=\"data row4 col7\" >1</td>\n",
       "      <td id=\"T_a31e2_row4_col8\" class=\"data row4 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row4_col9\" class=\"data row4 col9\" >29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_a31e2_row5_col0\" class=\"data row5 col0\" >NeuralNetFastAI_r145_BAG_L1</td>\n",
       "      <td id=\"T_a31e2_row5_col1\" class=\"data row5 col1\" >0.939950</td>\n",
       "      <td id=\"T_a31e2_row5_col2\" class=\"data row5 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row5_col3\" class=\"data row5 col3\" >5.319819</td>\n",
       "      <td id=\"T_a31e2_row5_col4\" class=\"data row5 col4\" >2147.939373</td>\n",
       "      <td id=\"T_a31e2_row5_col5\" class=\"data row5 col5\" >5.319819</td>\n",
       "      <td id=\"T_a31e2_row5_col6\" class=\"data row5 col6\" >2147.939373</td>\n",
       "      <td id=\"T_a31e2_row5_col7\" class=\"data row5 col7\" >1</td>\n",
       "      <td id=\"T_a31e2_row5_col8\" class=\"data row5 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row5_col9\" class=\"data row5 col9\" >28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_a31e2_row6_col0\" class=\"data row6 col0\" >CatBoost_r177_BAG_L1</td>\n",
       "      <td id=\"T_a31e2_row6_col1\" class=\"data row6 col1\" >0.939773</td>\n",
       "      <td id=\"T_a31e2_row6_col2\" class=\"data row6 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row6_col3\" class=\"data row6 col3\" >0.263458</td>\n",
       "      <td id=\"T_a31e2_row6_col4\" class=\"data row6 col4\" >82.324453</td>\n",
       "      <td id=\"T_a31e2_row6_col5\" class=\"data row6 col5\" >0.263458</td>\n",
       "      <td id=\"T_a31e2_row6_col6\" class=\"data row6 col6\" >82.324453</td>\n",
       "      <td id=\"T_a31e2_row6_col7\" class=\"data row6 col7\" >1</td>\n",
       "      <td id=\"T_a31e2_row6_col8\" class=\"data row6 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row6_col9\" class=\"data row6 col9\" >14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_a31e2_row7_col0\" class=\"data row7 col0\" >NeuralNetTorch_r30_BAG_L1</td>\n",
       "      <td id=\"T_a31e2_row7_col1\" class=\"data row7 col1\" >0.939687</td>\n",
       "      <td id=\"T_a31e2_row7_col2\" class=\"data row7 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row7_col3\" class=\"data row7 col3\" >0.908180</td>\n",
       "      <td id=\"T_a31e2_row7_col4\" class=\"data row7 col4\" >406.101564</td>\n",
       "      <td id=\"T_a31e2_row7_col5\" class=\"data row7 col5\" >0.908180</td>\n",
       "      <td id=\"T_a31e2_row7_col6\" class=\"data row7 col6\" >406.101564</td>\n",
       "      <td id=\"T_a31e2_row7_col7\" class=\"data row7 col7\" >1</td>\n",
       "      <td id=\"T_a31e2_row7_col8\" class=\"data row7 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row7_col9\" class=\"data row7 col9\" >30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_a31e2_row8_col0\" class=\"data row8 col0\" >LightGBMXT_BAG_L1</td>\n",
       "      <td id=\"T_a31e2_row8_col1\" class=\"data row8 col1\" >0.939680</td>\n",
       "      <td id=\"T_a31e2_row8_col2\" class=\"data row8 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row8_col3\" class=\"data row8 col3\" >0.846736</td>\n",
       "      <td id=\"T_a31e2_row8_col4\" class=\"data row8 col4\" >60.586237</td>\n",
       "      <td id=\"T_a31e2_row8_col5\" class=\"data row8 col5\" >0.846736</td>\n",
       "      <td id=\"T_a31e2_row8_col6\" class=\"data row8 col6\" >60.586237</td>\n",
       "      <td id=\"T_a31e2_row8_col7\" class=\"data row8 col7\" >1</td>\n",
       "      <td id=\"T_a31e2_row8_col8\" class=\"data row8 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row8_col9\" class=\"data row8 col9\" >3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_a31e2_row9_col0\" class=\"data row9 col0\" >LightGBM_r130_BAG_L1</td>\n",
       "      <td id=\"T_a31e2_row9_col1\" class=\"data row9 col1\" >0.939680</td>\n",
       "      <td id=\"T_a31e2_row9_col2\" class=\"data row9 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row9_col3\" class=\"data row9 col3\" >1.087503</td>\n",
       "      <td id=\"T_a31e2_row9_col4\" class=\"data row9 col4\" >78.587364</td>\n",
       "      <td id=\"T_a31e2_row9_col5\" class=\"data row9 col5\" >1.087503</td>\n",
       "      <td id=\"T_a31e2_row9_col6\" class=\"data row9 col6\" >78.587364</td>\n",
       "      <td id=\"T_a31e2_row9_col7\" class=\"data row9 col7\" >1</td>\n",
       "      <td id=\"T_a31e2_row9_col8\" class=\"data row9 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row9_col9\" class=\"data row9 col9\" >31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_a31e2_row10_col0\" class=\"data row10 col0\" >XGBoost_BAG_L1</td>\n",
       "      <td id=\"T_a31e2_row10_col1\" class=\"data row10 col1\" >0.939574</td>\n",
       "      <td id=\"T_a31e2_row10_col2\" class=\"data row10 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row10_col3\" class=\"data row10 col3\" >0.543766</td>\n",
       "      <td id=\"T_a31e2_row10_col4\" class=\"data row10 col4\" >33.385642</td>\n",
       "      <td id=\"T_a31e2_row10_col5\" class=\"data row10 col5\" >0.543766</td>\n",
       "      <td id=\"T_a31e2_row10_col6\" class=\"data row10 col6\" >33.385642</td>\n",
       "      <td id=\"T_a31e2_row10_col7\" class=\"data row10 col7\" >1</td>\n",
       "      <td id=\"T_a31e2_row10_col8\" class=\"data row10 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row10_col9\" class=\"data row10 col9\" >11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_a31e2_row11_col0\" class=\"data row11 col0\" >CatBoost_r9_BAG_L1</td>\n",
       "      <td id=\"T_a31e2_row11_col1\" class=\"data row11 col1\" >0.939538</td>\n",
       "      <td id=\"T_a31e2_row11_col2\" class=\"data row11 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row11_col3\" class=\"data row11 col3\" >0.569602</td>\n",
       "      <td id=\"T_a31e2_row11_col4\" class=\"data row11 col4\" >106.997489</td>\n",
       "      <td id=\"T_a31e2_row11_col5\" class=\"data row11 col5\" >0.569602</td>\n",
       "      <td id=\"T_a31e2_row11_col6\" class=\"data row11 col6\" >106.997489</td>\n",
       "      <td id=\"T_a31e2_row11_col7\" class=\"data row11 col7\" >1</td>\n",
       "      <td id=\"T_a31e2_row11_col8\" class=\"data row11 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row11_col9\" class=\"data row11 col9\" >18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_a31e2_row12_col0\" class=\"data row12 col0\" >CatBoost_BAG_L1</td>\n",
       "      <td id=\"T_a31e2_row12_col1\" class=\"data row12 col1\" >0.939417</td>\n",
       "      <td id=\"T_a31e2_row12_col2\" class=\"data row12 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row12_col3\" class=\"data row12 col3\" >0.266048</td>\n",
       "      <td id=\"T_a31e2_row12_col4\" class=\"data row12 col4\" >87.858100</td>\n",
       "      <td id=\"T_a31e2_row12_col5\" class=\"data row12 col5\" >0.266048</td>\n",
       "      <td id=\"T_a31e2_row12_col6\" class=\"data row12 col6\" >87.858100</td>\n",
       "      <td id=\"T_a31e2_row12_col7\" class=\"data row12 col7\" >1</td>\n",
       "      <td id=\"T_a31e2_row12_col8\" class=\"data row12 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row12_col9\" class=\"data row12 col9\" >7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_a31e2_row13_col0\" class=\"data row13 col0\" >NeuralNetTorch_r79_BAG_L1</td>\n",
       "      <td id=\"T_a31e2_row13_col1\" class=\"data row13 col1\" >0.939147</td>\n",
       "      <td id=\"T_a31e2_row13_col2\" class=\"data row13 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row13_col3\" class=\"data row13 col3\" >0.803480</td>\n",
       "      <td id=\"T_a31e2_row13_col4\" class=\"data row13 col4\" >574.294412</td>\n",
       "      <td id=\"T_a31e2_row13_col5\" class=\"data row13 col5\" >0.803480</td>\n",
       "      <td id=\"T_a31e2_row13_col6\" class=\"data row13 col6\" >574.294412</td>\n",
       "      <td id=\"T_a31e2_row13_col7\" class=\"data row13 col7\" >1</td>\n",
       "      <td id=\"T_a31e2_row13_col8\" class=\"data row13 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row13_col9\" class=\"data row13 col9\" >15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_a31e2_row14_col0\" class=\"data row14 col0\" >LightGBM_BAG_L1</td>\n",
       "      <td id=\"T_a31e2_row14_col1\" class=\"data row14 col1\" >0.939076</td>\n",
       "      <td id=\"T_a31e2_row14_col2\" class=\"data row14 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row14_col3\" class=\"data row14 col3\" >0.873712</td>\n",
       "      <td id=\"T_a31e2_row14_col4\" class=\"data row14 col4\" >62.290979</td>\n",
       "      <td id=\"T_a31e2_row14_col5\" class=\"data row14 col5\" >0.873712</td>\n",
       "      <td id=\"T_a31e2_row14_col6\" class=\"data row14 col6\" >62.290979</td>\n",
       "      <td id=\"T_a31e2_row14_col7\" class=\"data row14 col7\" >1</td>\n",
       "      <td id=\"T_a31e2_row14_col8\" class=\"data row14 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row14_col9\" class=\"data row14 col9\" >4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_a31e2_row15_col0\" class=\"data row15 col0\" >CatBoost_r13_BAG_L1</td>\n",
       "      <td id=\"T_a31e2_row15_col1\" class=\"data row15 col1\" >0.939055</td>\n",
       "      <td id=\"T_a31e2_row15_col2\" class=\"data row15 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row15_col3\" class=\"data row15 col3\" >0.298672</td>\n",
       "      <td id=\"T_a31e2_row15_col4\" class=\"data row15 col4\" >162.506758</td>\n",
       "      <td id=\"T_a31e2_row15_col5\" class=\"data row15 col5\" >0.298672</td>\n",
       "      <td id=\"T_a31e2_row15_col6\" class=\"data row15 col6\" >162.506758</td>\n",
       "      <td id=\"T_a31e2_row15_col7\" class=\"data row15 col7\" >1</td>\n",
       "      <td id=\"T_a31e2_row15_col8\" class=\"data row15 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row15_col9\" class=\"data row15 col9\" >25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_a31e2_row16_col0\" class=\"data row16 col0\" >NeuralNetFastAI_BAG_L1</td>\n",
       "      <td id=\"T_a31e2_row16_col1\" class=\"data row16 col1\" >0.938998</td>\n",
       "      <td id=\"T_a31e2_row16_col2\" class=\"data row16 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row16_col3\" class=\"data row16 col3\" >2.712328</td>\n",
       "      <td id=\"T_a31e2_row16_col4\" class=\"data row16 col4\" >1289.360516</td>\n",
       "      <td id=\"T_a31e2_row16_col5\" class=\"data row16 col5\" >2.712328</td>\n",
       "      <td id=\"T_a31e2_row16_col6\" class=\"data row16 col6\" >1289.360516</td>\n",
       "      <td id=\"T_a31e2_row16_col7\" class=\"data row16 col7\" >1</td>\n",
       "      <td id=\"T_a31e2_row16_col8\" class=\"data row16 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row16_col9\" class=\"data row16 col9\" >10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_a31e2_row17_col0\" class=\"data row17 col0\" >NeuralNetTorch_r22_BAG_L1</td>\n",
       "      <td id=\"T_a31e2_row17_col1\" class=\"data row17 col1\" >0.938920</td>\n",
       "      <td id=\"T_a31e2_row17_col2\" class=\"data row17 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row17_col3\" class=\"data row17 col3\" >0.737733</td>\n",
       "      <td id=\"T_a31e2_row17_col4\" class=\"data row17 col4\" >536.255293</td>\n",
       "      <td id=\"T_a31e2_row17_col5\" class=\"data row17 col5\" >0.737733</td>\n",
       "      <td id=\"T_a31e2_row17_col6\" class=\"data row17 col6\" >536.255293</td>\n",
       "      <td id=\"T_a31e2_row17_col7\" class=\"data row17 col7\" >1</td>\n",
       "      <td id=\"T_a31e2_row17_col8\" class=\"data row17 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row17_col9\" class=\"data row17 col9\" >20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_a31e2_row18_col0\" class=\"data row18 col0\" >CatBoost_r137_BAG_L1</td>\n",
       "      <td id=\"T_a31e2_row18_col1\" class=\"data row18 col1\" >0.938877</td>\n",
       "      <td id=\"T_a31e2_row18_col2\" class=\"data row18 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row18_col3\" class=\"data row18 col3\" >0.182074</td>\n",
       "      <td id=\"T_a31e2_row18_col4\" class=\"data row18 col4\" >69.907603</td>\n",
       "      <td id=\"T_a31e2_row18_col5\" class=\"data row18 col5\" >0.182074</td>\n",
       "      <td id=\"T_a31e2_row18_col6\" class=\"data row18 col6\" >69.907603</td>\n",
       "      <td id=\"T_a31e2_row18_col7\" class=\"data row18 col7\" >1</td>\n",
       "      <td id=\"T_a31e2_row18_col8\" class=\"data row18 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row18_col9\" class=\"data row18 col9\" >23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_a31e2_row19_col0\" class=\"data row19 col0\" >LightGBMLarge_BAG_L1</td>\n",
       "      <td id=\"T_a31e2_row19_col1\" class=\"data row19 col1\" >0.938849</td>\n",
       "      <td id=\"T_a31e2_row19_col2\" class=\"data row19 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row19_col3\" class=\"data row19 col3\" >1.099534</td>\n",
       "      <td id=\"T_a31e2_row19_col4\" class=\"data row19 col4\" >85.373052</td>\n",
       "      <td id=\"T_a31e2_row19_col5\" class=\"data row19 col5\" >1.099534</td>\n",
       "      <td id=\"T_a31e2_row19_col6\" class=\"data row19 col6\" >85.373052</td>\n",
       "      <td id=\"T_a31e2_row19_col7\" class=\"data row19 col7\" >1</td>\n",
       "      <td id=\"T_a31e2_row19_col8\" class=\"data row19 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row19_col9\" class=\"data row19 col9\" >13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_a31e2_row20_col0\" class=\"data row20 col0\" >NeuralNetFastAI_r102_BAG_L1</td>\n",
       "      <td id=\"T_a31e2_row20_col1\" class=\"data row20 col1\" >0.938522</td>\n",
       "      <td id=\"T_a31e2_row20_col2\" class=\"data row20 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row20_col3\" class=\"data row20 col3\" >0.712495</td>\n",
       "      <td id=\"T_a31e2_row20_col4\" class=\"data row20 col4\" >241.361123</td>\n",
       "      <td id=\"T_a31e2_row20_col5\" class=\"data row20 col5\" >0.712495</td>\n",
       "      <td id=\"T_a31e2_row20_col6\" class=\"data row20 col6\" >241.361123</td>\n",
       "      <td id=\"T_a31e2_row20_col7\" class=\"data row20 col7\" >1</td>\n",
       "      <td id=\"T_a31e2_row20_col8\" class=\"data row20 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row20_col9\" class=\"data row20 col9\" >24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_a31e2_row21_col0\" class=\"data row21 col0\" >NeuralNetTorch_BAG_L1</td>\n",
       "      <td id=\"T_a31e2_row21_col1\" class=\"data row21 col1\" >0.938244</td>\n",
       "      <td id=\"T_a31e2_row21_col2\" class=\"data row21 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row21_col3\" class=\"data row21 col3\" >0.721094</td>\n",
       "      <td id=\"T_a31e2_row21_col4\" class=\"data row21 col4\" >436.920664</td>\n",
       "      <td id=\"T_a31e2_row21_col5\" class=\"data row21 col5\" >0.721094</td>\n",
       "      <td id=\"T_a31e2_row21_col6\" class=\"data row21 col6\" >436.920664</td>\n",
       "      <td id=\"T_a31e2_row21_col7\" class=\"data row21 col7\" >1</td>\n",
       "      <td id=\"T_a31e2_row21_col8\" class=\"data row21 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row21_col9\" class=\"data row21 col9\" >12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_a31e2_row22_col0\" class=\"data row22 col0\" >RandomForestGini_BAG_L1</td>\n",
       "      <td id=\"T_a31e2_row22_col1\" class=\"data row22 col1\" >0.937178</td>\n",
       "      <td id=\"T_a31e2_row22_col2\" class=\"data row22 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row22_col3\" class=\"data row22 col3\" >5.330663</td>\n",
       "      <td id=\"T_a31e2_row22_col4\" class=\"data row22 col4\" >24.891028</td>\n",
       "      <td id=\"T_a31e2_row22_col5\" class=\"data row22 col5\" >5.330663</td>\n",
       "      <td id=\"T_a31e2_row22_col6\" class=\"data row22 col6\" >24.891028</td>\n",
       "      <td id=\"T_a31e2_row22_col7\" class=\"data row22 col7\" >1</td>\n",
       "      <td id=\"T_a31e2_row22_col8\" class=\"data row22 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row22_col9\" class=\"data row22 col9\" >5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_a31e2_row23_col0\" class=\"data row23 col0\" >RandomForestEntr_BAG_L1</td>\n",
       "      <td id=\"T_a31e2_row23_col1\" class=\"data row23 col1\" >0.936795</td>\n",
       "      <td id=\"T_a31e2_row23_col2\" class=\"data row23 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row23_col3\" class=\"data row23 col3\" >5.233160</td>\n",
       "      <td id=\"T_a31e2_row23_col4\" class=\"data row23 col4\" >24.198915</td>\n",
       "      <td id=\"T_a31e2_row23_col5\" class=\"data row23 col5\" >5.233160</td>\n",
       "      <td id=\"T_a31e2_row23_col6\" class=\"data row23 col6\" >24.198915</td>\n",
       "      <td id=\"T_a31e2_row23_col7\" class=\"data row23 col7\" >1</td>\n",
       "      <td id=\"T_a31e2_row23_col8\" class=\"data row23 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row23_col9\" class=\"data row23 col9\" >6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "      <td id=\"T_a31e2_row24_col0\" class=\"data row24 col0\" >ExtraTreesEntr_BAG_L1</td>\n",
       "      <td id=\"T_a31e2_row24_col1\" class=\"data row24 col1\" >0.936034</td>\n",
       "      <td id=\"T_a31e2_row24_col2\" class=\"data row24 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row24_col3\" class=\"data row24 col3\" >5.621628</td>\n",
       "      <td id=\"T_a31e2_row24_col4\" class=\"data row24 col4\" >17.685741</td>\n",
       "      <td id=\"T_a31e2_row24_col5\" class=\"data row24 col5\" >5.621628</td>\n",
       "      <td id=\"T_a31e2_row24_col6\" class=\"data row24 col6\" >17.685741</td>\n",
       "      <td id=\"T_a31e2_row24_col7\" class=\"data row24 col7\" >1</td>\n",
       "      <td id=\"T_a31e2_row24_col8\" class=\"data row24 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row24_col9\" class=\"data row24 col9\" >9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "      <td id=\"T_a31e2_row25_col0\" class=\"data row25 col0\" >ExtraTreesGini_BAG_L1</td>\n",
       "      <td id=\"T_a31e2_row25_col1\" class=\"data row25 col1\" >0.935970</td>\n",
       "      <td id=\"T_a31e2_row25_col2\" class=\"data row25 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row25_col3\" class=\"data row25 col3\" >5.717815</td>\n",
       "      <td id=\"T_a31e2_row25_col4\" class=\"data row25 col4\" >16.960611</td>\n",
       "      <td id=\"T_a31e2_row25_col5\" class=\"data row25 col5\" >5.717815</td>\n",
       "      <td id=\"T_a31e2_row25_col6\" class=\"data row25 col6\" >16.960611</td>\n",
       "      <td id=\"T_a31e2_row25_col7\" class=\"data row25 col7\" >1</td>\n",
       "      <td id=\"T_a31e2_row25_col8\" class=\"data row25 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row25_col9\" class=\"data row25 col9\" >8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "      <td id=\"T_a31e2_row26_col0\" class=\"data row26 col0\" >ExtraTrees_r42_BAG_L1</td>\n",
       "      <td id=\"T_a31e2_row26_col1\" class=\"data row26 col1\" >0.935210</td>\n",
       "      <td id=\"T_a31e2_row26_col2\" class=\"data row26 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row26_col3\" class=\"data row26 col3\" >5.371206</td>\n",
       "      <td id=\"T_a31e2_row26_col4\" class=\"data row26 col4\" >28.402528</td>\n",
       "      <td id=\"T_a31e2_row26_col5\" class=\"data row26 col5\" >5.371206</td>\n",
       "      <td id=\"T_a31e2_row26_col6\" class=\"data row26 col6\" >28.402528</td>\n",
       "      <td id=\"T_a31e2_row26_col7\" class=\"data row26 col7\" >1</td>\n",
       "      <td id=\"T_a31e2_row26_col8\" class=\"data row26 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row26_col9\" class=\"data row26 col9\" >22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "      <td id=\"T_a31e2_row27_col0\" class=\"data row27 col0\" >RandomForest_r195_BAG_L1</td>\n",
       "      <td id=\"T_a31e2_row27_col1\" class=\"data row27 col1\" >0.935053</td>\n",
       "      <td id=\"T_a31e2_row27_col2\" class=\"data row27 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row27_col3\" class=\"data row27 col3\" >4.972858</td>\n",
       "      <td id=\"T_a31e2_row27_col4\" class=\"data row27 col4\" >58.362750</td>\n",
       "      <td id=\"T_a31e2_row27_col5\" class=\"data row27 col5\" >4.972858</td>\n",
       "      <td id=\"T_a31e2_row27_col6\" class=\"data row27 col6\" >58.362750</td>\n",
       "      <td id=\"T_a31e2_row27_col7\" class=\"data row27 col7\" >1</td>\n",
       "      <td id=\"T_a31e2_row27_col8\" class=\"data row27 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row27_col9\" class=\"data row27 col9\" >26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "      <td id=\"T_a31e2_row28_col0\" class=\"data row28 col0\" >KNeighborsUnif_BAG_L1</td>\n",
       "      <td id=\"T_a31e2_row28_col1\" class=\"data row28 col1\" >0.909794</td>\n",
       "      <td id=\"T_a31e2_row28_col2\" class=\"data row28 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row28_col3\" class=\"data row28 col3\" >1.424436</td>\n",
       "      <td id=\"T_a31e2_row28_col4\" class=\"data row28 col4\" >0.469615</td>\n",
       "      <td id=\"T_a31e2_row28_col5\" class=\"data row28 col5\" >1.424436</td>\n",
       "      <td id=\"T_a31e2_row28_col6\" class=\"data row28 col6\" >0.469615</td>\n",
       "      <td id=\"T_a31e2_row28_col7\" class=\"data row28 col7\" >1</td>\n",
       "      <td id=\"T_a31e2_row28_col8\" class=\"data row28 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row28_col9\" class=\"data row28 col9\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "      <td id=\"T_a31e2_row29_col0\" class=\"data row29 col0\" >KNeighborsDist_BAG_L1</td>\n",
       "      <td id=\"T_a31e2_row29_col1\" class=\"data row29 col1\" >0.902893</td>\n",
       "      <td id=\"T_a31e2_row29_col2\" class=\"data row29 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row29_col3\" class=\"data row29 col3\" >1.717552</td>\n",
       "      <td id=\"T_a31e2_row29_col4\" class=\"data row29 col4\" >0.291564</td>\n",
       "      <td id=\"T_a31e2_row29_col5\" class=\"data row29 col5\" >1.717552</td>\n",
       "      <td id=\"T_a31e2_row29_col6\" class=\"data row29 col6\" >0.291564</td>\n",
       "      <td id=\"T_a31e2_row29_col7\" class=\"data row29 col7\" >1</td>\n",
       "      <td id=\"T_a31e2_row29_col8\" class=\"data row29 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row29_col9\" class=\"data row29 col9\" >2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
       "      <td id=\"T_a31e2_row30_col0\" class=\"data row30 col0\" >LightGBM_r131_BAG_L1</td>\n",
       "      <td id=\"T_a31e2_row30_col1\" class=\"data row30 col1\" >0.818287</td>\n",
       "      <td id=\"T_a31e2_row30_col2\" class=\"data row30 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row30_col3\" class=\"data row30 col3\" >0.088631</td>\n",
       "      <td id=\"T_a31e2_row30_col4\" class=\"data row30 col4\" >39.524772</td>\n",
       "      <td id=\"T_a31e2_row30_col5\" class=\"data row30 col5\" >0.088631</td>\n",
       "      <td id=\"T_a31e2_row30_col6\" class=\"data row30 col6\" >39.524772</td>\n",
       "      <td id=\"T_a31e2_row30_col7\" class=\"data row30 col7\" >1</td>\n",
       "      <td id=\"T_a31e2_row30_col8\" class=\"data row30 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row30_col9\" class=\"data row30 col9\" >16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
       "      <td id=\"T_a31e2_row31_col0\" class=\"data row31 col0\" >LightGBM_r96_BAG_L1</td>\n",
       "      <td id=\"T_a31e2_row31_col1\" class=\"data row31 col1\" >0.818287</td>\n",
       "      <td id=\"T_a31e2_row31_col2\" class=\"data row31 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row31_col3\" class=\"data row31 col3\" >0.090672</td>\n",
       "      <td id=\"T_a31e2_row31_col4\" class=\"data row31 col4\" >38.028713</td>\n",
       "      <td id=\"T_a31e2_row31_col5\" class=\"data row31 col5\" >0.090672</td>\n",
       "      <td id=\"T_a31e2_row31_col6\" class=\"data row31 col6\" >38.028713</td>\n",
       "      <td id=\"T_a31e2_row31_col7\" class=\"data row31 col7\" >1</td>\n",
       "      <td id=\"T_a31e2_row31_col8\" class=\"data row31 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row31_col9\" class=\"data row31 col9\" >19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a31e2_level0_row32\" class=\"row_heading level0 row32\" >32</th>\n",
       "      <td id=\"T_a31e2_row32_col0\" class=\"data row32 col0\" >XGBoost_r33_BAG_L1</td>\n",
       "      <td id=\"T_a31e2_row32_col1\" class=\"data row32 col1\" >0.818287</td>\n",
       "      <td id=\"T_a31e2_row32_col2\" class=\"data row32 col2\" >accuracy</td>\n",
       "      <td id=\"T_a31e2_row32_col3\" class=\"data row32 col3\" >0.497941</td>\n",
       "      <td id=\"T_a31e2_row32_col4\" class=\"data row32 col4\" >30.736006</td>\n",
       "      <td id=\"T_a31e2_row32_col5\" class=\"data row32 col5\" >0.497941</td>\n",
       "      <td id=\"T_a31e2_row32_col6\" class=\"data row32 col6\" >30.736006</td>\n",
       "      <td id=\"T_a31e2_row32_col7\" class=\"data row32 col7\" >1</td>\n",
       "      <td id=\"T_a31e2_row32_col8\" class=\"data row32 col8\" >True</td>\n",
       "      <td id=\"T_a31e2_row32_col9\" class=\"data row32 col9\" >21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7e9fa48f0cd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.leaderboard(silent=True).style.background_gradient(subset=['score_val'], cmap='RdYlGn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73c1246",
   "metadata": {
    "papermill": {
     "duration": 0.078128,
     "end_time": "2024-11-02T18:38:54.779624",
     "exception": false,
     "start_time": "2024-11-02T18:38:54.701496",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Creating a submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a08cc51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T18:38:54.940530Z",
     "iopub.status.busy": "2024-11-02T18:38:54.939773Z",
     "iopub.status.idle": "2024-11-02T18:41:10.967776Z",
     "shell.execute_reply": "2024-11-02T18:41:10.966690Z"
    },
    "papermill": {
     "duration": 136.188623,
     "end_time": "2024-11-02T18:41:11.048762",
     "exception": false,
     "start_time": "2024-11-02T18:38:54.860139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Depression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140700</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>140701</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>140702</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>140703</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>140704</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93795</th>\n",
       "      <td>234495</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93796</th>\n",
       "      <td>234496</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93797</th>\n",
       "      <td>234497</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93798</th>\n",
       "      <td>234498</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93799</th>\n",
       "      <td>234499</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  Depression\n",
       "0      140700           0\n",
       "1      140701           0\n",
       "2      140702           0\n",
       "3      140703           1\n",
       "4      140704           0\n",
       "...       ...         ...\n",
       "93795  234495           0\n",
       "93796  234496           1\n",
       "93797  234497           0\n",
       "93798  234498           1\n",
       "93799  234499           0\n",
       "\n",
       "[93800 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.read_csv(CFG.sample_sub_path)\n",
    "sub[CFG.target] = predictor.predict(test).values\n",
    "sub.to_csv('sub_autogluon.csv', index=False)\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02bb8ed1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T18:41:11.209244Z",
     "iopub.status.busy": "2024-11-02T18:41:11.208356Z",
     "iopub.status.idle": "2024-11-02T18:41:11.721574Z",
     "shell.execute_reply": "2024-11-02T18:41:11.719932Z"
    },
    "papermill": {
     "duration": 0.59619,
     "end_time": "2024-11-02T18:41:11.723902",
     "exception": false,
     "start_time": "2024-11-02T18:41:11.127712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "shutil.rmtree(\"AutogluonModels\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 10008389,
     "sourceId": 84895,
     "sourceType": "competition"
    },
    {
     "datasetId": 5868381,
     "sourceId": 9616093,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 36375.695824,
   "end_time": "2024-11-02T18:41:17.023628",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-02T08:35:01.327804",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
