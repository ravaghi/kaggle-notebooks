{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86946,"databundleVersionId":10131489,"sourceType":"competition"},{"sourceId":10506357,"sourceType":"datasetVersion","datasetId":6447806},{"sourceId":107993,"sourceType":"modelInstanceVersion","modelInstanceId":90450,"modelId":114668}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":84.82002,"end_time":"2025-01-14T12:44:43.730909","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-01-14T12:43:18.910889","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports and configs","metadata":{"papermill":{"duration":0.003488,"end_time":"2025-01-14T12:43:21.198745","exception":false,"start_time":"2025-01-14T12:43:21.195257","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from transformers import Gemma2ForSequenceClassification, GemmaTokenizerFast\nfrom transformers.data.data_collator import pad_without_fast_tokenizer_warning\nfrom concurrent.futures import ThreadPoolExecutor\nfrom peft import PeftModel\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\nimport torch","metadata":{"papermill":{"duration":15.584189,"end_time":"2025-01-14T12:43:44.409647","exception":false,"start_time":"2025-01-14T12:43:28.825458","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:31:31.557595Z","iopub.execute_input":"2025-01-18T13:31:31.557902Z","iopub.status.idle":"2025-01-18T13:31:43.829844Z","shell.execute_reply.started":"2025-01-18T13:31:31.557882Z","shell.execute_reply":"2025-01-18T13:31:43.829160Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class CFG:\n    test_path = \"/kaggle/input/wsdm-cup-multilingual-chatbot-arena/test.parquet\"\n    \n    gemma_dir = \"/kaggle/input/gemma-2-9b-it-bnb-4bit-unsloth/transformers/default/1\"\n    lora_dir = \"/kaggle/input/wsdm-cup-gemma-2-9b-4-bit-qlora/gemma2-9b-4bit/gemma-2-9b-it-bnb-4bit-3072-8-f0/checkpoint-2422\"\n    \n    max_length = 3072\n    batch_size = 4","metadata":{"papermill":{"duration":0.009201,"end_time":"2025-01-14T12:43:44.422533","exception":false,"start_time":"2025-01-14T12:43:44.413332","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:31:43.831040Z","iopub.execute_input":"2025-01-18T13:31:43.831672Z","iopub.status.idle":"2025-01-18T13:31:43.835387Z","shell.execute_reply.started":"2025-01-18T13:31:43.831639Z","shell.execute_reply":"2025-01-18T13:31:43.834648Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Loading data","metadata":{"papermill":{"duration":0.003079,"end_time":"2025-01-14T12:43:44.428948","exception":false,"start_time":"2025-01-14T12:43:44.425869","status":"completed"},"tags":[]}},{"cell_type":"code","source":"test = pd.read_parquet(CFG.test_path).fillna('')","metadata":{"papermill":{"duration":0.151004,"end_time":"2025-01-14T12:43:44.583187","exception":false,"start_time":"2025-01-14T12:43:44.432183","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:31:43.836570Z","iopub.execute_input":"2025-01-18T13:31:43.836870Z","iopub.status.idle":"2025-01-18T13:31:43.981763Z","shell.execute_reply.started":"2025-01-18T13:31:43.836842Z","shell.execute_reply":"2025-01-18T13:31:43.981141Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Tokenizing","metadata":{"papermill":{"duration":0.003177,"end_time":"2025-01-14T12:43:44.589972","exception":false,"start_time":"2025-01-14T12:43:44.586795","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def tokenize(tokenizer, prompt, response_a, response_b, max_length=CFG.max_length):\n    prompt = [\"<prompt>: \" + t for t in prompt]\n    response_a = [\"\\n\\n<response_a>: \" + t for t in response_a]\n    response_b = [\"\\n\\n<response_b>: \" + t for t in response_b]\n    \n    texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n    tokenized = tokenizer(texts, max_length=max_length, truncation=True)\n    \n    return tokenized['input_ids'], tokenized['attention_mask']","metadata":{"papermill":{"duration":0.010411,"end_time":"2025-01-14T12:43:44.603854","exception":false,"start_time":"2025-01-14T12:43:44.593443","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:31:43.982799Z","iopub.execute_input":"2025-01-18T13:31:43.983060Z","iopub.status.idle":"2025-01-18T13:31:43.987189Z","shell.execute_reply.started":"2025-01-18T13:31:43.983027Z","shell.execute_reply":"2025-01-18T13:31:43.986525Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"tokenizer = GemmaTokenizerFast.from_pretrained(CFG.gemma_dir)\ntokenizer.add_eos_token = True\ntokenizer.padding_side = \"right\"","metadata":{"papermill":{"duration":1.034016,"end_time":"2025-01-14T12:43:45.641211","exception":false,"start_time":"2025-01-14T12:43:44.607195","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:31:43.987929Z","iopub.execute_input":"2025-01-18T13:31:43.988111Z","iopub.status.idle":"2025-01-18T13:31:45.021934Z","shell.execute_reply.started":"2025-01-18T13:31:43.988094Z","shell.execute_reply":"2025-01-18T13:31:45.021027Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"for col in ['prompt', 'response_a', 'response_b']:\n    test[col] = test[col].fillna('')\n    text_list = []\n    if col == 'prompt':\n        max_no = 1024\n        s_no = 511\n        e_no = -512\n    else:\n        max_no = 3072\n        s_no = 1535\n        e_no = -1536\n    for text in tqdm(test[col]):\n        encoded = tokenizer(text, return_offsets_mapping=True)\n        if len(encoded['input_ids']) > max_no:\n            start_idx, end_idx = encoded['offset_mapping'][s_no]\n            new_text = text[:end_idx]\n            start_idx, end_idx = encoded['offset_mapping'][e_no]\n            new_text = new_text + \"\\n(snip)\\n\" + text[start_idx:]\n            text = new_text\n        text_list.append(text)\n    test[col] = text_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:31:45.022856Z","iopub.execute_input":"2025-01-18T13:31:45.023161Z","iopub.status.idle":"2025-01-18T13:31:45.057108Z","shell.execute_reply.started":"2025-01-18T13:31:45.023133Z","shell.execute_reply":"2025-01-18T13:31:45.056468Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 3/3 [00:00<00:00, 790.04it/s]\n100%|██████████| 3/3 [00:00<00:00, 755.96it/s]\n100%|██████████| 3/3 [00:00<00:00, 652.71it/s]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"data = pd.DataFrame()\ndata[\"id\"] = test[\"id\"]\ndata[\"input_ids\"], data[\"attention_mask\"] = tokenize(tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])\ndata[\"length\"] = data[\"input_ids\"].apply(len)\n\naug_data = pd.DataFrame()\naug_data[\"id\"] = test[\"id\"]\n# swap response_a & response_b\naug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test[\"prompt\"], test[\"response_b\"], test[\"response_a\"])\naug_data[\"length\"] = aug_data[\"input_ids\"].apply(len)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:31:45.057855Z","iopub.execute_input":"2025-01-18T13:31:45.058074Z","iopub.status.idle":"2025-01-18T13:31:45.075061Z","shell.execute_reply.started":"2025-01-18T13:31:45.058056Z","shell.execute_reply":"2025-01-18T13:31:45.074329Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Model","metadata":{"papermill":{"duration":0.005391,"end_time":"2025-01-14T12:43:45.696577","exception":false,"start_time":"2025-01-14T12:43:45.691186","status":"completed"},"tags":[]}},{"cell_type":"code","source":"model_0 = Gemma2ForSequenceClassification.from_pretrained(\n    CFG.gemma_dir,\n    device_map=torch.device(\"cuda:0\"),\n    use_cache=False,\n)\n\n\nmodel_1 = Gemma2ForSequenceClassification.from_pretrained(\n    CFG.gemma_dir,\n    device_map=torch.device(\"cuda:1\"),\n    use_cache=False,\n)","metadata":{"_kg_hide-output":true,"papermill":{"duration":50.15961,"end_time":"2025-01-14T12:44:35.861899","exception":false,"start_time":"2025-01-14T12:43:45.702289","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:31:45.077160Z","iopub.execute_input":"2025-01-18T13:31:45.077365Z","iopub.status.idle":"2025-01-18T13:32:33.738555Z","shell.execute_reply.started":"2025-01-18T13:31:45.077347Z","shell.execute_reply":"2025-01-18T13:32:33.737689Z"}},"outputs":[{"name":"stderr","text":"Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\nSome weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/gemma-2-9b-it-bnb-4bit-unsloth/transformers/default/1 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nUnused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\nSome weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/gemma-2-9b-it-bnb-4bit-unsloth/transformers/default/1 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"model_0 = PeftModel.from_pretrained(model_0, CFG.lora_dir)\nmodel_1 = PeftModel.from_pretrained(model_1, CFG.lora_dir)","metadata":{"papermill":{"duration":1.073886,"end_time":"2025-01-14T12:44:36.942850","exception":false,"start_time":"2025-01-14T12:44:35.868964","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:32:33.739860Z","iopub.execute_input":"2025-01-18T13:32:33.740168Z","iopub.status.idle":"2025-01-18T13:32:35.409563Z","shell.execute_reply.started":"2025-01-18T13:32:33.740137Z","shell.execute_reply":"2025-01-18T13:32:35.408856Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"model_0.eval()\nmodel_1.eval()","metadata":{"trusted":true,"_kg_hide-output":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-01-18T13:32:35.410343Z","iopub.execute_input":"2025-01-18T13:32:35.410555Z","iopub.status.idle":"2025-01-18T13:32:35.431113Z","shell.execute_reply.started":"2025-01-18T13:32:35.410536Z","shell.execute_reply":"2025-01-18T13:32:35.430274Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"PeftModelForSequenceClassification(\n  (base_model): LoraModel(\n    (model): Gemma2ForSequenceClassification(\n      (model): Gemma2Model(\n        (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n        (layers): ModuleList(\n          (0-15): 16 x Gemma2DecoderLayer(\n            (self_attn): Gemma2SdpaAttention(\n              (q_proj): Linear4bit(in_features=3584, out_features=4096, bias=False)\n              (k_proj): Linear4bit(in_features=3584, out_features=2048, bias=False)\n              (v_proj): Linear4bit(in_features=3584, out_features=2048, bias=False)\n              (o_proj): Linear4bit(in_features=4096, out_features=3584, bias=False)\n              (rotary_emb): Gemma2RotaryEmbedding()\n            )\n            (mlp): Gemma2MLP(\n              (gate_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n              (up_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n              (down_proj): Linear4bit(in_features=14336, out_features=3584, bias=False)\n              (act_fn): PytorchGELUTanh()\n            )\n            (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n            (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n            (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n            (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n          )\n          (16-41): 26 x Gemma2DecoderLayer(\n            (self_attn): Gemma2SdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3584, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3584, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3584, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3584, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): Linear4bit(in_features=4096, out_features=3584, bias=False)\n              (rotary_emb): Gemma2RotaryEmbedding()\n            )\n            (mlp): Gemma2MLP(\n              (gate_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n              (up_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n              (down_proj): Linear4bit(in_features=14336, out_features=3584, bias=False)\n              (act_fn): PytorchGELUTanh()\n            )\n            (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n            (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n            (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n            (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n          )\n        )\n        (norm): Gemma2RMSNorm((3584,), eps=1e-06)\n      )\n      (score): ModulesToSaveWrapper(\n        (original_module): Linear(in_features=3584, out_features=2, bias=False)\n        (modules_to_save): ModuleDict(\n          (default): Linear(in_features=3584, out_features=2, bias=False)\n        )\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"# Inference","metadata":{"papermill":{"duration":0.003592,"end_time":"2025-01-14T12:44:36.950550","exception":false,"start_time":"2025-01-14T12:44:36.946958","status":"completed"},"tags":[]}},{"cell_type":"code","source":"@torch.no_grad()\n@torch.cuda.amp.autocast()\ndef inference(df, model, device, batch_size, max_length=CFG.max_length):\n    winners = []\n    \n    for start_idx in range(0, len(df), batch_size):\n        end_idx = min(start_idx + batch_size, len(df))\n        tmp = df.iloc[start_idx:end_idx]\n        input_ids = tmp[\"input_ids\"].to_list()\n        attention_mask = tmp[\"attention_mask\"].to_list()\n        inputs = pad_without_fast_tokenizer_warning(\n            tokenizer,\n            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n            padding=\"longest\",\n            pad_to_multiple_of=None,\n            return_tensors=\"pt\",\n        )\n        outputs = model(**inputs.to(device))\n        proba = outputs.logits.softmax(-1).cpu()\n        \n        winners.extend(proba[:, 1].tolist())\n    \n    df['winner'] = winners\n    \n    return df","metadata":{"_kg_hide-output":true,"papermill":{"duration":0.014165,"end_time":"2025-01-14T12:44:36.968406","exception":false,"start_time":"2025-01-14T12:44:36.954241","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:32:35.431946Z","iopub.execute_input":"2025-01-18T13:32:35.432238Z","iopub.status.idle":"2025-01-18T13:32:35.443964Z","shell.execute_reply.started":"2025-01-18T13:32:35.432205Z","shell.execute_reply":"2025-01-18T13:32:35.443245Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-12-aeb673b51808>:2: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  @torch.cuda.amp.autocast()\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"data = pd.concat([data, aug_data]).reset_index(drop=True)\ndata['index'] = np.arange(len(data), dtype=np.int32)\ndata = data.sort_values(\"length\", ascending=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:32:35.444702Z","iopub.execute_input":"2025-01-18T13:32:35.444892Z","iopub.status.idle":"2025-01-18T13:32:35.467651Z","shell.execute_reply.started":"2025-01-18T13:32:35.444876Z","shell.execute_reply":"2025-01-18T13:32:35.467021Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"data_dict = {}\ndata_dict[0] = data[data[\"length\"] > 1024].reset_index(drop=True)\ndata_dict[1] = data[data[\"length\"] <= 1024].reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:32:35.468314Z","iopub.execute_input":"2025-01-18T13:32:35.468526Z","iopub.status.idle":"2025-01-18T13:32:35.473377Z","shell.execute_reply.started":"2025-01-18T13:32:35.468507Z","shell.execute_reply":"2025-01-18T13:32:35.472590Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"result_df = []\nfor i, batch_size in enumerate([2, 16]):\n    if len(data_dict[i]) == 0:\n        continue\n        \n    sub_1 = data_dict[i].iloc[0::2].copy()\n    sub_2 = data_dict[i].iloc[1::2].copy()\n    \n    with ThreadPoolExecutor(max_workers=2) as executor:\n        results = executor.map(\n            inference, \n            (sub_1, sub_2), \n            (model_0, model_1), \n            (torch.device(\"cuda:0\"), torch.device(\"cuda:1\")), \n            (batch_size, batch_size)\n        )\n        \n    result_df.append(pd.concat(list(results), axis=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:32:35.474053Z","iopub.execute_input":"2025-01-18T13:32:35.474298Z","iopub.status.idle":"2025-01-18T13:32:40.417980Z","shell.execute_reply.started":"2025-01-18T13:32:35.474265Z","shell.execute_reply":"2025-01-18T13:32:40.417304Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"result_df = pd.concat(result_df).sort_values('index').reset_index(drop=True)\nproba = result_df['winner'].values[:len(aug_data)]\ntta_proba = 1 - result_df['winner'].values[len(aug_data):]\nproba = (proba + tta_proba) / 2\nresult_df = result_df[:len(aug_data)].copy()\nresult_df['winner'] = proba","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:32:40.418712Z","iopub.execute_input":"2025-01-18T13:32:40.418933Z","iopub.status.idle":"2025-01-18T13:32:40.425922Z","shell.execute_reply.started":"2025-01-18T13:32:40.418903Z","shell.execute_reply":"2025-01-18T13:32:40.425160Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"sub = result_df[['id', 'winner']].copy()\nsub['winner'] = np.where(sub['winner'] < 0.5, 'model_a', 'model_b')\nsub.to_csv('submission.csv', index=False)\nsub.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T13:32:40.426767Z","iopub.execute_input":"2025-01-18T13:32:40.426986Z","iopub.status.idle":"2025-01-18T13:32:40.452246Z","shell.execute_reply.started":"2025-01-18T13:32:40.426966Z","shell.execute_reply":"2025-01-18T13:32:40.451297Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"        id   winner\n0   327228  model_b\n1  1139415  model_a\n2  1235630  model_a","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>winner</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>327228</td>\n      <td>model_b</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1139415</td>\n      <td>model_a</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1235630</td>\n      <td>model_a</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":17}]}