{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9ca9e3d",
   "metadata": {
    "papermill": {
     "duration": 0.0058,
     "end_time": "2025-01-28T07:37:19.322362",
     "exception": false,
     "start_time": "2025-01-28T07:37:19.316562",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports and configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86d0c13a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T07:37:19.333465Z",
     "iopub.status.busy": "2025-01-28T07:37:19.333163Z",
     "iopub.status.idle": "2025-01-28T07:37:36.178861Z",
     "shell.execute_reply": "2025-01-28T07:37:36.177932Z"
    },
    "papermill": {
     "duration": 16.853026,
     "end_time": "2025-01-28T07:37:36.180526",
     "exception": false,
     "start_time": "2025-01-28T07:37:19.327500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Gemma2ForSequenceClassification, GemmaTokenizerFast\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from timeit import default_timer as timer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import joblib\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89e09ac3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T07:37:36.195448Z",
     "iopub.status.busy": "2025-01-28T07:37:36.194850Z",
     "iopub.status.idle": "2025-01-28T07:37:36.199825Z",
     "shell.execute_reply": "2025-01-28T07:37:36.198992Z"
    },
    "papermill": {
     "duration": 0.013148,
     "end_time": "2025-01-28T07:37:36.200972",
     "exception": false,
     "start_time": "2025-01-28T07:37:36.187824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    train_path = '/kaggle/input/wsdm-cup-multilingual-chatbot-arena/train.parquet'\n",
    "    test_path = '/kaggle/input/wsdm-cup-multilingual-chatbot-arena/test.parquet'\n",
    "    sample_sub_path = '/kaggle/input/wsdm-cup-multilingual-chatbot-arena/sample_submission.csv'\n",
    "    \n",
    "    gemma_dir = \"/kaggle/input/gemma-2-9b-4bit-it-unsloth/transformers/default/1/gemma-2-9b-it-4bit-unsloth_old\"\n",
    "    lora_dir = \"/kaggle/input/wsdm-cup-gemma-2-9b-4-bit-qlora/gemma2-9b-4bit/gemma-2-9b-it-bnb-4bit-3072-8/checkpoint-3027\"\n",
    "\n",
    "    data_path = '/kaggle/input/wsdm-cup-gemma-2-9b-4-bit-qlora'\n",
    "\n",
    "    target = 'winner'\n",
    "    n_folds = 5\n",
    "    seed = 42\n",
    "    \n",
    "    max_length = 3072\n",
    "    batch_size = 4\n",
    "\n",
    "    char_vectorizer_params = {\n",
    "        'analyzer': \"char\",\n",
    "        \"lowercase\": False,\n",
    "        \"max_df\": 0.605,\n",
    "        \"max_features\": 331,\n",
    "        \"min_df\": 0.075,\n",
    "        \"ngram_range\": (1, 3),\n",
    "        \"strip_accents\": \"unicode\"\n",
    "    }\n",
    "\n",
    "    word_vectorizer_params = {\n",
    "        \"analyzer\": \"word\",\n",
    "        \"lowercase\": True,\n",
    "        \"max_df\": 0.985,\n",
    "        \"max_features\": 769,\n",
    "        \"min_df\": 0.01,\n",
    "        \"ngram_range\": (1, 2),\n",
    "        \"strip_accents\": \"unicode\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83aabe8",
   "metadata": {
    "papermill": {
     "duration": 0.005166,
     "end_time": "2025-01-28T07:37:36.211996",
     "exception": false,
     "start_time": "2025-01-28T07:37:36.206830",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Gemma-2 9b 4-bit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2105127",
   "metadata": {
    "papermill": {
     "duration": 0.005194,
     "end_time": "2025-01-28T07:37:36.222700",
     "exception": false,
     "start_time": "2025-01-28T07:37:36.217506",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b4479d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T07:37:36.234436Z",
     "iopub.status.busy": "2025-01-28T07:37:36.234134Z",
     "iopub.status.idle": "2025-01-28T07:37:36.387344Z",
     "shell.execute_reply": "2025-01-28T07:37:36.386326Z"
    },
    "papermill": {
     "duration": 0.16072,
     "end_time": "2025-01-28T07:37:36.388869",
     "exception": false,
     "start_time": "2025-01-28T07:37:36.228149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_parquet(CFG.test_path).fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cfa99f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T07:37:36.400850Z",
     "iopub.status.busy": "2025-01-28T07:37:36.400564Z",
     "iopub.status.idle": "2025-01-28T07:37:36.404287Z",
     "shell.execute_reply": "2025-01-28T07:37:36.403444Z"
    },
    "papermill": {
     "duration": 0.011028,
     "end_time": "2025-01-28T07:37:36.405557",
     "exception": false,
     "start_time": "2025-01-28T07:37:36.394529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if len(test) > 10_000:\n",
    "    time_limit = int(3600 * 12) \n",
    "else:\n",
    "    time_limit = int(3600 * 4.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301659e4",
   "metadata": {
    "papermill": {
     "duration": 0.005126,
     "end_time": "2025-01-28T07:37:36.415988",
     "exception": false,
     "start_time": "2025-01-28T07:37:36.410862",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b069748",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T07:37:36.426913Z",
     "iopub.status.busy": "2025-01-28T07:37:36.426689Z",
     "iopub.status.idle": "2025-01-28T07:37:36.431081Z",
     "shell.execute_reply": "2025-01-28T07:37:36.430297Z"
    },
    "papermill": {
     "duration": 0.011412,
     "end_time": "2025-01-28T07:37:36.432379",
     "exception": false,
     "start_time": "2025-01-28T07:37:36.420967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(tokenizer, prompt, response_a, response_b, max_length=CFG.max_length):\n",
    "    prompt = [\"<prompt>: \" + t for t in prompt]\n",
    "    response_a = [\"\\n\\n<response_a>: \" + t for t in response_a]\n",
    "    response_b = [\"\\n\\n<response_b>: \" + t for t in response_b]\n",
    "    \n",
    "    texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "    tokenized = tokenizer(texts, max_length=max_length, truncation=True)\n",
    "    \n",
    "    return tokenized['input_ids'], tokenized['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "862c856c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T07:37:36.443559Z",
     "iopub.status.busy": "2025-01-28T07:37:36.443326Z",
     "iopub.status.idle": "2025-01-28T07:37:37.425189Z",
     "shell.execute_reply": "2025-01-28T07:37:37.424468Z"
    },
    "papermill": {
     "duration": 0.988999,
     "end_time": "2025-01-28T07:37:37.426686",
     "exception": false,
     "start_time": "2025-01-28T07:37:36.437687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = GemmaTokenizerFast.from_pretrained(CFG.gemma_dir)\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f672a03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T07:37:37.438650Z",
     "iopub.status.busy": "2025-01-28T07:37:37.438411Z",
     "iopub.status.idle": "2025-01-28T07:37:37.473523Z",
     "shell.execute_reply": "2025-01-28T07:37:37.472726Z"
    },
    "papermill": {
     "duration": 0.042256,
     "end_time": "2025-01-28T07:37:37.474872",
     "exception": false,
     "start_time": "2025-01-28T07:37:37.432616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 773.14it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 722.95it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 570.14it/s]\n"
     ]
    }
   ],
   "source": [
    "for col in ['prompt', 'response_a', 'response_b']:\n",
    "    test[col] = test[col].fillna('')\n",
    "    text_list = []\n",
    "    if col == \"prompt\":\n",
    "        max_no = 512\n",
    "        s_no = 255\n",
    "        e_no = -256\n",
    "    else:\n",
    "        max_no = 3072\n",
    "        s_no = 1535\n",
    "        e_no = -1536\n",
    "    for text in tqdm(test[col]):\n",
    "        encoded = tokenizer(text, return_offsets_mapping=True)\n",
    "        if len(encoded['input_ids']) > max_no:\n",
    "            start_idx, end_idx = encoded['offset_mapping'][s_no]\n",
    "            new_text = text[:end_idx]\n",
    "            start_idx, end_idx = encoded['offset_mapping'][e_no]\n",
    "            new_text = new_text + \"\\n(snip)\\n\" + text[start_idx:]\n",
    "            text = new_text\n",
    "        text_list.append(text)\n",
    "    test[col] = text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b75314d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T07:37:37.487776Z",
     "iopub.status.busy": "2025-01-28T07:37:37.487539Z",
     "iopub.status.idle": "2025-01-28T07:37:37.505579Z",
     "shell.execute_reply": "2025-01-28T07:37:37.504701Z"
    },
    "papermill": {
     "duration": 0.025599,
     "end_time": "2025-01-28T07:37:37.506811",
     "exception": false,
     "start_time": "2025-01-28T07:37:37.481212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "data[\"id\"] = test[\"id\"]\n",
    "data[\"input_ids\"], data[\"attention_mask\"] = tokenize(tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])\n",
    "data[\"length\"] = data[\"input_ids\"].apply(len)\n",
    "\n",
    "aug_data = pd.DataFrame()\n",
    "aug_data[\"id\"] = test[\"id\"]\n",
    "# swap response_a & response_b\n",
    "aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test[\"prompt\"], test[\"response_b\"], test[\"response_a\"])\n",
    "aug_data[\"length\"] = aug_data[\"input_ids\"].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d0d8f0",
   "metadata": {
    "papermill": {
     "duration": 0.00559,
     "end_time": "2025-01-28T07:37:37.518109",
     "exception": false,
     "start_time": "2025-01-28T07:37:37.512519",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af9331d6",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-01-28T07:37:37.529928Z",
     "iopub.status.busy": "2025-01-28T07:37:37.529701Z",
     "iopub.status.idle": "2025-01-28T07:38:27.166817Z",
     "shell.execute_reply": "2025-01-28T07:38:27.165869Z"
    },
    "papermill": {
     "duration": 49.644779,
     "end_time": "2025-01-28T07:38:27.168491",
     "exception": false,
     "start_time": "2025-01-28T07:37:37.523712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Some weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/gemma-2-9b-4bit-it-unsloth/transformers/default/1/gemma-2-9b-it-4bit-unsloth_old and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Some weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/gemma-2-9b-4bit-it-unsloth/transformers/default/1/gemma-2-9b-it-4bit-unsloth_old and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_0 = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    CFG.gemma_dir,\n",
    "    device_map=torch.device(\"cuda:0\"),\n",
    "    use_cache=False,\n",
    ")\n",
    "\n",
    "model_1 = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    CFG.gemma_dir,\n",
    "    device_map=torch.device(\"cuda:1\"),\n",
    "    use_cache=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b6b6120",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T07:38:27.182618Z",
     "iopub.status.busy": "2025-01-28T07:38:27.182392Z",
     "iopub.status.idle": "2025-01-28T07:38:29.173636Z",
     "shell.execute_reply": "2025-01-28T07:38:29.172723Z"
    },
    "papermill": {
     "duration": 2.000543,
     "end_time": "2025-01-28T07:38:29.175548",
     "exception": false,
     "start_time": "2025-01-28T07:38:27.175005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_0 = PeftModel.from_pretrained(model_0, CFG.lora_dir)\n",
    "model_1 = PeftModel.from_pretrained(model_1, CFG.lora_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd69f131",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-01-28T07:38:29.189976Z",
     "iopub.status.busy": "2025-01-28T07:38:29.189651Z",
     "iopub.status.idle": "2025-01-28T07:38:29.216402Z",
     "shell.execute_reply": "2025-01-28T07:38:29.215576Z"
    },
    "papermill": {
     "duration": 0.035809,
     "end_time": "2025-01-28T07:38:29.217912",
     "exception": false,
     "start_time": "2025-01-28T07:38:29.182103",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma2ForSequenceClassification(\n",
       "      (model): Gemma2Model(\n",
       "        (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x Gemma2DecoderLayer(\n",
       "            (self_attn): Gemma2SdpaAttention(\n",
       "              (q_proj): Linear4bit(in_features=3584, out_features=4096, bias=False)\n",
       "              (k_proj): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
       "              (v_proj): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
       "              (o_proj): Linear4bit(in_features=4096, out_features=3584, bias=False)\n",
       "              (rotary_emb): Gemma2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Gemma2MLP(\n",
       "              (gate_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=3584, bias=False)\n",
       "              (act_fn): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "            (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "            (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "            (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "          )\n",
       "          (16-41): 26 x Gemma2DecoderLayer(\n",
       "            (self_attn): Gemma2SdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=4096, out_features=3584, bias=False)\n",
       "              (rotary_emb): Gemma2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Gemma2MLP(\n",
       "              (gate_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=3584, bias=False)\n",
       "              (act_fn): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "            (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "            (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "            (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=3584, out_features=2, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=3584, out_features=2, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0.eval()\n",
    "model_1.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b268930",
   "metadata": {
    "papermill": {
     "duration": 0.007471,
     "end_time": "2025-01-28T07:38:29.236883",
     "exception": false,
     "start_time": "2025-01-28T07:38:29.229412",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46d894d9",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-01-28T07:38:29.250477Z",
     "iopub.status.busy": "2025-01-28T07:38:29.250140Z",
     "iopub.status.idle": "2025-01-28T07:38:29.257495Z",
     "shell.execute_reply": "2025-01-28T07:38:29.256699Z"
    },
    "papermill": {
     "duration": 0.015661,
     "end_time": "2025-01-28T07:38:29.258808",
     "exception": false,
     "start_time": "2025-01-28T07:38:29.243147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference(df, model, device, batch_size, max_length=CFG.max_length):\n",
    "    winners = []\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        tmp = df.iloc[start_idx:end_idx]\n",
    "        input_ids = tmp[\"input_ids\"].to_list()\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        outputs = model(**inputs.to(device))\n",
    "        proba = outputs.logits.softmax(-1).cpu()\n",
    "        \n",
    "        winners.extend(proba[:, 1].tolist())\n",
    "    \n",
    "    df['winner'] = winners\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dc72cac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T07:38:29.278441Z",
     "iopub.status.busy": "2025-01-28T07:38:29.278171Z",
     "iopub.status.idle": "2025-01-28T07:38:29.281650Z",
     "shell.execute_reply": "2025-01-28T07:38:29.280826Z"
    },
    "papermill": {
     "duration": 0.016065,
     "end_time": "2025-01-28T07:38:29.282858",
     "exception": false,
     "start_time": "2025-01-28T07:38:29.266793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_timer = timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce5e56fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T07:38:29.296517Z",
     "iopub.status.busy": "2025-01-28T07:38:29.296278Z",
     "iopub.status.idle": "2025-01-28T07:38:29.308200Z",
     "shell.execute_reply": "2025-01-28T07:38:29.307424Z"
    },
    "papermill": {
     "duration": 0.020245,
     "end_time": "2025-01-28T07:38:29.309564",
     "exception": false,
     "start_time": "2025-01-28T07:38:29.289319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['index'] = np.arange(len(data), dtype=np.int32)\n",
    "data = data.sort_values(\"length\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39b67672",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T07:38:29.323330Z",
     "iopub.status.busy": "2025-01-28T07:38:29.323097Z",
     "iopub.status.idle": "2025-01-28T07:38:29.329008Z",
     "shell.execute_reply": "2025-01-28T07:38:29.328127Z"
    },
    "papermill": {
     "duration": 0.014508,
     "end_time": "2025-01-28T07:38:29.330560",
     "exception": false,
     "start_time": "2025-01-28T07:38:29.316052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "data_dict[0] = data[data[\"length\"] > 1024].reset_index(drop=True)\n",
    "data_dict[1] = data[data[\"length\"] <= 1024].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f50b689e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T07:38:29.350713Z",
     "iopub.status.busy": "2025-01-28T07:38:29.350427Z",
     "iopub.status.idle": "2025-01-28T07:38:32.898002Z",
     "shell.execute_reply": "2025-01-28T07:38:32.897328Z"
    },
    "papermill": {
     "duration": 3.556891,
     "end_time": "2025-01-28T07:38:32.899681",
     "exception": false,
     "start_time": "2025-01-28T07:38:29.342790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_df = []\n",
    "for i, batch_size in enumerate([CFG.batch_size, CFG.batch_size]):\n",
    "    if len(data_dict[i]) == 0:\n",
    "        continue\n",
    "        \n",
    "    sub_1 = data_dict[i].iloc[0::2].copy()\n",
    "    sub_2 = data_dict[i].iloc[1::2].copy()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        results = executor.map(\n",
    "            inference, \n",
    "            (sub_1, sub_2), \n",
    "            (model_0, model_1), \n",
    "            (torch.device(\"cuda:0\"), torch.device(\"cuda:1\")), \n",
    "            (batch_size, batch_size)\n",
    "        )\n",
    "        \n",
    "    result_df.append(pd.concat(list(results), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d79a043",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T07:38:32.914632Z",
     "iopub.status.busy": "2025-01-28T07:38:32.914410Z",
     "iopub.status.idle": "2025-01-28T07:38:32.919381Z",
     "shell.execute_reply": "2025-01-28T07:38:32.918735Z"
    },
    "papermill": {
     "duration": 0.013652,
     "end_time": "2025-01-28T07:38:32.920581",
     "exception": false,
     "start_time": "2025-01-28T07:38:32.906929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_df = pd.concat(result_df).sort_values('index').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2092d4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T07:38:32.934012Z",
     "iopub.status.busy": "2025-01-28T07:38:32.933795Z",
     "iopub.status.idle": "2025-01-28T07:38:32.937679Z",
     "shell.execute_reply": "2025-01-28T07:38:32.937053Z"
    },
    "papermill": {
     "duration": 0.012236,
     "end_time": "2025-01-28T07:38:32.939048",
     "exception": false,
     "start_time": "2025-01-28T07:38:32.926812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "aug_data['index'] = np.arange(len(aug_data), dtype=np.int32)\n",
    "aug_data = aug_data.sort_values(\"length\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b55178c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T07:38:32.955214Z",
     "iopub.status.busy": "2025-01-28T07:38:32.954976Z",
     "iopub.status.idle": "2025-01-28T07:38:32.964411Z",
     "shell.execute_reply": "2025-01-28T07:38:32.963751Z"
    },
    "papermill": {
     "duration": 0.017788,
     "end_time": "2025-01-28T07:38:32.965709",
     "exception": false,
     "start_time": "2025-01-28T07:38:32.947921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONFIDENCE_THRESHOLD = 0.2\n",
    "not_confident_mask = abs(result_df['winner'] - 0.5) < CONFIDENCE_THRESHOLD\n",
    "\n",
    "aug_data = aug_data[aug_data['index'].isin(result_df[not_confident_mask]['index'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4df7aced",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T07:38:32.980642Z",
     "iopub.status.busy": "2025-01-28T07:38:32.980449Z",
     "iopub.status.idle": "2025-01-28T07:38:32.985417Z",
     "shell.execute_reply": "2025-01-28T07:38:32.984802Z"
    },
    "papermill": {
     "duration": 0.013832,
     "end_time": "2025-01-28T07:38:32.986681",
     "exception": false,
     "start_time": "2025-01-28T07:38:32.972849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "aug_data_dict = {}\n",
    "aug_data_dict[0] = aug_data[aug_data[\"length\"] > 1024].reset_index(drop=True)\n",
    "aug_data_dict[1] = aug_data[aug_data[\"length\"] <= 1024].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f8709f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T07:38:33.001143Z",
     "iopub.status.busy": "2025-01-28T07:38:33.000910Z",
     "iopub.status.idle": "2025-01-28T07:38:35.344006Z",
     "shell.execute_reply": "2025-01-28T07:38:35.343359Z"
    },
    "papermill": {
     "duration": 2.351811,
     "end_time": "2025-01-28T07:38:35.345615",
     "exception": false,
     "start_time": "2025-01-28T07:38:32.993804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "aug_result_df = []\n",
    "for i, batch_size in enumerate([CFG.batch_size, CFG.batch_size]):\n",
    "    if len(aug_data_dict[i]) == 0:\n",
    "        continue\n",
    "\n",
    "    if timer() - global_timer > (time_limit - 1800):\n",
    "        break\n",
    "        \n",
    "    sub_1 = aug_data_dict[i].iloc[0::2].copy()\n",
    "    sub_2 = aug_data_dict[i].iloc[1::2].copy()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        results = executor.map(\n",
    "            inference, \n",
    "            (sub_1, sub_2), \n",
    "            (model_0, model_1), \n",
    "            (torch.device(\"cuda:0\"), torch.device(\"cuda:1\")), \n",
    "            (batch_size, batch_size)\n",
    "        )\n",
    "        \n",
    "    aug_result_df.append(pd.concat(list(results), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "626d758b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T07:38:35.360790Z",
     "iopub.status.busy": "2025-01-28T07:38:35.360509Z",
     "iopub.status.idle": "2025-01-28T07:38:35.365510Z",
     "shell.execute_reply": "2025-01-28T07:38:35.364440Z"
    },
    "papermill": {
     "duration": 0.013692,
     "end_time": "2025-01-28T07:38:35.366752",
     "exception": false,
     "start_time": "2025-01-28T07:38:35.353060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for 10_000 (max  4.5 hr):  5.6\n",
      "time for 25_000 (max 12.0 hr): 14.1\n"
     ]
    }
   ],
   "source": [
    "time_taken = timer() - global_timer\n",
    "\n",
    "print(f'time for 10_000 (max  4.5 hr): {10_000/3*time_taken/60/60:4.1f}')\n",
    "print(f'time for 25_000 (max 12.0 hr): {25_000/3*time_taken/60/60:4.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a77108aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T07:38:35.381222Z",
     "iopub.status.busy": "2025-01-28T07:38:35.380983Z",
     "iopub.status.idle": "2025-01-28T07:38:35.401815Z",
     "shell.execute_reply": "2025-01-28T07:38:35.401155Z"
    },
    "papermill": {
     "duration": 0.029069,
     "end_time": "2025-01-28T07:38:35.402989",
     "exception": false,
     "start_time": "2025-01-28T07:38:35.373920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if len(aug_result_df) > 0:\n",
    "    aug_result_df = pd.concat(aug_result_df).sort_values('index').reset_index(drop=True)\n",
    "    aug_result_df[\"winner\"] = 1 - aug_result_df['winner']\n",
    "    \n",
    "    result_df = result_df.merge(\n",
    "        aug_result_df[['index', 'winner']], \n",
    "        on='index', \n",
    "        how='left', \n",
    "        suffixes=('', '_aug')\n",
    "    )\n",
    "    \n",
    "    mask = result_df['winner_aug'].notna()\n",
    "    result_df.loc[mask, 'winner'] = (result_df.loc[mask, 'winner'] + result_df.loc[mask, 'winner_aug']) / 2\n",
    "    \n",
    "    result_df = result_df.drop('winner_aug', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "954dffa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T07:38:35.417568Z",
     "iopub.status.busy": "2025-01-28T07:38:35.417356Z",
     "iopub.status.idle": "2025-01-28T07:38:35.420526Z",
     "shell.execute_reply": "2025-01-28T07:38:35.419913Z"
    },
    "papermill": {
     "duration": 0.011472,
     "end_time": "2025-01-28T07:38:35.421696",
     "exception": false,
     "start_time": "2025-01-28T07:38:35.410224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gemma_test_pred_probs = result_df[\"winner\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0ddd6b",
   "metadata": {
    "papermill": {
     "duration": 0.006629,
     "end_time": "2025-01-28T07:38:35.434893",
     "exception": false,
     "start_time": "2025-01-28T07:38:35.428264",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# AutoGluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01b7e534",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-28T07:38:35.449237Z",
     "iopub.status.busy": "2025-01-28T07:38:35.448983Z",
     "iopub.status.idle": "2025-01-28T07:38:35.475529Z",
     "shell.execute_reply": "2025-01-28T07:38:35.474889Z"
    },
    "papermill": {
     "duration": 0.035214,
     "end_time": "2025-01-28T07:38:35.476704",
     "exception": false,
     "start_time": "2025-01-28T07:38:35.441490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(dataframe):\n",
    "    print('--- Reducing memory usage')\n",
    "    initial_mem_usage = dataframe.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    for col in dataframe.columns:\n",
    "        col_type = dataframe[col].dtype\n",
    "\n",
    "        if col_type.name in ['category', 'object']:\n",
    "            continue\n",
    "\n",
    "        c_min = dataframe[col].min()\n",
    "        c_max = dataframe[col].max()\n",
    "        if str(col_type)[:3] == 'int':\n",
    "            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                dataframe[col] = dataframe[col].astype(np.int8)\n",
    "            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                dataframe[col] = dataframe[col].astype(np.int16)\n",
    "            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                dataframe[col] = dataframe[col].astype(np.int32)\n",
    "            elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                dataframe[col] = dataframe[col].astype(np.int64)\n",
    "        else:\n",
    "            if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                dataframe[col] = dataframe[col].astype(np.float16)\n",
    "            elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                dataframe[col] = dataframe[col].astype(np.float32)\n",
    "            else:\n",
    "                dataframe[col] = dataframe[col].astype(np.float64)\n",
    "\n",
    "    final_mem_usage = dataframe.memory_usage().sum() / 1024**2\n",
    "    print('------ Memory usage before: {:.2f} MB'.format(initial_mem_usage))\n",
    "    print('------ Memory usage after: {:.2f} MB'.format(final_mem_usage))\n",
    "    print('------ Decreased memory usage by {:.1f}%'.format(100 * (initial_mem_usage - final_mem_usage) / initial_mem_usage))\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "def add_word_features(df, column):\n",
    "    # Basic word counts\n",
    "    df[f'{column}_word_count'] = df[column].apply(lambda x: len(x.split()))\n",
    "    df[f'{column}_unique_word_count'] = df[column].apply(lambda x: len(set(x.lower().split())))\n",
    "    \n",
    "    # Word length statistics\n",
    "    def safe_word_stats(text):\n",
    "        words = str(text).split()\n",
    "        if not words:\n",
    "            return 0, 0  # avg_length, max_length for empty text\n",
    "        word_lengths = [len(word) for word in words]\n",
    "        return np.mean(word_lengths), max(word_lengths)\n",
    "    \n",
    "    word_stats = df[column].apply(safe_word_stats)\n",
    "    df[f'{column}_avg_word_length'] = word_stats.apply(lambda x: x[0])\n",
    "    df[f'{column}_max_word_length'] = word_stats.apply(lambda x: x[1])\n",
    "    \n",
    "    # Lexical diversity (unique words / total words)\n",
    "    df[f'{column}_lexical_diversity'] = df.apply(\n",
    "        lambda x: x[f'{column}_unique_word_count'] / x[f'{column}_word_count'] \n",
    "        if x[f'{column}_word_count'] > 0 else 0, axis=1\n",
    "    )\n",
    "    \n",
    "    # Count specific word types\n",
    "    df[f'{column}_uppercase_word_count'] = df[column].apply(lambda x: sum(1 for word in x.split() if word.isupper()))\n",
    "    df[f'{column}_title_case_word_count'] = df[column].apply(lambda x: sum(1 for word in x.split() if word.istitle()))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_char_features(df, column):\n",
    "    # Basic character counts\n",
    "    df[f'{column}_char_count'] = df[column].str.len()\n",
    "    df[f'{column}_letter_count'] = df[column].apply(lambda x: sum(c.isalpha() for c in x))\n",
    "    df[f'{column}_digit_count'] = df[column].apply(lambda x: sum(c.isdigit() for c in x))\n",
    "    df[f'{column}_whitespace_count'] = df[column].apply(lambda x: sum(c.isspace() for c in x))\n",
    "    \n",
    "    # Punctuation counts\n",
    "    df[f'{column}_punctuation_count'] = df[column].apply(lambda x: sum(c in '.,!?;:' for c in x))\n",
    "    df[f'{column}_special_char_count'] = df[column].apply(lambda x: sum(not (c.isalnum() or c.isspace()) for c in x))\n",
    "    \n",
    "    # Character ratios with safe division\n",
    "    df[f'{column}_uppercase_ratio'] = df[column].apply(lambda x: sum(c.isupper() for c in x) / max(len(x), 1))\n",
    "    df[f'{column}_lowercase_ratio'] = df[column].apply(lambda x: sum(c.islower() for c in x) / max(len(x), 1))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_sentence_features(df, column):\n",
    "    # Sentence counts\n",
    "    df[f'{column}_sentence_count'] = df[column].apply(lambda x: len(sent_tokenize(x)))\n",
    "    \n",
    "    # Average sentence length with safe division\n",
    "    df[f'{column}_avg_sentence_length'] = df[column].apply(\n",
    "        lambda x: np.mean([len(sent.split()) for sent in sent_tokenize(x)])\n",
    "        if len(sent_tokenize(x)) > 0 else 0\n",
    "    )\n",
    "    \n",
    "    # Sentence length variation with safe handling\n",
    "    df[f'{column}_sentence_length_std'] = df[column].apply(\n",
    "        lambda x: np.std([len(sent.split()) for sent in sent_tokenize(x)]) \n",
    "        if len(sent_tokenize(x)) > 1 else 0\n",
    "    )\n",
    "    \n",
    "    # Question and exclamation counts\n",
    "    df[f'{column}_question_count'] = df[column].str.count('\\?')\n",
    "    df[f'{column}_exclamation_count'] = df[column].str.count('!')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_stats_features(df, column):\n",
    "    # Readability metrics (simplified Flesch Reading Ease)\n",
    "    def calculate_readability(text):\n",
    "        sentences = sent_tokenize(text)\n",
    "        words = text.split()\n",
    "        if not words or not sentences:\n",
    "            return 0\n",
    "        avg_sentence_length = len(words) / len(sentences)\n",
    "        avg_syllables_per_word = sum(count_syllables(word) for word in words) / len(words)\n",
    "        return 206.835 - (1.015 * avg_sentence_length) - (84.6 * avg_syllables_per_word)\n",
    "    \n",
    "    def count_syllables(word):\n",
    "        word = word.lower()\n",
    "        count = 0\n",
    "        vowels = 'aeiouy'\n",
    "        if word[0] in vowels:\n",
    "            count += 1\n",
    "        for index in range(1, len(word)):\n",
    "            if word[index] in vowels and word[index-1] not in vowels:\n",
    "                count += 1\n",
    "        if word.endswith('e'):\n",
    "            count -= 1\n",
    "        if count == 0:\n",
    "            count = 1\n",
    "        return count\n",
    "    \n",
    "    df[f'{column}_readability_score'] = df[column].apply(calculate_readability)\n",
    "    \n",
    "    # Text complexity features with safe division\n",
    "    df[f'{column}_avg_word_per_sentence'] = df.apply(\n",
    "        lambda x: x[f'{column}_word_count'] / x[f'{column}_sentence_count']\n",
    "        if x[f'{column}_sentence_count'] > 0 else 0, axis=1\n",
    "    )\n",
    "    \n",
    "    df[f'{column}_char_per_word'] = df.apply(\n",
    "        lambda x: x[f'{column}_char_count'] / x[f'{column}_word_count']\n",
    "        if x[f'{column}_word_count'] > 0 else 0, axis=1\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_text_similarity(text1, text2):\n",
    "    text1, text2 = str(text1), str(text2)\n",
    "    \n",
    "    chars1, chars2 = set(text1.lower()), set(text2.lower())\n",
    "    char_similarity = len(chars1 & chars2) / max(len(chars1 | chars2), 1)\n",
    "    \n",
    "    words1 = set(text1.lower().split())\n",
    "    words2 = set(text2.lower().split())\n",
    "    word_similarity = len(words1 & words2) / max(len(words1 | words2), 1)\n",
    "    \n",
    "    length_ratio = len(text1) / max(len(text2), 1)\n",
    "    \n",
    "    return char_similarity, word_similarity, length_ratio\n",
    "\n",
    "def add_features(df, is_train):\n",
    "    if is_train:\n",
    "        features = joblib.load(f'{CFG.data_path}/features/train_features.pkl')\n",
    "        df = df.merge(features, on='id', how='left')\n",
    "        return df\n",
    "    \n",
    "    for column in ['prompt', 'response_a', 'response_b']:\n",
    "        df = add_word_features(df, column)\n",
    "        df = add_char_features(df, column)\n",
    "        df = add_sentence_features(df, column)\n",
    "        df = add_stats_features(df, column)\n",
    "        \n",
    "    # Add text similarity features between prompt and response_a/response_b\n",
    "    similarities_a = df.apply(lambda row: get_text_similarity(row['prompt'], row['response_a']), axis=1)\n",
    "    similarities_b = df.apply(lambda row: get_text_similarity(row['prompt'], row['response_b']), axis=1)\n",
    "    df['prompt_response_a_char_sim'], df['prompt_response_a_word_sim'], df['prompt_response_a_length_ratio'] = zip(*similarities_a)\n",
    "    df['prompt_response_b_char_sim'], df['prompt_response_b_word_sim'], df['prompt_response_b_length_ratio'] = zip(*similarities_b)\n",
    "        \n",
    "    # Add comparative features between response_a and response_b with safe division\n",
    "    for feature in df.columns:\n",
    "        if feature.startswith('response_a_'):\n",
    "            corresponding_b = feature.replace('response_a_', 'response_b_')\n",
    "            if corresponding_b in df.columns:\n",
    "                df[f'diff_{feature.replace(\"response_a_\", \"\")}'] = df[feature] - df[corresponding_b]\n",
    "                df[f'ratio_{feature.replace(\"response_a_\", \"\")}'] = df.apply(\n",
    "                    lambda x: x[feature] / x[corresponding_b] \n",
    "                    if x[corresponding_b] != 0 else 0, axis=1\n",
    "                )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_tfidf_features(df, is_train):\n",
    "    if is_train:\n",
    "        tfidf_features = joblib.load(f'{CFG.data_path}/features/train_tfidf_features.pkl')\n",
    "        df = df.merge(tfidf_features, on='id', how='left')\n",
    "        return df\n",
    "\n",
    "    for column in ['prompt', 'response_a', 'response_b']:\n",
    "        for params in [CFG.char_vectorizer_params, CFG.word_vectorizer_params]:\n",
    "            \n",
    "            tfidf_vectorizer = joblib.load(f'{CFG.data_path}/vectorizers/{column}_{params[\"analyzer\"]}_tfidf_vectorizer.pkl')\n",
    "            tfidf_matrix = tfidf_vectorizer.transform(df[column].fillna(''))\n",
    "            tfidf_dense = tfidf_matrix.toarray()\n",
    "            \n",
    "            feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "            for i in range(len(feature_names)):\n",
    "                df[f'{column}_{params[\"analyzer\"]}_tfidf_{i}'] = tfidf_dense[:, i]\n",
    "            \n",
    "            del tfidf_vectorizer, tfidf_matrix, tfidf_dense\n",
    "            \n",
    "            gc.collect()\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d9de08",
   "metadata": {
    "papermill": {
     "duration": 0.006545,
     "end_time": "2025-01-28T07:38:35.490190",
     "exception": false,
     "start_time": "2025-01-28T07:38:35.483645",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "214a1619",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T07:38:35.504476Z",
     "iopub.status.busy": "2025-01-28T07:38:35.504255Z",
     "iopub.status.idle": "2025-01-28T07:38:35.512766Z",
     "shell.execute_reply": "2025-01-28T07:38:35.512133Z"
    },
    "papermill": {
     "duration": 0.017196,
     "end_time": "2025-01-28T07:38:35.513914",
     "exception": false,
     "start_time": "2025-01-28T07:38:35.496718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_parquet(CFG.test_path)\n",
    "test = test.drop(columns=['scored'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa16e561",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-01-28T07:38:35.528698Z",
     "iopub.status.busy": "2025-01-28T07:38:35.528471Z",
     "iopub.status.idle": "2025-01-28T07:38:39.384771Z",
     "shell.execute_reply": "2025-01-28T07:38:39.384055Z"
    },
    "papermill": {
     "duration": 3.865249,
     "end_time": "2025-01-28T07:38:39.386522",
     "exception": false,
     "start_time": "2025-01-28T07:38:35.521273",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = add_features(test, is_train=False)\n",
    "test = add_tfidf_features(test, is_train=False)\n",
    "test['tta_oof'] = gemma_test_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec1efbcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T07:38:39.401409Z",
     "iopub.status.busy": "2025-01-28T07:38:39.401154Z",
     "iopub.status.idle": "2025-01-28T07:38:39.454218Z",
     "shell.execute_reply": "2025-01-28T07:38:39.453380Z"
    },
    "papermill": {
     "duration": 0.06182,
     "end_time": "2025-01-28T07:38:39.455639",
     "exception": false,
     "start_time": "2025-01-28T07:38:39.393819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = test.drop(columns=['id', 'prompt', 'response_a', 'response_b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e43a7ffc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T07:38:39.470501Z",
     "iopub.status.busy": "2025-01-28T07:38:39.470257Z",
     "iopub.status.idle": "2025-01-28T07:38:40.776589Z",
     "shell.execute_reply": "2025-01-28T07:38:40.775608Z"
    },
    "papermill": {
     "duration": 1.315367,
     "end_time": "2025-01-28T07:38:40.778175",
     "exception": false,
     "start_time": "2025-01-28T07:38:39.462808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Reducing memory usage\n",
      "------ Memory usage before: 0.08 MB\n",
      "------ Memory usage after: 0.02 MB\n",
      "------ Decreased memory usage by 75.0%\n"
     ]
    }
   ],
   "source": [
    "test = reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870c19f3",
   "metadata": {
    "papermill": {
     "duration": 0.006855,
     "end_time": "2025-01-28T07:38:40.793288",
     "exception": false,
     "start_time": "2025-01-28T07:38:40.786433",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Running inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8fc9ab40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T07:38:40.807912Z",
     "iopub.status.busy": "2025-01-28T07:38:40.807689Z",
     "iopub.status.idle": "2025-01-28T07:38:41.082976Z",
     "shell.execute_reply": "2025-01-28T07:38:41.082312Z"
    },
    "papermill": {
     "duration": 0.284276,
     "end_time": "2025-01-28T07:38:41.084472",
     "exception": false,
     "start_time": "2025-01-28T07:38:40.800196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = TabularPredictor.load('/kaggle/input/wsdm-cup-gemma-2-9b-4-bit-qlora/autogluon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b71e240f",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-01-28T07:38:41.099369Z",
     "iopub.status.busy": "2025-01-28T07:38:41.099130Z",
     "iopub.status.idle": "2025-01-28T07:40:15.659547Z",
     "shell.execute_reply": "2025-01-28T07:40:15.658465Z"
    },
    "papermill": {
     "duration": 94.569667,
     "end_time": "2025-01-28T07:40:15.661412",
     "exception": false,
     "start_time": "2025-01-28T07:38:41.091745",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ag_test_preds = predictor.predict(test).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7156420b",
   "metadata": {
    "papermill": {
     "duration": 0.006693,
     "end_time": "2025-01-28T07:40:15.676102",
     "exception": false,
     "start_time": "2025-01-28T07:40:15.669409",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7aec96ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T07:40:15.691059Z",
     "iopub.status.busy": "2025-01-28T07:40:15.690444Z",
     "iopub.status.idle": "2025-01-28T07:40:15.704837Z",
     "shell.execute_reply": "2025-01-28T07:40:15.704048Z"
    },
    "papermill": {
     "duration": 0.023113,
     "end_time": "2025-01-28T07:40:15.705964",
     "exception": false,
     "start_time": "2025-01-28T07:40:15.682851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>327228</td>\n",
       "      <td>model_b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1139415</td>\n",
       "      <td>model_a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1235630</td>\n",
       "      <td>model_a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   winner\n",
       "0   327228  model_b\n",
       "1  1139415  model_a\n",
       "2  1235630  model_a"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = result_df[['id', 'winner']].copy()\n",
    "sub['winner'] = np.where(ag_test_preds == 0, 'model_a', 'model_b')\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "sub.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 10131489,
     "sourceId": 86946,
     "sourceType": "competition"
    },
    {
     "datasetId": 6447806,
     "sourceId": 10576723,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 193770,
     "modelInstanceId": 171453,
     "sourceId": 200971,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 181.871988,
   "end_time": "2025-01-28T07:40:18.814602",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-28T07:37:16.942614",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
